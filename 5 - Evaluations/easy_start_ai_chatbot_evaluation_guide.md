# Easyâ€‘Start Guide â€“ Measuring Your First AI Chatbot in LangSmith

> **Goal:** Know if your bot is *finding the right info*, *writing good answers*, *running fast*, and *helping users*â€”without deep ML expertise.

---

## 1. The Four Things to Measure

| Layer | Plainâ€‘English Question | Starter Metric |
|-------|-----------------------|----------------|
| **1. Retrieval** | â€œDid the bot pull the right document into context?â€ | **Precisionâ€¯@â€¯5** â€“ out of the topâ€‘5 docs, how many are relevant? |
| **2. Generation** | â€œDid the botâ€™s answer match the facts it pulled?â€ | **Grounded Accuracy** â€“ thumbsâ€‘up / thumbsâ€‘down on â€œIs this answer correct and based on the docs?â€<br>**LLMâ€‘asâ€‘Judge Similarity** â€“ automated 0â€‘1 score vs. your gold answer |
| **3. System Health** | â€œIs the bot fast and affordable?â€ | **p95 Latency** (speed) and **Cost per 1â€¯k tokens** |
| **4. Business Impact** | â€œAre people happy and finishing tasks?â€ | **User Satisfaction** (thumbsâ€‘upâ€¯%) and **Query Success** (answer marked helpful) |

*(Add more metrics laterâ€”start here.)*

---

## 2. What â€œGood Enoughâ€ Looks Like

| Metric | Firstâ€‘Launch Target |
|--------|---------------------|
| Precisionâ€¯@â€¯5 â‰¥ **70â€¯%** |
| Grounded Accuracy â‰¥ **85â€¯%** |
| LLMâ€‘asâ€‘Judge Similarity â‰¥ **0.85** |
| p95 Latency â‰¤ **2â€¯s** |
| Cost â‰¤ **$0.02** per 1â€¯k tokens *(example)* |
| User Satisfaction â‰¥ **80â€¯% thumbsâ€‘up** |
| Query Success â‰¥ **80â€¯%** |

---

## 3. Plugging Metrics into LangSmith (Stepâ€‘byâ€‘Step)

1. **Create Two Tiny Datasets**  
   * *Retrieval set* â€“ 10 real user questions **+** the IDs of the docs that should be fetched.  
   * *Answer set* â€“ the same questions **+** a short â€œgoldâ€ answer you wrote.

2. **Add Builtâ€‘In Evaluators**  

   ```python
   evaluators = [
       "context_precision",   # Precisionâ€¯@â€¯k
       "qa_correctness",      # LLMâ€‘asâ€‘Judge accuracy / similarity
   ]
   ```

3. **Run an Experiment**  

   ```python
   from langsmith import evaluate
   evaluate(my_chatbot, data="answer_set", evaluators=evaluators)
   ```

4. **Read the Report**  
   LangSmith shows scores, failed examples, and lets you compare runs sideâ€‘byâ€‘side.

5. **Track System Metrics**  
   In your code, log `latency_ms` and `cost_usd` for every requestâ€”LangSmith will chart them automatically.

6. **Collect Real User Feedback**  
   Add ğŸ‘ / ğŸ‘ buttons in your UI. Send the signal back to LangSmith with one line of code.

---

## 4. Everyday Workflow

| When | Quick Check |
|------|-------------|
| **Every code or prompt change** | Run the 10â€‘example *smoke test*. Block release if any key metric drops. |
| **Nightly** | Run the full dataset (50â€‘100â€¯Qs). Slack the summary. |
| **Weekly** | Review latency & cost graphs for spikes. |
| **Monthly** | Review user satisfaction & query success. Plan improvements. |

---

## 5. Tips for New Teams

* **Start small.** Ten wellâ€‘chosen examples beat 1â€¯000 random ones.  
* **Fail fast.** One low Precision score often means a bad embedding or filter.  
* **Click through.** In LangSmith, failed test â†’ retrieved docs â†’ answer, all in one place.  
* **Iterate.** Add extra metrics (recall, toxicity) only after the basics are green.  

---

## 6. Cheatâ€‘Sheet of Terms

| Term | Oneâ€‘Line Definition |
|------|---------------------|
| **Precisionâ€¯@â€¯k** | % of the topâ€‘k docs that are relevant. Higher = fewer junk docs. |
| **Grounded Accuracy** | Manual thumbsâ€‘up that the answer matches the docs. |
| **LLMâ€‘asâ€‘Judge Similarity** | Automated 0â€‘1 score measuring closeness to your gold answer. |
| **p95 Latency** | 95â€¯% of responses are faster than this time. |
| **User Satisfaction** | Thumbsâ€‘up ratio or short survey score. |

---

## AppendixÂ â€“Â LLMâ€‘asâ€‘Judge Setup (2Â Minutes)

1. **Dataset needs:** `query`, `reference_answer`.  
2. **Use builtâ€‘in evaluator:**  

   ```python
   evaluate(
       my_chatbot,
       data="answer_set",
       evaluators=["qa_correctness"],  # LLMâ€‘asâ€‘Judge
   )
   ```

3. **Making it yours:**  
   *Want a 1â€‘5 rubric?* Create a **custom evaluator** in the LangSmith UI, paste your rubric, and save.  
   *Judge keeps misâ€‘scoring?* Click **â€œCorrectâ€** in the UI; future runs will fewâ€‘shot learn from your edit.

---

**Takeaway:** Track four core layers, automate a tiny smoke test, and let LangSmithâ€™s UI do the heavy lifting while your team ramps up.
