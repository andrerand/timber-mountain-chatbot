{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21a1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\n",
      "============================================================\n",
      "✅ All libraries imported successfully!\n",
      "📅 Processing started at: 2025-07-01 06:06:47\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# TIMBER MOUNTAIN AI CHATBOT - DATA PROCESSING PIPELINE\n",
    "# ===================================================================\n",
    "# Step 1: Process and Combine Your Data\n",
    "# This notebook processes A/B test metadata and PDF presentations\n",
    "# to create a unified dataset for the Neo4j GraphRAG system.\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📅 Processing started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b42d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 LOADING A/B TEST METADATA\n",
      "----------------------------------------\n",
      "✅ Successfully loaded metadata for 5 A/B tests\n",
      "📁 Metadata file: Timber Mountain - AB Test Metadata.xlsx\n",
      "📋 Columns: ['Test Name', 'PDF File Name', 'Test Launch', 'Test End', 'Country', 'Target Segment', 'Page / Placement', 'Test Hypothesis', 'Test Result & Interpretation']\n",
      "\n",
      "🧪 A/B TEST OVERVIEW:\n",
      "----------------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   📄 PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   📅 Duration: 2024-07-08 → 2024-07-28\n",
      "   🎯 Target: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\n",
      "\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   📄 PDF: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   📅 Duration: 2024-08-05 → 2024-08-25\n",
      "   🎯 Target: Wild Willy AI Travel Planner Users\n",
      "\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   📄 PDF: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   📅 Duration: 2024-09-02 → 2024-09-22\n",
      "   🎯 Target: Visitors adding ≥ 1 non-ticket item\n",
      "\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   📄 PDF: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   📅 Duration: 2024-10-07 → 2024-10-21\n",
      "   🎯 Target: Mobile visitors\n",
      "\n",
      "5. Homepage: Special Offers Carousel — Merchandising Test\n",
      "   📄 PDF: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   📅 Duration: 2024-11-04 → 2024-11-24\n",
      "   🎯 Target: First-time visitors\n",
      "\n",
      "🔍 Created metadata lookup for 5 tests\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: LOAD AND ANALYZE A/B TEST METADATA\n",
    "# ===================================================================\n",
    "\n",
    "# Define file paths\n",
    "project_root = Path(\"..\")\n",
    "metadata_path = project_root / \"2 - Synthetic Metadata\" / \"Timber Mountain - AB Test Metadata.xlsx\"\n",
    "pdf_directory = project_root / \"3 - Synthetic A:B Test Results Decks\" / \"2 - Results Presentations\"\n",
    "\n",
    "print(\"📊 LOADING A/B TEST METADATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Load metadata from Excel file\n",
    "    metadata_df = pd.read_excel(metadata_path, sheet_name='Metadata')\n",
    "    \n",
    "    print(f\"✅ Successfully loaded metadata for {len(metadata_df)} A/B tests\")\n",
    "    print(f\"📁 Metadata file: {metadata_path.name}\")\n",
    "    print(f\"📋 Columns: {list(metadata_df.columns)}\")\n",
    "    \n",
    "    # Display summary of tests\n",
    "    print(f\"\\n🧪 A/B TEST OVERVIEW:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        print(f\"{i+1}. {row['Test Name']}\")\n",
    "        print(f\"   📄 PDF: {row['PDF File Name']}\")\n",
    "        print(f\"   📅 Duration: {row['Test Launch'].strftime('%Y-%m-%d')} → {row['Test End'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   🎯 Target: {row['Target Segment']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create metadata lookup dictionary for efficient access\n",
    "    metadata_lookup = {}\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        pdf_filename = row['PDF File Name']\n",
    "        metadata_lookup[pdf_filename] = {\n",
    "            'test_name': row['Test Name'],\n",
    "            'test_launch': row['Test Launch'].strftime('%Y-%m-%d'),\n",
    "            'test_end': row['Test End'].strftime('%Y-%m-%d'),\n",
    "            'country': row['Country'],\n",
    "            'target_segment': row['Target Segment'],\n",
    "            'page_placement': row['Page / Placement'],\n",
    "            'test_hypothesis': row['Test Hypothesis'],\n",
    "            'test_result': row['Test Result & Interpretation']\n",
    "        }\n",
    "    \n",
    "    print(f\"🔍 Created metadata lookup for {len(metadata_lookup)} tests\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Metadata file not found at {metadata_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "h26wg3rx0si",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 PROCESSING PDF FILES\n",
      "----------------------------------------\n",
      "📂 Found 5 PDF files to process:\n",
      "   - 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   - 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   - 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   - 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   - 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "\n",
      "🔍 EXTRACTING TEXT FROM PDF FILES:\n",
      "----------------------------------------\n",
      "Processing: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Extracted 2,992 characters from 7 pages\n",
      "Processing: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Extracted 4,031 characters from 9 pages\n",
      "Processing: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Extracted 4,595 characters from 10 pages\n",
      "Processing: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Extracted 4,244 characters from 10 pages\n",
      "Processing: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Extracted 5,169 characters from 10 pages\n",
      "\n",
      "📊 PDF PROCESSING SUMMARY:\n",
      "   • Successfully processed: 5\n",
      "   • Failed to process: 0\n",
      "   • Total text extracted: 21,031 characters\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 2: PROCESS PDF FILES AND EXTRACT TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n📄 PROCESSING PDF FILES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find all PDF files in the results presentations directory\n",
    "pdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"❌ ERROR: No PDF files found in {pdf_directory}\")\n",
    "    raise FileNotFoundError(\"PDF files not found\")\n",
    "\n",
    "print(f\"📂 Found {len(pdf_files)} PDF files to process:\")\n",
    "for pdf_file in sorted(pdf_files):\n",
    "    print(f\"   - {pdf_file.name}\")\n",
    "\n",
    "# Dictionary to store extracted text content\n",
    "pdf_text_content = {}\n",
    "\n",
    "print(f\"\\n🔍 EXTRACTING TEXT FROM PDF FILES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for pdf_path in sorted(pdf_files):\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract all text from the PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Extract text from all pages\n",
    "            all_pages_text = []\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n{page_text}\")\n",
    "                else:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n[No text extracted]\")\n",
    "            \n",
    "            # Combine all pages\n",
    "            full_text = \"\\n\\n\".join(all_pages_text)\n",
    "            \n",
    "            # Store the text content\n",
    "            pdf_text_content[pdf_path.name] = {\n",
    "                'file_path': str(pdf_path),\n",
    "                'page_count': len(pdf.pages),\n",
    "                'text_length': len(full_text),\n",
    "                'full_text': full_text\n",
    "            }\n",
    "            \n",
    "            print(f\"   ✅ Extracted {len(full_text):,} characters from {len(pdf.pages)} pages\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERROR processing {pdf_path.name}: {e}\")\n",
    "        pdf_text_content[pdf_path.name] = {\n",
    "            'file_path': str(pdf_path),\n",
    "            'page_count': 0,\n",
    "            'text_length': 0,\n",
    "            'full_text': f\"[ERROR: Could not extract text - {e}]\",\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(f\"\\n📊 PDF PROCESSING SUMMARY:\")\n",
    "print(f\"   • Successfully processed: {len([k for k, v in pdf_text_content.items() if 'error' not in v])}\")\n",
    "print(f\"   • Failed to process: {len([k for k, v in pdf_text_content.items() if 'error' in v])}\")\n",
    "print(f\"   • Total text extracted: {sum(v['text_length'] for v in pdf_text_content.values()):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2apak0qhg45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 COMBINING METADATA WITH PDF CONTENT\n",
      "----------------------------------------\n",
      "Combining data for: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Metadata found: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   ✅ PDF content: 2,992 characters\n",
      "\n",
      "Combining data for: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Metadata found: AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   ✅ PDF content: 4,031 characters\n",
      "\n",
      "Combining data for: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Metadata found: Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   ✅ PDF content: 4,595 characters\n",
      "\n",
      "Combining data for: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Metadata found: Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   ✅ PDF content: 4,244 characters\n",
      "\n",
      "Combining data for: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Metadata found: Homepage: Special Offers Carousel — Merchandising Test\n",
      "   ✅ PDF content: 5,169 characters\n",
      "\n",
      "📊 COMBINATION SUMMARY:\n",
      "   • Total documents processed: 5\n",
      "   • Successful metadata matches: 5\n",
      "   • Missing metadata: 0\n",
      "   • PDF extraction failures: 0\n",
      "   • Unified documents created: 5\n",
      "\n",
      "📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\n",
      "----------------------------------------\n",
      "Document ID: timber_mountain_001\n",
      "Source PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "Test Name: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "Content Length: === PAGE 1 ===\n",
      "Locale-Aware Experience: How\n",
      "We Boosted International\n",
      "Conversions at Timber\n",
      "Mountain\n",
      "...\n",
      "Metadata Keys: ['test_name', 'test_launch', 'test_end', 'country', 'target_segment', 'page_placement', 'test_hypothesis', 'test_result']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 3: COMBINE METADATA WITH PDF TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔗 COMBINING METADATA WITH PDF CONTENT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "unified_documents = []\n",
    "processing_summary = {\n",
    "    'successful_matches': 0,\n",
    "    'missing_metadata': 0,\n",
    "    'missing_pdf_content': 0,\n",
    "    'total_processed': 0\n",
    "}\n",
    "\n",
    "# Iterate through each PDF file and combine with metadata\n",
    "for pdf_filename in pdf_text_content.keys():\n",
    "    print(f\"Combining data for: {pdf_filename}\")\n",
    "    \n",
    "    # Get metadata for this PDF\n",
    "    metadata = metadata_lookup.get(pdf_filename, {})\n",
    "    \n",
    "    # Get PDF text content\n",
    "    pdf_data = pdf_text_content.get(pdf_filename, {})\n",
    "    \n",
    "    # Create unified document\n",
    "    unified_doc = {\n",
    "        'document_id': f\"timber_mountain_{len(unified_documents) + 1:03d}\",\n",
    "        'source_pdf_filename': pdf_filename,\n",
    "        'source_pdf_path': pdf_data.get('file_path', ''),\n",
    "        'pdf_processing': {\n",
    "            'page_count': pdf_data.get('page_count', 0),\n",
    "            'text_length': pdf_data.get('text_length', 0),\n",
    "            'extraction_error': pdf_data.get('error', None)\n",
    "        },\n",
    "        'metadata': metadata,\n",
    "        'content': {\n",
    "            'full_text': pdf_data.get('full_text', ''),\n",
    "            'processed_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Update processing summary\n",
    "    processing_summary['total_processed'] += 1\n",
    "    \n",
    "    if metadata:\n",
    "        processing_summary['successful_matches'] += 1\n",
    "        print(f\"   ✅ Metadata found: {metadata.get('test_name', 'Unknown')}\")\n",
    "    else:\n",
    "        processing_summary['missing_metadata'] += 1\n",
    "        print(f\"   ⚠️  No metadata found for {pdf_filename}\")\n",
    "    \n",
    "    if pdf_data.get('full_text') and 'error' not in pdf_data:\n",
    "        print(f\"   ✅ PDF content: {pdf_data['text_length']:,} characters\")\n",
    "    else:\n",
    "        processing_summary['missing_pdf_content'] += 1\n",
    "        print(f\"   ⚠️  PDF content extraction failed\")\n",
    "    \n",
    "    unified_documents.append(unified_doc)\n",
    "    print()\n",
    "\n",
    "print(f\"📊 COMBINATION SUMMARY:\")\n",
    "print(f\"   • Total documents processed: {processing_summary['total_processed']}\")\n",
    "print(f\"   • Successful metadata matches: {processing_summary['successful_matches']}\")\n",
    "print(f\"   • Missing metadata: {processing_summary['missing_metadata']}\")\n",
    "print(f\"   • PDF extraction failures: {processing_summary['missing_pdf_content']}\")\n",
    "print(f\"   • Unified documents created: {len(unified_documents)}\")\n",
    "\n",
    "# Display sample unified document structure\n",
    "if unified_documents:\n",
    "    print(f\"\\n📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    sample_doc = unified_documents[0]\n",
    "    print(f\"Document ID: {sample_doc['document_id']}\")\n",
    "    print(f\"Source PDF: {sample_doc['source_pdf_filename']}\")\n",
    "    print(f\"Test Name: {sample_doc['metadata'].get('test_name', 'N/A')}\")\n",
    "    print(f\"Content Length: {sample_doc['content']['full_text'][:100]}...\")\n",
    "    print(f\"Metadata Keys: {list(sample_doc['metadata'].keys()) if sample_doc['metadata'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12r7oth4vzba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 GENERATING UNIFIED JSON OUTPUT\n",
      "----------------------------------------\n",
      "✅ Successfully saved unified dataset!\n",
      "📁 Output file: processed_documents.json\n",
      "📏 File size: 0.03 MB\n",
      "📊 Contains 5 documents\n",
      "\n",
      "📋 JSON STRUCTURE SUMMARY:\n",
      "----------------------------------------\n",
      "└── processing_metadata\n",
      "    ├── created_timestamp\n",
      "    ├── source_excel_file\n",
      "    ├── source_pdf_directory\n",
      "    ├── total_documents\n",
      "    └── processing_summary\n",
      "└── documents (array)\n",
      "    └── [document]\n",
      "        ├── document_id\n",
      "        ├── source_pdf_filename\n",
      "        ├── source_pdf_path\n",
      "        ├── pdf_processing\n",
      "        ├── metadata\n",
      "        └── content\n",
      "\n",
      "🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\n",
      "📝 Use 'processed_documents.json' as input for graph population\n",
      "\n",
      "📄 SAMPLE JSON OUTPUT:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"processing_metadata\": {\n",
      "    \"created_timestamp\": \"2025-07-01T06:16:03.658141\",\n",
      "    \"source_excel_file\": \"../2 - Synthetic Metadata/Timber Mountain - AB Test Metadata.xlsx\",\n",
      "    \"source_pdf_directory\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations\",\n",
      "    \"total_documents\": 5,\n",
      "    \"processing_summary\": {\n",
      "      \"successful_matches\": 5,\n",
      "      \"missing_metadata\": 0,\n",
      "      \"missing_pdf_content\": 0,\n",
      "      \"total_processed\": 5\n",
      "    }\n",
      "  },\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"document_id\": \"timber_mountain_001\",\n",
      "      \"source_pdf_filename\": \"1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"source_pdf_path\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations/1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"pdf_processing\": {\n",
      "        \"page_count\": 7,\n",
      "        \"text_length\": 2992,\n",
      "        \"extraction_error\": null\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"test_name\": \"...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 4: GENERATE UNIFIED JSON OUTPUT FOR NEO4J GRAPHRAG\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n💾 GENERATING UNIFIED JSON OUTPUT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define output path\n",
    "output_json_path = project_root / \"processed_documents.json\"\n",
    "\n",
    "# Create the final output structure optimized for GraphRAG\n",
    "final_output = {\n",
    "    'processing_metadata': {\n",
    "        'created_timestamp': datetime.now().isoformat(),\n",
    "        'source_excel_file': str(metadata_path),\n",
    "        'source_pdf_directory': str(pdf_directory),\n",
    "        'total_documents': len(unified_documents),\n",
    "        'processing_summary': processing_summary\n",
    "    },\n",
    "    'documents': unified_documents\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Save to JSON file with proper formatting\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Calculate file size\n",
    "    file_size_mb = output_json_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✅ Successfully saved unified dataset!\")\n",
    "    print(f\"📁 Output file: {output_json_path.name}\")\n",
    "    print(f\"📏 File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"📊 Contains {len(unified_documents)} documents\")\n",
    "    \n",
    "    # Display JSON structure summary\n",
    "    print(f\"\\n📋 JSON STRUCTURE SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"└── processing_metadata\")\n",
    "    print(\"    ├── created_timestamp\")\n",
    "    print(\"    ├── source_excel_file\") \n",
    "    print(\"    ├── source_pdf_directory\")\n",
    "    print(\"    ├── total_documents\")\n",
    "    print(\"    └── processing_summary\")\n",
    "    print(\"└── documents (array)\")\n",
    "    print(\"    └── [document]\")\n",
    "    print(\"        ├── document_id\")\n",
    "    print(\"        ├── source_pdf_filename\")\n",
    "    print(\"        ├── source_pdf_path\")\n",
    "    print(\"        ├── pdf_processing\")\n",
    "    print(\"        ├── metadata\")\n",
    "    print(\"        └── content\")\n",
    "    \n",
    "    print(f\"\\n🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\")\n",
    "    print(f\"📝 Use '{output_json_path.name}' as input for graph population\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR saving JSON file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display sample of final JSON structure\n",
    "print(f\"\\n📄 SAMPLE JSON OUTPUT:\")\n",
    "print(\"-\" * 40)\n",
    "sample_output = {\n",
    "    'processing_metadata': final_output['processing_metadata'],\n",
    "    'documents': [final_output['documents'][0]] if final_output['documents'] else []\n",
    "}\n",
    "\n",
    "print(json.dumps(sample_output, indent=2)[:1000] + \"...\" if len(str(sample_output)) > 1000 else json.dumps(sample_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d72gx3i5aw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DATA VALIDATION AND FINAL SUMMARY\n",
      "============================================================\n",
      "🔍 VALIDATION RESULTS:\n",
      "------------------------------\n",
      "✅ All A/B tests processed: True (5/5)\n",
      "✅ All metadata matched: True (5/5)\n",
      "✅ All PDFs extracted: True (failures: 0)\n",
      "✅ Output file created: True (processed_documents.json)\n",
      "\n",
      "📊 CONTENT STATISTICS:\n",
      "------------------------------\n",
      "Total text content: 21,031 characters\n",
      "Average per document: 4,206 characters\n",
      "\n",
      "📋 DOCUMENT-LEVEL BREAKDOWN:\n",
      "------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Co...\n",
      "   📄 7 pages, 2,992 characters\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Ad...\n",
      "   📄 9 pages, 4,031 characters\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamle...\n",
      "   📄 10 pages, 4,595 characters\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More...\n",
      "   📄 10 pages, 4,244 characters\n",
      "5. Homepage: Special Offers Carousel — Merchandising ...\n",
      "   📄 10 pages, 5,169 characters\n",
      "\n",
      "⚠️  POTENTIAL ISSUES:\n",
      "------------------------------\n",
      "✅ No issues detected - data quality looks good!\n",
      "\n",
      "🎯 PROCESSING COMPLETE!\n",
      "============================================================\n",
      "📈 Successfully processed 5 A/B test documents\n",
      "💾 Output saved to: ../processed_documents.json\n",
      "🔗 Ready for Neo4j graph database population\n",
      "📅 Processing completed at: 2025-07-01 06:17:28\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "------------------------------\n",
      "1. 🗄️  Set up Neo4j database connection\n",
      "2. 🏗️  Create graph schema for A/B test data\n",
      "3. 📥 Import processed_documents.json into Neo4j\n",
      "4. 🔍 Build GraphRAG query system\n",
      "5. 🤖 Integrate with LangChain for chatbot responses\n",
      "6. 🌐 Deploy Streamlit frontend\n",
      "\n",
      "🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 5: DATA VALIDATION AND SUMMARY STATISTICS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n✅ DATA VALIDATION AND FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validation checks\n",
    "validation_results = {\n",
    "    'all_tests_processed': len(unified_documents) == len(metadata_df),\n",
    "    'all_metadata_matched': processing_summary['missing_metadata'] == 0,\n",
    "    'all_pdfs_extracted': processing_summary['missing_pdf_content'] == 0,\n",
    "    'output_file_created': output_json_path.exists(),\n",
    "    'total_characters': sum(len(doc['content']['full_text']) for doc in unified_documents),\n",
    "    'avg_characters_per_doc': 0\n",
    "}\n",
    "\n",
    "if len(unified_documents) > 0:\n",
    "    validation_results['avg_characters_per_doc'] = validation_results['total_characters'] / len(unified_documents)\n",
    "\n",
    "# Display validation results\n",
    "print(\"🔍 VALIDATION RESULTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"✅ All A/B tests processed: {validation_results['all_tests_processed']} ({len(unified_documents)}/{len(metadata_df)})\")\n",
    "print(f\"✅ All metadata matched: {validation_results['all_metadata_matched']} ({processing_summary['successful_matches']}/{len(unified_documents)})\")\n",
    "print(f\"✅ All PDFs extracted: {validation_results['all_pdfs_extracted']} (failures: {processing_summary['missing_pdf_content']})\")\n",
    "print(f\"✅ Output file created: {validation_results['output_file_created']} ({output_json_path.name})\")\n",
    "\n",
    "print(f\"\\n📊 CONTENT STATISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total text content: {validation_results['total_characters']:,} characters\")\n",
    "print(f\"Average per document: {validation_results['avg_characters_per_doc']:,.0f} characters\")\n",
    "\n",
    "# Document-level statistics\n",
    "print(f\"\\n📋 DOCUMENT-LEVEL BREAKDOWN:\")\n",
    "print(\"-\" * 30)\n",
    "for i, doc in enumerate(unified_documents, 1):\n",
    "    test_name = doc['metadata'].get('test_name', 'Unknown Test')\n",
    "    char_count = len(doc['content']['full_text'])\n",
    "    page_count = doc['pdf_processing']['page_count']\n",
    "    \n",
    "    print(f\"{i}. {test_name[:50]}{'...' if len(test_name) > 50 else ''}\")\n",
    "    print(f\"   📄 {page_count} pages, {char_count:,} characters\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(f\"\\n⚠️  POTENTIAL ISSUES:\")\n",
    "print(\"-\" * 30)\n",
    "issues_found = 0\n",
    "\n",
    "for doc in unified_documents:\n",
    "    if doc['pdf_processing'].get('extraction_error'):\n",
    "        print(f\"❌ PDF extraction error in: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if not doc['metadata']:\n",
    "        print(f\"⚠️  Missing metadata for: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if len(doc['content']['full_text']) < 100:\n",
    "        print(f\"⚠️  Very short content in: {doc['source_pdf_filename']} ({len(doc['content']['full_text'])} chars)\")\n",
    "        issues_found += 1\n",
    "\n",
    "if issues_found == 0:\n",
    "    print(\"✅ No issues detected - data quality looks good!\")\n",
    "\n",
    "# Final processing summary\n",
    "print(f\"\\n🎯 PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📈 Successfully processed {len(unified_documents)} A/B test documents\")\n",
    "print(f\"💾 Output saved to: {output_json_path}\")\n",
    "print(f\"🔗 Ready for Neo4j graph database population\")\n",
    "print(f\"📅 Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Next steps guidance\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. 🗄️  Set up Neo4j database connection\")\n",
    "print(\"2. 🏗️  Create graph schema for A/B test data\")\n",
    "print(\"3. 📥 Import processed_documents.json into Neo4j\")\n",
    "print(\"4. 🔍 Build GraphRAG query system\")\n",
    "print(\"5. 🤖 Integrate with LangChain for chatbot responses\")\n",
    "print(f\"6. 🌐 Deploy Streamlit frontend\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uwkee03yqt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 NEO4J KNOWLEDGE GRAPH POPULATION\n",
      "============================================================\n",
      "🎯 Step 2: Transform documents into enriched knowledge graph\n",
      "✅ Environment variables loaded successfully\n",
      "✅ Neo4j connection established: Connection successful\n",
      "📊 Neo4j Kernel: 5.27-aura\n",
      "📊 Cypher: 5\n",
      "\n",
      "🔧 SETUP COMPLETE - Ready for graph transformation!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 6: ENVIRONMENT SETUP AND NEO4J CONNECTION\n",
    "# ===================================================================\n",
    "# Step 2: Populate the Enriched Neo4j Knowledge Graph\n",
    "# This section uses LangChain's LLMGraphTransformer to automatically\n",
    "# create an enriched knowledge graph from our processed documents.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n🔗 NEO4J KNOWLEDGE GRAPH POPULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 Step 2: Transform documents into enriched knowledge graph\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables\n",
    "required_env_vars = ['OPENAI_API_KEY', 'NEO4J_URI', 'NEO4J_USERNAME', 'NEO4J_PASSWORD']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ ERROR: Missing environment variables: {missing_vars}\")\n",
    "    print(\"Please ensure your .env file contains:\")\n",
    "    print(\"  - OPENAI_API_KEY=your_openai_key\")\n",
    "    print(\"  - NEO4J_URI=neo4j+s://your_neo4j_uri\")\n",
    "    print(\"  - NEO4J_USERNAME=your_username\")\n",
    "    print(\"  - NEO4J_PASSWORD=your_password\")\n",
    "    raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "print(\"✅ Environment variables loaded successfully\")\n",
    "\n",
    "# Test Neo4j connection\n",
    "try:\n",
    "    neo4j_graph = Neo4jGraph(\n",
    "        url=os.getenv('NEO4J_URI'),\n",
    "        username=os.getenv('NEO4J_USERNAME'),\n",
    "        password=os.getenv('NEO4J_PASSWORD')\n",
    "    )\n",
    "    \n",
    "    # Test the connection\n",
    "    result = neo4j_graph.query(\"RETURN 'Connection successful' as message\")\n",
    "    print(f\"✅ Neo4j connection established: {result[0]['message']}\")\n",
    "    \n",
    "    # Get database info\n",
    "    db_info = neo4j_graph.query(\"CALL dbms.components() YIELD name, versions RETURN name, versions[0] as version\")\n",
    "    for info in db_info:\n",
    "        print(f\"📊 {info['name']}: {info['version']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR connecting to Neo4j: {e}\")\n",
    "    print(\"Please verify your Neo4j credentials and connection string\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n🔧 SETUP COMPLETE - Ready for graph transformation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bwd960h1koe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 7: LOAD AND PREPARE PROCESSED DOCUMENTS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n📂 LOADING PROCESSED DOCUMENTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the processed documents JSON\n",
    "try:\n",
    "    with open(output_json_path, 'r', encoding='utf-8') as f:\n",
    "        processed_data = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Loaded processed documents from: {output_json_path.name}\")\n",
    "    print(f\"📊 Processing metadata: {processed_data['processing_metadata']['created_timestamp']}\")\n",
    "    print(f\"📈 Total documents: {processed_data['processing_metadata']['total_documents']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: {output_json_path.name} not found. Please run the data processing steps first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR loading processed documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Convert to LangChain Document objects with enriched metadata\n",
    "langchain_documents = []\n",
    "\n",
    "print(f\"\\n🔄 CONVERTING TO LANGCHAIN DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for doc_data in processed_data['documents']:\n",
    "    # Create metadata dictionary that includes both structured and processing info\n",
    "    metadata = {\n",
    "        'document_id': doc_data['document_id'],\n",
    "        'source_pdf': doc_data['source_pdf_filename'],\n",
    "        'page_count': doc_data['pdf_processing']['page_count'],\n",
    "        'text_length': doc_data['pdf_processing']['text_length'],\n",
    "        # Structured metadata from Excel\n",
    "        'test_name': doc_data['metadata'].get('test_name', ''),\n",
    "        'test_launch': doc_data['metadata'].get('test_launch', ''),\n",
    "        'test_end': doc_data['metadata'].get('test_end', ''),\n",
    "        'country': doc_data['metadata'].get('country', ''),\n",
    "        'target_segment': doc_data['metadata'].get('target_segment', ''),\n",
    "        'page_placement': doc_data['metadata'].get('page_placement', ''),\n",
    "        'test_hypothesis': doc_data['metadata'].get('test_hypothesis', ''),\n",
    "        'test_result': doc_data['metadata'].get('test_result', ''),\n",
    "    }\n",
    "    \n",
    "    # Create enriched content that includes both text and structured data\n",
    "    # This is the key insight: LLMGraphTransformer will see both unstructured text\n",
    "    # and structured metadata, allowing it to create nodes with rich properties\n",
    "    enriched_content = f\\\"\\\"\\\"\n",
    "A/B TEST: {metadata['test_name']}\n",
    "\n",
    "METADATA:\n",
    "- Document ID: {metadata['document_id']}\n",
    "- Test Launch Date: {metadata['test_launch']}\n",
    "- Test End Date: {metadata['test_end']}\n",
    "- Country: {metadata['country']}\n",
    "- Target Segment: {metadata['target_segment']}\n",
    "- Page/Placement: {metadata['page_placement']}\n",
    "- Test Hypothesis: {metadata['test_hypothesis']}\n",
    "- Test Result: {metadata['test_result']}\n",
    "\n",
    "FULL PRESENTATION CONTENT:\n",
    "{doc_data['content']['full_text']}\n",
    "\\\"\\\"\\\"\n",
    "    \n",
    "    # Create LangChain Document\n",
    "    langchain_doc = Document(\n",
    "        page_content=enriched_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    \n",
    "    langchain_documents.append(langchain_doc)\n",
    "    \n",
    "    print(f\"✅ {metadata['document_id']}: {metadata['test_name'][:50]}...\")\n",
    "    print(f\"   📝 Content length: {len(enriched_content):,} characters\")\n",
    "\n",
    "print(f\"\\n📋 DOCUMENT PREPARATION SUMMARY:\")\n",
    "print(f\"   • LangChain documents created: {len(langchain_documents)}\")\n",
    "print(f\"   • Total content for transformation: {sum(len(doc.page_content) for doc in langchain_documents):,} characters\")\n",
    "print(f\"   • Ready for LLMGraphTransformer processing\")\n",
    "\n",
    "# Display sample of enriched content structure\n",
    "if langchain_documents:\n",
    "    sample_doc = langchain_documents[0]\n",
    "    print(f\"\\n📄 SAMPLE ENRICHED CONTENT STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Document ID: {sample_doc.metadata['document_id']}\")\n",
    "    print(f\"Content preview: {sample_doc.page_content[:300]}...\")\n",
    "    print(f\"Metadata keys: {list(sample_doc.metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiw492z0ona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 8: CONFIGURE LLMGRAPHTRANSFORMER\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🤖 CONFIGURING LLMGRAPHTRANSFORMER\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialize OpenAI LLM for graph transformation\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",  # Using cost-effective model for graph extraction\n",
    "        temperature=0,        # Deterministic output for consistent graph structure\n",
    "        api_key=os.getenv('OPENAI_API_KEY')\n",
    "    )\n",
    "    print(\"✅ OpenAI LLM initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR initializing OpenAI LLM: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configure LLMGraphTransformer with specific node types and relationships\n",
    "# This will help the transformer understand the A/B testing domain\n",
    "try:\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        # Define allowed node labels - these guide the LLM to create structured nodes\n",
    "        allowed_nodes=[\n",
    "            \"ABTest\",           # Main A/B test entities\n",
    "            \"Metric\",           # KPIs and measurements\n",
    "            \"Segment\",          # Target segments and audiences\n",
    "            \"Feature\",          # Features being tested\n",
    "            \"Result\",           # Test outcomes and findings\n",
    "            \"Hypothesis\",       # Test hypotheses\n",
    "            \"Page\",             # Web pages/placements\n",
    "            \"Variant\",          # Test variants (control/treatment)\n",
    "            \"Conversion\",       # Conversion events\n",
    "            \"Insight\",          # Key insights and learnings\n",
    "            \"Recommendation\"    # Strategic recommendations\n",
    "        ],\n",
    "        # Define relationship types for connecting entities\n",
    "        allowed_relationships=[\n",
    "            \"TESTED_ON\",        # ABTest -> Page\n",
    "            \"MEASURED_BY\",      # ABTest -> Metric\n",
    "            \"TARGETS\",          # ABTest -> Segment\n",
    "            \"HAS_VARIANT\",      # ABTest -> Variant\n",
    "            \"PRODUCED\",         # ABTest -> Result\n",
    "            \"VALIDATES\",        # Result -> Hypothesis\n",
    "            \"INDICATES\",        # Result -> Insight\n",
    "            \"SUGGESTS\",         # Insight -> Recommendation\n",
    "            \"AFFECTS\",          # Feature -> Metric\n",
    "            \"CONVERTS_TO\",      # Segment -> Conversion\n",
    "            \"RELATES_TO\",       # Generic relationship\n",
    "        ],\n",
    "        # Enable strict mode for better structure\n",
    "        strict_mode=False,  # Allow flexibility for diverse content\n",
    "    )\n",
    "    \n",
    "    print(\"✅ LLMGraphTransformer configured successfully\")\n",
    "    print(f\"📝 Allowed node types: {len(transformer.allowed_nodes)}\")\n",
    "    print(f\"🔗 Allowed relationship types: {len(transformer.allowed_relationships)}\")\n",
    "    \n",
    "    # Display configuration details\n",
    "    print(f\"\\n📊 TRANSFORMATION CONFIGURATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"🎯 Node Types: {', '.join(transformer.allowed_nodes)}\")\n",
    "    print(f\"🔗 Relationships: {', '.join(transformer.allowed_relationships)}\")\n",
    "    print(f\"🧠 LLM Model: {llm.model_name}\")\n",
    "    print(f\"🌡️  Temperature: {llm.temperature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR configuring LLMGraphTransformer: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n🎯 READY FOR GRAPH TRANSFORMATION!\")\n",
    "print(\"The LLMGraphTransformer will now:\")\n",
    "print(\"  1. 📖 Read the enriched document content\")\n",
    "print(\"  2. 🧠 Use GPT-4 to identify entities and relationships\")\n",
    "print(\"  3. 🏗️  Create structured graph nodes with metadata properties\")\n",
    "print(\"  4. 🔗 Establish meaningful connections between entities\")\n",
    "print(\"  5. 💾 Prepare for Neo4j database population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naibc71kj8t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 9: GRAPH TRANSFORMATION AND NEO4J POPULATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🏗️  TRANSFORMING DOCUMENTS TO KNOWLEDGE GRAPH\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Clear existing data in Neo4j (optional - for clean start)\n",
    "clear_db = input(\"Clear existing Neo4j database? (y/N): \").lower().strip()\n",
    "if clear_db == 'y':\n",
    "    try:\n",
    "        neo4j_graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"🗑️  Cleared existing database\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not clear database: {e}\")\n",
    "\n",
    "# Transform documents into graph elements\n",
    "all_graph_documents = []\n",
    "transformation_summary = {\n",
    "    'total_documents': len(langchain_documents),\n",
    "    'successful_transformations': 0,\n",
    "    'total_nodes': 0,\n",
    "    'total_relationships': 0,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(f\"\\n🔄 PROCESSING {len(langchain_documents)} DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, document in enumerate(langchain_documents, 1):\n",
    "    doc_id = document.metadata['document_id']\n",
    "    test_name = document.metadata['test_name']\n",
    "    \n",
    "    print(f\"{i}. Processing {doc_id}: {test_name[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Transform single document to graph\n",
    "        graph_documents = transformer.convert_to_graph_documents([document])\n",
    "        \n",
    "        if graph_documents:\n",
    "            graph_doc = graph_documents[0]\n",
    "            \n",
    "            # Count nodes and relationships\n",
    "            node_count = len(graph_doc.nodes)\n",
    "            rel_count = len(graph_doc.relationships)\n",
    "            \n",
    "            print(f\"   ✅ Created {node_count} nodes, {rel_count} relationships\")\n",
    "            \n",
    "            # Add to collection\n",
    "            all_graph_documents.extend(graph_documents)\n",
    "            \n",
    "            # Update summary\n",
    "            transformation_summary['successful_transformations'] += 1\n",
    "            transformation_summary['total_nodes'] += node_count\n",
    "            transformation_summary['total_relationships'] += rel_count\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ⚠️  No graph elements created\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {doc_id}: {str(e)}\"\n",
    "        print(f\"   ❌ {error_msg}\")\n",
    "        transformation_summary['errors'].append(error_msg)\n",
    "\n",
    "# Populate Neo4j database\n",
    "print(f\"\\n💾 POPULATING NEO4J DATABASE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if all_graph_documents:\n",
    "    try:\n",
    "        # Add graph documents to Neo4j\n",
    "        neo4j_graph.add_graph_documents(\n",
    "            all_graph_documents,\n",
    "            baseEntityLabel=True,  # Add base Entity label to all nodes\n",
    "            include_source=True    # Include source document info\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully populated Neo4j database!\")\n",
    "        \n",
    "        # Verify population with basic queries\n",
    "        node_count = neo4j_graph.query(\"MATCH (n) RETURN count(n) as count\")[0]['count']\n",
    "        rel_count = neo4j_graph.query(\"MATCH ()-[r]->() RETURN count(r) as count\")[0]['count']\n",
    "        \n",
    "        print(f\"📊 Database population verified:\")\n",
    "        print(f\"   • Total nodes in database: {node_count}\")\n",
    "        print(f\"   • Total relationships in database: {rel_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR populating Neo4j database: {e}\")\n",
    "        transformation_summary['errors'].append(f\"Database population error: {str(e)}\")\n",
    "\n",
    "# Display transformation summary\n",
    "print(f\"\\n📈 TRANSFORMATION SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Documents processed: {transformation_summary['total_documents']}\")\n",
    "print(f\"Successful transformations: {transformation_summary['successful_transformations']}\")\n",
    "print(f\"Total nodes created: {transformation_summary['total_nodes']}\")\n",
    "print(f\"Total relationships created: {transformation_summary['total_relationships']}\")\n",
    "\n",
    "if transformation_summary['errors']:\n",
    "    print(f\"\\\\nErrors encountered: {len(transformation_summary['errors'])}\")\n",
    "    for error in transformation_summary['errors']:\n",
    "        print(f\"  • {error}\")\n",
    "else:\n",
    "    print(\"\\\\n✅ No errors encountered!\")\n",
    "\n",
    "print(f\"\\\\n🎉 KNOWLEDGE GRAPH POPULATION COMPLETE!\")\n",
    "print(\"Your Timber Mountain A/B test data is now structured as an enriched knowledge graph in Neo4j.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awp4ogpfepd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 10: GRAPH VALIDATION AND VERIFICATION QUERIES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔍 VALIDATING KNOWLEDGE GRAPH STRUCTURE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define validation queries to understand what was created\n",
    "validation_queries = {\n",
    "    \"Node Types\": \"MATCH (n) RETURN labels(n) as node_labels, count(*) as count ORDER BY count DESC\",\n",
    "    \"Relationship Types\": \"MATCH ()-[r]->() RETURN type(r) as relationship_type, count(*) as count ORDER BY count DESC\",\n",
    "    \"ABTest Nodes\": \"MATCH (n:ABTest) RETURN n.id as test_id, n.test_name as name, n.test_launch as launch_date LIMIT 10\",\n",
    "    \"Node Properties\": \"MATCH (n) WHERE labels(n) <> [] WITH labels(n)[0] as label, keys(n) as props RETURN label, collect(DISTINCT props) as properties\",\n",
    "    \"Graph Connectivity\": \"MATCH (n)-[r]->(m) RETURN labels(n)[0] as from_type, type(r) as relationship, labels(m)[0] as to_type, count(*) as frequency ORDER BY frequency DESC LIMIT 10\"\n",
    "}\n",
    "\n",
    "# Execute validation queries\n",
    "for query_name, query in validation_queries.items():\n",
    "    print(f\"\\n📊 {query_name.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        results = neo4j_graph.query(query)\n",
    "        \n",
    "        if results:\n",
    "            for result in results:\n",
    "                if query_name == \"Node Types\":\n",
    "                    labels = result['node_labels']\n",
    "                    count = result['count']\n",
    "                    print(f\"  {labels}: {count} nodes\")\n",
    "                    \n",
    "                elif query_name == \"Relationship Types\":\n",
    "                    rel_type = result['relationship_type']\n",
    "                    count = result['count']\n",
    "                    print(f\"  {rel_type}: {count} relationships\")\n",
    "                    \n",
    "                elif query_name == \"ABTest Nodes\":\n",
    "                    test_id = result.get('test_id', 'N/A')\n",
    "                    name = result.get('name', 'N/A')\n",
    "                    launch = result.get('launch_date', 'N/A')\n",
    "                    print(f\"  {test_id}: {name[:50]}... (Launch: {launch})\")\n",
    "                    \n",
    "                elif query_name == \"Node Properties\":\n",
    "                    label = result['label']\n",
    "                    props = result['properties']\n",
    "                    print(f\"  {label}: {props}\")\n",
    "                    \n",
    "                elif query_name == \"Graph Connectivity\":\n",
    "                    from_type = result['from_type']\n",
    "                    rel = result['relationship']\n",
    "                    to_type = result['to_type']\n",
    "                    freq = result['frequency']\n",
    "                    print(f\"  {from_type} -[{rel}]-> {to_type}: {freq}x\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error executing query: {e}\")\n",
    "\n",
    "# Sample GraphRAG queries to test functionality\n",
    "print(f\"\\n🎯 SAMPLE GRAPHRAG QUERIES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"name\": \"Find all A/B tests with their results\",\n",
    "        \"query\": \"MATCH (test:ABTest)-[:PRODUCED]->(result:Result) RETURN test.test_name as test, result.id as result LIMIT 5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Get conversion metrics for tests\",\n",
    "        \"query\": \"MATCH (test:ABTest)-[:MEASURED_BY]->(metric:Metric) WHERE metric.id CONTAINS 'conversion' RETURN test.test_name as test, metric.id as metric LIMIT 5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Find test insights and recommendations\",\n",
    "        \"query\": \"MATCH (test:ABTest)-[:PRODUCED]->(result:Result)-[:INDICATES]->(insight:Insight) RETURN test.test_name as test, insight.id as insight LIMIT 5\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for sample in sample_queries:\n",
    "    print(f\"\\n📋 {sample['name']}:\")\n",
    "    try:\n",
    "        results = neo4j_graph.query(sample['query'])\n",
    "        if results:\n",
    "            for result in results:\n",
    "                print(f\"  • {result}\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Query error: {e}\")\n",
    "\n",
    "# Graph schema summary\n",
    "print(f\"\\n📋 GRAPH SCHEMA SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    schema_info = neo4j_graph.get_schema\n",
    "    print(\"Node Labels and Properties:\")\n",
    "    print(schema_info)\n",
    "except:\n",
    "    print(\"Schema information not available - this is normal for some Neo4j versions\")\n",
    "\n",
    "# Final success confirmation\n",
    "print(f\"\\n🎉 KNOWLEDGE GRAPH VALIDATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Your Timber Mountain A/B test knowledge graph is ready!\")\n",
    "print(\"🔗 Neo4j database populated with enriched entities and relationships\")\n",
    "print(\"🤖 Ready for GraphRAG-powered chatbot queries\")\n",
    "print(\"📊 Structured metadata preserved as node properties\")\n",
    "\n",
    "print(f\"\\n📋 NEXT STEPS FOR CHATBOT DEVELOPMENT:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. 🔍 Build GraphRAG query system with LangChain\")\n",
    "print(\"2. 🤖 Create conversation chain for natural language queries\")\n",
    "print(\"3. 🌐 Develop Streamlit frontend interface\")\n",
    "print(\"4. 🚀 Deploy publicly accessible chatbot\")\n",
    "print(\"5. 📈 Test with complex A/B testing questions\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain AI Chatbot - Knowledge Graph Phase Complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timber_chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
