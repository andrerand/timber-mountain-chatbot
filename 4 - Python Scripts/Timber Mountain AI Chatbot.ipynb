{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21a1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\n",
      "============================================================\n",
      "✅ All libraries imported successfully!\n",
      "📅 Processing started at: 2025-07-01 06:06:47\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# TIMBER MOUNTAIN AI CHATBOT - DATA PROCESSING PIPELINE\n",
    "# ===================================================================\n",
    "# Step 1: Process and Combine Your Data\n",
    "# This notebook processes A/B test metadata and PDF presentations\n",
    "# to create a unified dataset for the Neo4j GraphRAG system.\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📅 Processing started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b42d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 LOADING A/B TEST METADATA\n",
      "----------------------------------------\n",
      "✅ Successfully loaded metadata for 5 A/B tests\n",
      "📁 Metadata file: Timber Mountain - AB Test Metadata.xlsx\n",
      "📋 Columns: ['Test Name', 'PDF File Name', 'Test Launch', 'Test End', 'Country', 'Target Segment', 'Page / Placement', 'Test Hypothesis', 'Test Result & Interpretation']\n",
      "\n",
      "🧪 A/B TEST OVERVIEW:\n",
      "----------------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   📄 PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   📅 Duration: 2024-07-08 → 2024-07-28\n",
      "   🎯 Target: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\n",
      "\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   📄 PDF: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   📅 Duration: 2024-08-05 → 2024-08-25\n",
      "   🎯 Target: Wild Willy AI Travel Planner Users\n",
      "\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   📄 PDF: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   📅 Duration: 2024-09-02 → 2024-09-22\n",
      "   🎯 Target: Visitors adding ≥ 1 non-ticket item\n",
      "\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   📄 PDF: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   📅 Duration: 2024-10-07 → 2024-10-21\n",
      "   🎯 Target: Mobile visitors\n",
      "\n",
      "5. Homepage: Special Offers Carousel — Merchandising Test\n",
      "   📄 PDF: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   📅 Duration: 2024-11-04 → 2024-11-24\n",
      "   🎯 Target: First-time visitors\n",
      "\n",
      "🔍 Created metadata lookup for 5 tests\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: LOAD AND ANALYZE A/B TEST METADATA\n",
    "# ===================================================================\n",
    "\n",
    "# Define file paths\n",
    "project_root = Path(\"..\")\n",
    "metadata_path = project_root / \"2 - Synthetic Metadata\" / \"Timber Mountain - AB Test Metadata.xlsx\"\n",
    "pdf_directory = project_root / \"3 - Synthetic A:B Test Results Decks\" / \"2 - Results Presentations\"\n",
    "\n",
    "print(\"📊 LOADING A/B TEST METADATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Load metadata from Excel file\n",
    "    metadata_df = pd.read_excel(metadata_path, sheet_name='Metadata')\n",
    "    \n",
    "    print(f\"✅ Successfully loaded metadata for {len(metadata_df)} A/B tests\")\n",
    "    print(f\"📁 Metadata file: {metadata_path.name}\")\n",
    "    print(f\"📋 Columns: {list(metadata_df.columns)}\")\n",
    "    \n",
    "    # Display summary of tests\n",
    "    print(f\"\\n🧪 A/B TEST OVERVIEW:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        print(f\"{i+1}. {row['Test Name']}\")\n",
    "        print(f\"   📄 PDF: {row['PDF File Name']}\")\n",
    "        print(f\"   📅 Duration: {row['Test Launch'].strftime('%Y-%m-%d')} → {row['Test End'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   🎯 Target: {row['Target Segment']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create metadata lookup dictionary for efficient access\n",
    "    metadata_lookup = {}\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        pdf_filename = row['PDF File Name']\n",
    "        metadata_lookup[pdf_filename] = {\n",
    "            'test_name': row['Test Name'],\n",
    "            'test_launch': row['Test Launch'].strftime('%Y-%m-%d'),\n",
    "            'test_end': row['Test End'].strftime('%Y-%m-%d'),\n",
    "            'country': row['Country'],\n",
    "            'target_segment': row['Target Segment'],\n",
    "            'page_placement': row['Page / Placement'],\n",
    "            'test_hypothesis': row['Test Hypothesis'],\n",
    "            'test_result': row['Test Result & Interpretation']\n",
    "        }\n",
    "    \n",
    "    print(f\"🔍 Created metadata lookup for {len(metadata_lookup)} tests\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Metadata file not found at {metadata_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "h26wg3rx0si",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 PROCESSING PDF FILES\n",
      "----------------------------------------\n",
      "📂 Found 5 PDF files to process:\n",
      "   - 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   - 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   - 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   - 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   - 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "\n",
      "🔍 EXTRACTING TEXT FROM PDF FILES:\n",
      "----------------------------------------\n",
      "Processing: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Extracted 2,992 characters from 7 pages\n",
      "Processing: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Extracted 4,031 characters from 9 pages\n",
      "Processing: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Extracted 4,595 characters from 10 pages\n",
      "Processing: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Extracted 4,244 characters from 10 pages\n",
      "Processing: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Extracted 5,169 characters from 10 pages\n",
      "\n",
      "📊 PDF PROCESSING SUMMARY:\n",
      "   • Successfully processed: 5\n",
      "   • Failed to process: 0\n",
      "   • Total text extracted: 21,031 characters\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 2: PROCESS PDF FILES AND EXTRACT TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n📄 PROCESSING PDF FILES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find all PDF files in the results presentations directory\n",
    "pdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"❌ ERROR: No PDF files found in {pdf_directory}\")\n",
    "    raise FileNotFoundError(\"PDF files not found\")\n",
    "\n",
    "print(f\"📂 Found {len(pdf_files)} PDF files to process:\")\n",
    "for pdf_file in sorted(pdf_files):\n",
    "    print(f\"   - {pdf_file.name}\")\n",
    "\n",
    "# Dictionary to store extracted text content\n",
    "pdf_text_content = {}\n",
    "\n",
    "print(f\"\\n🔍 EXTRACTING TEXT FROM PDF FILES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for pdf_path in sorted(pdf_files):\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract all text from the PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Extract text from all pages\n",
    "            all_pages_text = []\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n{page_text}\")\n",
    "                else:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n[No text extracted]\")\n",
    "            \n",
    "            # Combine all pages\n",
    "            full_text = \"\\n\\n\".join(all_pages_text)\n",
    "            \n",
    "            # Store the text content\n",
    "            pdf_text_content[pdf_path.name] = {\n",
    "                'file_path': str(pdf_path),\n",
    "                'page_count': len(pdf.pages),\n",
    "                'text_length': len(full_text),\n",
    "                'full_text': full_text\n",
    "            }\n",
    "            \n",
    "            print(f\"   ✅ Extracted {len(full_text):,} characters from {len(pdf.pages)} pages\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERROR processing {pdf_path.name}: {e}\")\n",
    "        pdf_text_content[pdf_path.name] = {\n",
    "            'file_path': str(pdf_path),\n",
    "            'page_count': 0,\n",
    "            'text_length': 0,\n",
    "            'full_text': f\"[ERROR: Could not extract text - {e}]\",\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(f\"\\n📊 PDF PROCESSING SUMMARY:\")\n",
    "print(f\"   • Successfully processed: {len([k for k, v in pdf_text_content.items() if 'error' not in v])}\")\n",
    "print(f\"   • Failed to process: {len([k for k, v in pdf_text_content.items() if 'error' in v])}\")\n",
    "print(f\"   • Total text extracted: {sum(v['text_length'] for v in pdf_text_content.values()):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2apak0qhg45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 COMBINING METADATA WITH PDF CONTENT\n",
      "----------------------------------------\n",
      "Combining data for: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Metadata found: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   ✅ PDF content: 2,992 characters\n",
      "\n",
      "Combining data for: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Metadata found: AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   ✅ PDF content: 4,031 characters\n",
      "\n",
      "Combining data for: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Metadata found: Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   ✅ PDF content: 4,595 characters\n",
      "\n",
      "Combining data for: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Metadata found: Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   ✅ PDF content: 4,244 characters\n",
      "\n",
      "Combining data for: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Metadata found: Homepage: Special Offers Carousel — Merchandising Test\n",
      "   ✅ PDF content: 5,169 characters\n",
      "\n",
      "📊 COMBINATION SUMMARY:\n",
      "   • Total documents processed: 5\n",
      "   • Successful metadata matches: 5\n",
      "   • Missing metadata: 0\n",
      "   • PDF extraction failures: 0\n",
      "   • Unified documents created: 5\n",
      "\n",
      "📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\n",
      "----------------------------------------\n",
      "Document ID: timber_mountain_001\n",
      "Source PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "Test Name: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "Content Length: === PAGE 1 ===\n",
      "Locale-Aware Experience: How\n",
      "We Boosted International\n",
      "Conversions at Timber\n",
      "Mountain\n",
      "...\n",
      "Metadata Keys: ['test_name', 'test_launch', 'test_end', 'country', 'target_segment', 'page_placement', 'test_hypothesis', 'test_result']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 3: COMBINE METADATA WITH PDF TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔗 COMBINING METADATA WITH PDF CONTENT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "unified_documents = []\n",
    "processing_summary = {\n",
    "    'successful_matches': 0,\n",
    "    'missing_metadata': 0,\n",
    "    'missing_pdf_content': 0,\n",
    "    'total_processed': 0\n",
    "}\n",
    "\n",
    "# Iterate through each PDF file and combine with metadata\n",
    "for pdf_filename in pdf_text_content.keys():\n",
    "    print(f\"Combining data for: {pdf_filename}\")\n",
    "    \n",
    "    # Get metadata for this PDF\n",
    "    metadata = metadata_lookup.get(pdf_filename, {})\n",
    "    \n",
    "    # Get PDF text content\n",
    "    pdf_data = pdf_text_content.get(pdf_filename, {})\n",
    "    \n",
    "    # Create unified document\n",
    "    unified_doc = {\n",
    "        'document_id': f\"timber_mountain_{len(unified_documents) + 1:03d}\",\n",
    "        'source_pdf_filename': pdf_filename,\n",
    "        'source_pdf_path': pdf_data.get('file_path', ''),\n",
    "        'pdf_processing': {\n",
    "            'page_count': pdf_data.get('page_count', 0),\n",
    "            'text_length': pdf_data.get('text_length', 0),\n",
    "            'extraction_error': pdf_data.get('error', None)\n",
    "        },\n",
    "        'metadata': metadata,\n",
    "        'content': {\n",
    "            'full_text': pdf_data.get('full_text', ''),\n",
    "            'processed_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Update processing summary\n",
    "    processing_summary['total_processed'] += 1\n",
    "    \n",
    "    if metadata:\n",
    "        processing_summary['successful_matches'] += 1\n",
    "        print(f\"   ✅ Metadata found: {metadata.get('test_name', 'Unknown')}\")\n",
    "    else:\n",
    "        processing_summary['missing_metadata'] += 1\n",
    "        print(f\"   ⚠️  No metadata found for {pdf_filename}\")\n",
    "    \n",
    "    if pdf_data.get('full_text') and 'error' not in pdf_data:\n",
    "        print(f\"   ✅ PDF content: {pdf_data['text_length']:,} characters\")\n",
    "    else:\n",
    "        processing_summary['missing_pdf_content'] += 1\n",
    "        print(f\"   ⚠️  PDF content extraction failed\")\n",
    "    \n",
    "    unified_documents.append(unified_doc)\n",
    "    print()\n",
    "\n",
    "print(f\"📊 COMBINATION SUMMARY:\")\n",
    "print(f\"   • Total documents processed: {processing_summary['total_processed']}\")\n",
    "print(f\"   • Successful metadata matches: {processing_summary['successful_matches']}\")\n",
    "print(f\"   • Missing metadata: {processing_summary['missing_metadata']}\")\n",
    "print(f\"   • PDF extraction failures: {processing_summary['missing_pdf_content']}\")\n",
    "print(f\"   • Unified documents created: {len(unified_documents)}\")\n",
    "\n",
    "# Display sample unified document structure\n",
    "if unified_documents:\n",
    "    print(f\"\\n📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    sample_doc = unified_documents[0]\n",
    "    print(f\"Document ID: {sample_doc['document_id']}\")\n",
    "    print(f\"Source PDF: {sample_doc['source_pdf_filename']}\")\n",
    "    print(f\"Test Name: {sample_doc['metadata'].get('test_name', 'N/A')}\")\n",
    "    print(f\"Content Length: {sample_doc['content']['full_text'][:100]}...\")\n",
    "    print(f\"Metadata Keys: {list(sample_doc['metadata'].keys()) if sample_doc['metadata'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12r7oth4vzba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 GENERATING UNIFIED JSON OUTPUT\n",
      "----------------------------------------\n",
      "✅ Successfully saved unified dataset!\n",
      "📁 Output file: processed_documents.json\n",
      "📏 File size: 0.03 MB\n",
      "📊 Contains 5 documents\n",
      "\n",
      "📋 JSON STRUCTURE SUMMARY:\n",
      "----------------------------------------\n",
      "└── processing_metadata\n",
      "    ├── created_timestamp\n",
      "    ├── source_excel_file\n",
      "    ├── source_pdf_directory\n",
      "    ├── total_documents\n",
      "    └── processing_summary\n",
      "└── documents (array)\n",
      "    └── [document]\n",
      "        ├── document_id\n",
      "        ├── source_pdf_filename\n",
      "        ├── source_pdf_path\n",
      "        ├── pdf_processing\n",
      "        ├── metadata\n",
      "        └── content\n",
      "\n",
      "🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\n",
      "📝 Use 'processed_documents.json' as input for graph population\n",
      "\n",
      "📄 SAMPLE JSON OUTPUT:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"processing_metadata\": {\n",
      "    \"created_timestamp\": \"2025-07-01T06:16:03.658141\",\n",
      "    \"source_excel_file\": \"../2 - Synthetic Metadata/Timber Mountain - AB Test Metadata.xlsx\",\n",
      "    \"source_pdf_directory\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations\",\n",
      "    \"total_documents\": 5,\n",
      "    \"processing_summary\": {\n",
      "      \"successful_matches\": 5,\n",
      "      \"missing_metadata\": 0,\n",
      "      \"missing_pdf_content\": 0,\n",
      "      \"total_processed\": 5\n",
      "    }\n",
      "  },\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"document_id\": \"timber_mountain_001\",\n",
      "      \"source_pdf_filename\": \"1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"source_pdf_path\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations/1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"pdf_processing\": {\n",
      "        \"page_count\": 7,\n",
      "        \"text_length\": 2992,\n",
      "        \"extraction_error\": null\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"test_name\": \"...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 4: GENERATE UNIFIED JSON OUTPUT FOR NEO4J GRAPHRAG\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n💾 GENERATING UNIFIED JSON OUTPUT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define output path\n",
    "output_json_path = project_root / \"processed_documents.json\"\n",
    "\n",
    "# Create the final output structure optimized for GraphRAG\n",
    "final_output = {\n",
    "    'processing_metadata': {\n",
    "        'created_timestamp': datetime.now().isoformat(),\n",
    "        'source_excel_file': str(metadata_path),\n",
    "        'source_pdf_directory': str(pdf_directory),\n",
    "        'total_documents': len(unified_documents),\n",
    "        'processing_summary': processing_summary\n",
    "    },\n",
    "    'documents': unified_documents\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Save to JSON file with proper formatting\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Calculate file size\n",
    "    file_size_mb = output_json_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✅ Successfully saved unified dataset!\")\n",
    "    print(f\"📁 Output file: {output_json_path.name}\")\n",
    "    print(f\"📏 File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"📊 Contains {len(unified_documents)} documents\")\n",
    "    \n",
    "    # Display JSON structure summary\n",
    "    print(f\"\\n📋 JSON STRUCTURE SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"└── processing_metadata\")\n",
    "    print(\"    ├── created_timestamp\")\n",
    "    print(\"    ├── source_excel_file\") \n",
    "    print(\"    ├── source_pdf_directory\")\n",
    "    print(\"    ├── total_documents\")\n",
    "    print(\"    └── processing_summary\")\n",
    "    print(\"└── documents (array)\")\n",
    "    print(\"    └── [document]\")\n",
    "    print(\"        ├── document_id\")\n",
    "    print(\"        ├── source_pdf_filename\")\n",
    "    print(\"        ├── source_pdf_path\")\n",
    "    print(\"        ├── pdf_processing\")\n",
    "    print(\"        ├── metadata\")\n",
    "    print(\"        └── content\")\n",
    "    \n",
    "    print(f\"\\n🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\")\n",
    "    print(f\"📝 Use '{output_json_path.name}' as input for graph population\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR saving JSON file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display sample of final JSON structure\n",
    "print(f\"\\n📄 SAMPLE JSON OUTPUT:\")\n",
    "print(\"-\" * 40)\n",
    "sample_output = {\n",
    "    'processing_metadata': final_output['processing_metadata'],\n",
    "    'documents': [final_output['documents'][0]] if final_output['documents'] else []\n",
    "}\n",
    "\n",
    "print(json.dumps(sample_output, indent=2)[:1000] + \"...\" if len(str(sample_output)) > 1000 else json.dumps(sample_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d72gx3i5aw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DATA VALIDATION AND FINAL SUMMARY\n",
      "============================================================\n",
      "🔍 VALIDATION RESULTS:\n",
      "------------------------------\n",
      "✅ All A/B tests processed: True (5/5)\n",
      "✅ All metadata matched: True (5/5)\n",
      "✅ All PDFs extracted: True (failures: 0)\n",
      "✅ Output file created: True (processed_documents.json)\n",
      "\n",
      "📊 CONTENT STATISTICS:\n",
      "------------------------------\n",
      "Total text content: 21,031 characters\n",
      "Average per document: 4,206 characters\n",
      "\n",
      "📋 DOCUMENT-LEVEL BREAKDOWN:\n",
      "------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Co...\n",
      "   📄 7 pages, 2,992 characters\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Ad...\n",
      "   📄 9 pages, 4,031 characters\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamle...\n",
      "   📄 10 pages, 4,595 characters\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More...\n",
      "   📄 10 pages, 4,244 characters\n",
      "5. Homepage: Special Offers Carousel — Merchandising ...\n",
      "   📄 10 pages, 5,169 characters\n",
      "\n",
      "⚠️  POTENTIAL ISSUES:\n",
      "------------------------------\n",
      "✅ No issues detected - data quality looks good!\n",
      "\n",
      "🎯 PROCESSING COMPLETE!\n",
      "============================================================\n",
      "📈 Successfully processed 5 A/B test documents\n",
      "💾 Output saved to: ../processed_documents.json\n",
      "🔗 Ready for Neo4j graph database population\n",
      "📅 Processing completed at: 2025-07-01 06:17:28\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "------------------------------\n",
      "1. 🗄️  Set up Neo4j database connection\n",
      "2. 🏗️  Create graph schema for A/B test data\n",
      "3. 📥 Import processed_documents.json into Neo4j\n",
      "4. 🔍 Build GraphRAG query system\n",
      "5. 🤖 Integrate with LangChain for chatbot responses\n",
      "6. 🌐 Deploy Streamlit frontend\n",
      "\n",
      "🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 5: DATA VALIDATION AND SUMMARY STATISTICS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n✅ DATA VALIDATION AND FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validation checks\n",
    "validation_results = {\n",
    "    'all_tests_processed': len(unified_documents) == len(metadata_df),\n",
    "    'all_metadata_matched': processing_summary['missing_metadata'] == 0,\n",
    "    'all_pdfs_extracted': processing_summary['missing_pdf_content'] == 0,\n",
    "    'output_file_created': output_json_path.exists(),\n",
    "    'total_characters': sum(len(doc['content']['full_text']) for doc in unified_documents),\n",
    "    'avg_characters_per_doc': 0\n",
    "}\n",
    "\n",
    "if len(unified_documents) > 0:\n",
    "    validation_results['avg_characters_per_doc'] = validation_results['total_characters'] / len(unified_documents)\n",
    "\n",
    "# Display validation results\n",
    "print(\"🔍 VALIDATION RESULTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"✅ All A/B tests processed: {validation_results['all_tests_processed']} ({len(unified_documents)}/{len(metadata_df)})\")\n",
    "print(f\"✅ All metadata matched: {validation_results['all_metadata_matched']} ({processing_summary['successful_matches']}/{len(unified_documents)})\")\n",
    "print(f\"✅ All PDFs extracted: {validation_results['all_pdfs_extracted']} (failures: {processing_summary['missing_pdf_content']})\")\n",
    "print(f\"✅ Output file created: {validation_results['output_file_created']} ({output_json_path.name})\")\n",
    "\n",
    "print(f\"\\n📊 CONTENT STATISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total text content: {validation_results['total_characters']:,} characters\")\n",
    "print(f\"Average per document: {validation_results['avg_characters_per_doc']:,.0f} characters\")\n",
    "\n",
    "# Document-level statistics\n",
    "print(f\"\\n📋 DOCUMENT-LEVEL BREAKDOWN:\")\n",
    "print(\"-\" * 30)\n",
    "for i, doc in enumerate(unified_documents, 1):\n",
    "    test_name = doc['metadata'].get('test_name', 'Unknown Test')\n",
    "    char_count = len(doc['content']['full_text'])\n",
    "    page_count = doc['pdf_processing']['page_count']\n",
    "    \n",
    "    print(f\"{i}. {test_name[:50]}{'...' if len(test_name) > 50 else ''}\")\n",
    "    print(f\"   📄 {page_count} pages, {char_count:,} characters\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(f\"\\n⚠️  POTENTIAL ISSUES:\")\n",
    "print(\"-\" * 30)\n",
    "issues_found = 0\n",
    "\n",
    "for doc in unified_documents:\n",
    "    if doc['pdf_processing'].get('extraction_error'):\n",
    "        print(f\"❌ PDF extraction error in: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if not doc['metadata']:\n",
    "        print(f\"⚠️  Missing metadata for: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if len(doc['content']['full_text']) < 100:\n",
    "        print(f\"⚠️  Very short content in: {doc['source_pdf_filename']} ({len(doc['content']['full_text'])} chars)\")\n",
    "        issues_found += 1\n",
    "\n",
    "if issues_found == 0:\n",
    "    print(\"✅ No issues detected - data quality looks good!\")\n",
    "\n",
    "# Final processing summary\n",
    "print(f\"\\n🎯 PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📈 Successfully processed {len(unified_documents)} A/B test documents\")\n",
    "print(f\"💾 Output saved to: {output_json_path}\")\n",
    "print(f\"🔗 Ready for Neo4j graph database population\")\n",
    "print(f\"📅 Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Next steps guidance\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. 🗄️  Set up Neo4j database connection\")\n",
    "print(\"2. 🏗️  Create graph schema for A/B test data\")\n",
    "print(\"3. 📥 Import processed_documents.json into Neo4j\")\n",
    "print(\"4. 🔍 Build GraphRAG query system\")\n",
    "print(\"5. 🤖 Integrate with LangChain for chatbot responses\")\n",
    "print(f\"6. 🌐 Deploy Streamlit frontend\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "uwkee03yqt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 NEO4J KNOWLEDGE GRAPH POPULATION\n",
      "============================================================\n",
      "🎯 Step 2: Transform documents into enriched knowledge graph\n",
      "✅ Environment variables loaded successfully\n",
      "✅ Neo4j connection established: Connection successful\n",
      "📊 Neo4j Kernel: 5.27-aura\n",
      "📊 Cypher: 5\n",
      "\n",
      "🔧 SETUP COMPLETE - Ready for graph transformation!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 6: ENVIRONMENT SETUP AND NEO4J CONNECTION\n",
    "# ===================================================================\n",
    "# Step 2: Populate the Enriched Neo4j Knowledge Graph\n",
    "# This section uses LangChain's LLMGraphTransformer to automatically\n",
    "# create an enriched knowledge graph from our processed documents.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n🔗 NEO4J KNOWLEDGE GRAPH POPULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 Step 2: Transform documents into enriched knowledge graph\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required environment variables\n",
    "required_env_vars = ['OPENAI_API_KEY', 'NEO4J_URI', 'NEO4J_USERNAME', 'NEO4J_PASSWORD']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"❌ ERROR: Missing environment variables: {missing_vars}\")\n",
    "    print(\"Please ensure your .env file contains:\")\n",
    "    print(\"  - OPENAI_API_KEY=your_openai_key\")\n",
    "    print(\"  - NEO4J_URI=neo4j+s://your_neo4j_uri\")\n",
    "    print(\"  - NEO4J_USERNAME=your_username\")\n",
    "    print(\"  - NEO4J_PASSWORD=your_password\")\n",
    "    raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "print(\"✅ Environment variables loaded successfully\")\n",
    "\n",
    "# Test Neo4j connection\n",
    "try:\n",
    "    neo4j_graph = Neo4jGraph(\n",
    "        url=os.getenv('NEO4J_URI'),\n",
    "        username=os.getenv('NEO4J_USERNAME'),\n",
    "        password=os.getenv('NEO4J_PASSWORD')\n",
    "    )\n",
    "    \n",
    "    # Test the connection\n",
    "    result = neo4j_graph.query(\"RETURN 'Connection successful' as message\")\n",
    "    print(f\"✅ Neo4j connection established: {result[0]['message']}\")\n",
    "    \n",
    "    # Get database info\n",
    "    db_info = neo4j_graph.query(\"CALL dbms.components() YIELD name, versions RETURN name, versions[0] as version\")\n",
    "    for info in db_info:\n",
    "        print(f\"📊 {info['name']}: {info['version']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR connecting to Neo4j: {e}\")\n",
    "    print(\"Please verify your Neo4j credentials and connection string\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n🔧 SETUP COMPLETE - Ready for graph transformation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bwd960h1koe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 LOADING PROCESSED DOCUMENTS\n",
      "----------------------------------------\n",
      "✅ Loaded processed documents from: processed_documents.json\n",
      "📊 Processing metadata: 2025-07-01T06:16:03.658141\n",
      "📈 Total documents: 5\n",
      "\n",
      "🔄 CONVERTING TO LANGCHAIN DOCUMENTS:\n",
      "----------------------------------------\n",
      "✅ timber_mountain_001: Homepage: Domestic vs. International Visitors — Co...\n",
      "   📝 Content length: 3,839 characters\n",
      "✅ timber_mountain_002: AI Planner: Add Verified Star Ratings — Trust & Ad...\n",
      "   📝 Content length: 4,802 characters\n",
      "✅ timber_mountain_003: Checkout: Unified Booking.com Bundle Flow — Seamle...\n",
      "   📝 Content length: 5,408 characters\n",
      "✅ timber_mountain_004: Site-wide CTA Copy: “Learn More” vs. “Explore More...\n",
      "   📝 Content length: 4,931 characters\n",
      "✅ timber_mountain_005: Homepage: Special Offers Carousel — Merchandising ...\n",
      "   📝 Content length: 5,939 characters\n",
      "\n",
      "📋 DOCUMENT PREPARATION SUMMARY:\n",
      "   • LangChain documents created: 5\n",
      "   • Total content for transformation: 24,919 characters\n",
      "   • Ready for LLMGraphTransformer processing\n",
      "\n",
      "📄 SAMPLE ENRICHED CONTENT STRUCTURE:\n",
      "----------------------------------------\n",
      "Document ID: timber_mountain_001\n",
      "Content preview: \n",
      "A/B TEST: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "\n",
      "METADATA:\n",
      "- Document ID: timber_mountain_001\n",
      "- Test Launch Date: 2024-07-08\n",
      "- Test End Date: 2024-07-28\n",
      "- Country: Global\n",
      "- Target Segment: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\n",
      "- Page/Placement...\n",
      "Metadata keys: ['document_id', 'source_pdf', 'page_count', 'text_length', 'test_name', 'test_launch', 'test_end', 'country', 'target_segment', 'page_placement', 'test_hypothesis', 'test_result']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 7: LOAD AND PREPARE PROCESSED DOCUMENTS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n📂 LOADING PROCESSED DOCUMENTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the processed documents JSON\n",
    "try:\n",
    "    with open(output_json_path, 'r', encoding='utf-8') as f:\n",
    "        processed_data = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Loaded processed documents from: {output_json_path.name}\")\n",
    "    print(f\"📊 Processing metadata: {processed_data['processing_metadata']['created_timestamp']}\")\n",
    "    print(f\"📈 Total documents: {processed_data['processing_metadata']['total_documents']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: {output_json_path.name} not found. Please run the data processing steps first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR loading processed documents: {e}\")\n",
    "    raise\n",
    "\n",
    "# Convert to LangChain Document objects with enriched metadata\n",
    "langchain_documents = []\n",
    "\n",
    "print(f\"\\n🔄 CONVERTING TO LANGCHAIN DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for doc_data in processed_data['documents']:\n",
    "    # Create metadata dictionary that includes both structured and processing info\n",
    "    metadata = {\n",
    "        'document_id': doc_data['document_id'],\n",
    "        'source_pdf': doc_data['source_pdf_filename'],\n",
    "        'page_count': doc_data['pdf_processing']['page_count'],\n",
    "        'text_length': doc_data['pdf_processing']['text_length'],\n",
    "        # Structured metadata from Excel\n",
    "        'test_name': doc_data['metadata'].get('test_name', ''),\n",
    "        'test_launch': doc_data['metadata'].get('test_launch', ''),\n",
    "        'test_end': doc_data['metadata'].get('test_end', ''),\n",
    "        'country': doc_data['metadata'].get('country', ''),\n",
    "        'target_segment': doc_data['metadata'].get('target_segment', ''),\n",
    "        'page_placement': doc_data['metadata'].get('page_placement', ''),\n",
    "        'test_hypothesis': doc_data['metadata'].get('test_hypothesis', ''),\n",
    "        'test_result': doc_data['metadata'].get('test_result', ''),\n",
    "    }\n",
    "    \n",
    "    # Create enriched content that includes both text and structured data\n",
    "    # This is the key insight: LLMGraphTransformer will see both unstructured text\n",
    "    # and structured metadata, allowing it to create nodes with rich properties\n",
    "    enriched_content = f\"\"\"\n",
    "A/B TEST: {metadata['test_name']}\n",
    "\n",
    "METADATA:\n",
    "- Document ID: {metadata['document_id']}\n",
    "- Test Launch Date: {metadata['test_launch']}\n",
    "- Test End Date: {metadata['test_end']}\n",
    "- Country: {metadata['country']}\n",
    "- Target Segment: {metadata['target_segment']}\n",
    "- Page/Placement: {metadata['page_placement']}\n",
    "- Test Hypothesis: {metadata['test_hypothesis']}\n",
    "- Test Result: {metadata['test_result']}\n",
    "\n",
    "FULL PRESENTATION CONTENT:\n",
    "{doc_data['content']['full_text']}\n",
    "\"\"\"\n",
    "    \n",
    "    # Create LangChain Document\n",
    "    langchain_doc = Document(\n",
    "        page_content=enriched_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    \n",
    "    langchain_documents.append(langchain_doc)\n",
    "    \n",
    "    print(f\"✅ {metadata['document_id']}: {metadata['test_name'][:50]}...\")\n",
    "    print(f\"   📝 Content length: {len(enriched_content):,} characters\")\n",
    "\n",
    "print(f\"\\n📋 DOCUMENT PREPARATION SUMMARY:\")\n",
    "print(f\"   • LangChain documents created: {len(langchain_documents)}\")\n",
    "print(f\"   • Total content for transformation: {sum(len(doc.page_content) for doc in langchain_documents):,} characters\")\n",
    "print(f\"   • Ready for LLMGraphTransformer processing\")\n",
    "\n",
    "# Display sample of enriched content structure\n",
    "if langchain_documents:\n",
    "    sample_doc = langchain_documents[0]\n",
    "    print(f\"\\n📄 SAMPLE ENRICHED CONTENT STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Document ID: {sample_doc.metadata['document_id']}\")\n",
    "    print(f\"Content preview: {sample_doc.page_content[:300]}...\")\n",
    "    print(f\"Metadata keys: {list(sample_doc.metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "jiw492z0ona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 CONFIGURING LLMGRAPHTRANSFORMER\n",
      "----------------------------------------\n",
      "✅ OpenAI LLM initialized successfully\n",
      "✅ LLMGraphTransformer configured successfully\n",
      "📝 Allowed node types: 11\n",
      "🔗 Allowed relationship types: 11\n",
      "\n",
      "📊 TRANSFORMATION CONFIGURATION:\n",
      "----------------------------------------\n",
      "🎯 Node Types: ABTest, Metric, Segment, Feature, Result, Hypothesis, Page, Variant, Conversion, Insight, Recommendation\n",
      "🔗 Relationships: TESTED_ON, MEASURED_BY, TARGETS, HAS_VARIANT, PRODUCED, VALIDATES, INDICATES, SUGGESTS, AFFECTS, CONVERTS_TO, RELATES_TO\n",
      "🧠 LLM Model: gpt-4o-mini\n",
      "🌡️  Temperature: 0.0\n",
      "\n",
      "🎯 READY FOR GRAPH TRANSFORMATION!\n",
      "The LLMGraphTransformer will now:\n",
      "  1. 📖 Read the enriched document content\n",
      "  2. 🧠 Use GPT-4 to identify entities and relationships\n",
      "  3. 🏗️  Create structured graph nodes with metadata properties\n",
      "  4. 🔗 Establish meaningful connections between entities\n",
      "  5. 💾 Prepare for Neo4j database population\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 8: CONFIGURE LLMGRAPHTRANSFORMER\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🤖 CONFIGURING LLMGRAPHTRANSFORMER\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialize OpenAI LLM for graph transformation\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",  # Using cost-effective model for graph extraction\n",
    "        temperature=0,        # Deterministic output for consistent graph structure\n",
    "        api_key=os.getenv('OPENAI_API_KEY')\n",
    "    )\n",
    "    print(\"✅ OpenAI LLM initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR initializing OpenAI LLM: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configure LLMGraphTransformer with specific node types and relationships\n",
    "# This will help the transformer understand the A/B testing domain\n",
    "try:\n",
    "    transformer = LLMGraphTransformer(\n",
    "        llm=llm,\n",
    "        # Define allowed node labels - these guide the LLM to create structured nodes\n",
    "        allowed_nodes=[\n",
    "            \"ABTest\",           # Main A/B test entities\n",
    "            \"Metric\",           # KPIs and measurements\n",
    "            \"Segment\",          # Target segments and audiences\n",
    "            \"Feature\",          # Features being tested\n",
    "            \"Result\",           # Test outcomes and findings\n",
    "            \"Hypothesis\",       # Test hypotheses\n",
    "            \"Page\",             # Web pages/placements\n",
    "            \"Variant\",          # Test variants (control/treatment)\n",
    "            \"Conversion\",       # Conversion events\n",
    "            \"Insight\",          # Key insights and learnings\n",
    "            \"Recommendation\"    # Strategic recommendations\n",
    "        ],\n",
    "        # Define relationship types for connecting entities\n",
    "        allowed_relationships=[\n",
    "            \"TESTED_ON\",        # ABTest -> Page\n",
    "            \"MEASURED_BY\",      # ABTest -> Metric\n",
    "            \"TARGETS\",          # ABTest -> Segment\n",
    "            \"HAS_VARIANT\",      # ABTest -> Variant\n",
    "            \"PRODUCED\",         # ABTest -> Result\n",
    "            \"VALIDATES\",        # Result -> Hypothesis\n",
    "            \"INDICATES\",        # Result -> Insight\n",
    "            \"SUGGESTS\",         # Insight -> Recommendation\n",
    "            \"AFFECTS\",          # Feature -> Metric\n",
    "            \"CONVERTS_TO\",      # Segment -> Conversion\n",
    "            \"RELATES_TO\",       # Generic relationship\n",
    "        ],\n",
    "        # Enable strict mode for better structure\n",
    "        strict_mode=False,  # Allow flexibility for diverse content\n",
    "    )\n",
    "    \n",
    "    print(\"✅ LLMGraphTransformer configured successfully\")\n",
    "    print(f\"📝 Allowed node types: {len(transformer.allowed_nodes)}\")\n",
    "    print(f\"🔗 Allowed relationship types: {len(transformer.allowed_relationships)}\")\n",
    "    \n",
    "    # Display configuration details\n",
    "    print(f\"\\n📊 TRANSFORMATION CONFIGURATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"🎯 Node Types: {', '.join(transformer.allowed_nodes)}\")\n",
    "    print(f\"🔗 Relationships: {', '.join(transformer.allowed_relationships)}\")\n",
    "    print(f\"🧠 LLM Model: {llm.model_name}\")\n",
    "    print(f\"🌡️  Temperature: {llm.temperature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR configuring LLMGraphTransformer: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n🎯 READY FOR GRAPH TRANSFORMATION!\")\n",
    "print(\"The LLMGraphTransformer will now:\")\n",
    "print(\"  1. 📖 Read the enriched document content\")\n",
    "print(\"  2. 🧠 Use GPT-4 to identify entities and relationships\")\n",
    "print(\"  3. 🏗️  Create structured graph nodes with metadata properties\")\n",
    "print(\"  4. 🔗 Establish meaningful connections between entities\")\n",
    "print(\"  5. 💾 Prepare for Neo4j database population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "naibc71kj8t",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏗️  TRANSFORMING DOCUMENTS TO KNOWLEDGE GRAPH\n",
      "----------------------------------------\n",
      "\n",
      "🔄 PROCESSING 5 DOCUMENTS:\n",
      "----------------------------------------\n",
      "1. Processing timber_mountain_001: Homepage: Domestic vs. International Visitors — Co...\n",
      "   ✅ Created 7 nodes, 6 relationships\n",
      "2. Processing timber_mountain_002: AI Planner: Add Verified Star Ratings — Trust & Ad...\n",
      "   ✅ Created 14 nodes, 11 relationships\n",
      "3. Processing timber_mountain_003: Checkout: Unified Booking.com Bundle Flow — Seamle...\n",
      "   ✅ Created 10 nodes, 9 relationships\n",
      "4. Processing timber_mountain_004: Site-wide CTA Copy: “Learn More” vs. “Explore More...\n",
      "   ✅ Created 12 nodes, 11 relationships\n",
      "5. Processing timber_mountain_005: Homepage: Special Offers Carousel — Merchandising ...\n",
      "   ✅ Created 13 nodes, 12 relationships\n",
      "\n",
      "💾 POPULATING NEO4J DATABASE:\n",
      "----------------------------------------\n",
      "✅ Successfully populated Neo4j database!\n",
      "📊 Database population verified:\n",
      "   • Total nodes in database: 58\n",
      "   • Total relationships in database: 105\n",
      "\n",
      "📈 TRANSFORMATION SUMMARY:\n",
      "========================================\n",
      "Documents processed: 5\n",
      "Successful transformations: 5\n",
      "Total nodes created: 56\n",
      "Total relationships created: 49\n",
      "\\n✅ No errors encountered!\n",
      "\\n🎉 KNOWLEDGE GRAPH POPULATION COMPLETE!\n",
      "Your Timber Mountain A/B test data is now structured as an enriched knowledge graph in Neo4j.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 9: GRAPH TRANSFORMATION AND NEO4J POPULATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🏗️  TRANSFORMING DOCUMENTS TO KNOWLEDGE GRAPH\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Clear existing data in Neo4j (optional - for clean start)\n",
    "clear_db = input(\"Clear existing Neo4j database? (y/N): \").lower().strip()\n",
    "if clear_db == 'y':\n",
    "    try:\n",
    "        neo4j_graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"🗑️  Cleared existing database\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not clear database: {e}\")\n",
    "\n",
    "# Transform documents into graph elements\n",
    "all_graph_documents = []\n",
    "transformation_summary = {\n",
    "    'total_documents': len(langchain_documents),\n",
    "    'successful_transformations': 0,\n",
    "    'total_nodes': 0,\n",
    "    'total_relationships': 0,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(f\"\\n🔄 PROCESSING {len(langchain_documents)} DOCUMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, document in enumerate(langchain_documents, 1):\n",
    "    doc_id = document.metadata['document_id']\n",
    "    test_name = document.metadata['test_name']\n",
    "    \n",
    "    print(f\"{i}. Processing {doc_id}: {test_name[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Transform single document to graph\n",
    "        graph_documents = transformer.convert_to_graph_documents([document])\n",
    "        \n",
    "        if graph_documents:\n",
    "            graph_doc = graph_documents[0]\n",
    "            \n",
    "            # Count nodes and relationships\n",
    "            node_count = len(graph_doc.nodes)\n",
    "            rel_count = len(graph_doc.relationships)\n",
    "            \n",
    "            print(f\"   ✅ Created {node_count} nodes, {rel_count} relationships\")\n",
    "            \n",
    "            # Add to collection\n",
    "            all_graph_documents.extend(graph_documents)\n",
    "            \n",
    "            # Update summary\n",
    "            transformation_summary['successful_transformations'] += 1\n",
    "            transformation_summary['total_nodes'] += node_count\n",
    "            transformation_summary['total_relationships'] += rel_count\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ⚠️  No graph elements created\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {doc_id}: {str(e)}\"\n",
    "        print(f\"   ❌ {error_msg}\")\n",
    "        transformation_summary['errors'].append(error_msg)\n",
    "\n",
    "# Populate Neo4j database\n",
    "print(f\"\\n💾 POPULATING NEO4J DATABASE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if all_graph_documents:\n",
    "    try:\n",
    "        # Add graph documents to Neo4j\n",
    "        neo4j_graph.add_graph_documents(\n",
    "            all_graph_documents,\n",
    "            baseEntityLabel=True,  # Add base Entity label to all nodes\n",
    "            include_source=True    # Include source document info\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully populated Neo4j database!\")\n",
    "        \n",
    "        # Verify population with basic queries\n",
    "        node_count = neo4j_graph.query(\"MATCH (n) RETURN count(n) as count\")[0]['count']\n",
    "        rel_count = neo4j_graph.query(\"MATCH ()-[r]->() RETURN count(r) as count\")[0]['count']\n",
    "        \n",
    "        print(f\"📊 Database population verified:\")\n",
    "        print(f\"   • Total nodes in database: {node_count}\")\n",
    "        print(f\"   • Total relationships in database: {rel_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR populating Neo4j database: {e}\")\n",
    "        transformation_summary['errors'].append(f\"Database population error: {str(e)}\")\n",
    "\n",
    "# Display transformation summary\n",
    "print(f\"\\n📈 TRANSFORMATION SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Documents processed: {transformation_summary['total_documents']}\")\n",
    "print(f\"Successful transformations: {transformation_summary['successful_transformations']}\")\n",
    "print(f\"Total nodes created: {transformation_summary['total_nodes']}\")\n",
    "print(f\"Total relationships created: {transformation_summary['total_relationships']}\")\n",
    "\n",
    "if transformation_summary['errors']:\n",
    "    print(f\"\\\\nErrors encountered: {len(transformation_summary['errors'])}\")\n",
    "    for error in transformation_summary['errors']:\n",
    "        print(f\"  • {error}\")\n",
    "else:\n",
    "    print(\"\\\\n✅ No errors encountered!\")\n",
    "\n",
    "print(f\"\\\\n🎉 KNOWLEDGE GRAPH POPULATION COMPLETE!\")\n",
    "print(\"Your Timber Mountain A/B test data is now structured as an enriched knowledge graph in Neo4j.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "awp4ogpfepd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 VALIDATING KNOWLEDGE GRAPH STRUCTURE\n",
      "----------------------------------------\n",
      "🔎 DISCOVERING ACTUAL GRAPH STRUCTURE:\n",
      "----------------------------------------\n",
      "\n",
      "📊 Database Overview:\n",
      "  Total nodes: 58\n",
      "  Unique label combinations: 11\n",
      "\n",
      "📊 All Node Labels:\n",
      "  __Entity__: 53 nodes\n",
      "  Segment: 17 nodes\n",
      "  Feature: 6 nodes\n",
      "  Metric: 6 nodes\n",
      "  Document: 5 nodes\n",
      "  Abtest: 5 nodes\n",
      "  Page: 5 nodes\n",
      "  Hypothesis: 5 nodes\n",
      "  Result: 5 nodes\n",
      "  Insight: 3 nodes\n",
      "  Recommendation: 2 nodes\n",
      "\n",
      "📊 All Relationship Types:\n",
      "  MENTIONS: 56 relationships\n",
      "  TARGETS: 19 relationships\n",
      "  TESTED_ON: 6 relationships\n",
      "  PRODUCED: 6 relationships\n",
      "  VALIDATES: 6 relationships\n",
      "  INDICATES: 6 relationships\n",
      "  HAS_VARIANT: 3 relationships\n",
      "  SUGGESTS: 2 relationships\n",
      "  RELATES_TO: 1 relationships\n",
      "\n",
      "📊 Sample Nodes:\n",
      "  Labels: ['Document']\n",
      "  Properties: ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']\n",
      "  Sample ID/Name: 602206da4e351b9fffd968b207fde6fd\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Abtest']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Timber_Mountain_001\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Page']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Homepage\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Segment']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Domestic_Visitors\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Segment']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: International_Visitors\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Feature']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Andre_Rand\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Hypothesis']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Test_Hypothesis\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Result']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Test_Result\n",
      "  ---\n",
      "  Labels: ['Document']\n",
      "  Properties: ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']\n",
      "  Sample ID/Name: c7c2e00c900e7897f76c8ffdf15744ab\n",
      "  ---\n",
      "  Labels: ['__Entity__', 'Feature']\n",
      "  Properties: ['id']\n",
      "  Sample ID/Name: Ai Planner\n",
      "  ---\n",
      "\n",
      "🎯 ADAPTIVE SAMPLE QUERIES (Based on Discovered Structure):\n",
      "----------------------------------------\n",
      "Available node labels: ['Abtest', 'Document', 'Feature', 'Hypothesis', 'Insight', 'Metric', 'Page', 'Recommendation', 'Result', 'Segment', '__Entity__']\n",
      "Available relationships: ['HAS_VARIANT', 'INDICATES', 'MENTIONS', 'PRODUCED', 'RELATES_TO', 'SUGGESTS', 'TARGETS', 'TESTED_ON', 'VALIDATES']\n",
      "\n",
      "📋 Find all Document nodes with their properties:\n",
      "  1. {'n': {'country': 'Global', 'test_end': '2024-07-28', 'test_launch': '2024-07-08', 'source_pdf': '1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf', 'document_id': 'timber_mountain_001', 'text_length': 2992, 'test_hypothesis': 'Localizing imagery, currency, and copy for international traffic will raise ticket-purchase conversion ≥ 5 % vs. one-size-fits-all content.', 'target_segment': 'Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)', 'test_result': 'Win — 6.2 % lift (p = 0.018). The personalized variant out-converted the generic page because prices showed the visitor’s local currency and shipping language, reducing cognitive load and reinforcing “this park welcomes me.” Control held flat in domestic traffic; all upside came from the int’l slice.', 'page_placement': 'Homepage hero, nav, and pricing banners', 'id': '602206da4e351b9fffd968b207fde6fd', 'text': '\\nA/B TEST: Homepage: Domestic vs. International Visitors — Content Personalization Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_001\\n- Test Launch Date: 2024-07-08\\n- Test End Date: 2024-07-28\\n- Country: Global\\n- Target Segment: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\\n- Page/Placement: Homepage hero, nav, and pricing banners\\n- Test Hypothesis: Localizing imagery, currency, and copy for international traffic will raise ticket-purchase conversion ≥ 5 % vs. one-size-fits-all content.\\n- Test Result: Win — 6.2 % lift (p = 0.018). The personalized variant out-converted the generic page because prices showed the visitor’s local currency and shipping language, reducing cognitive load and reinforcing “this park welcomes me.” Control held flat in domestic traffic; all upside came from the int’l slice.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nLocale-Aware Experience: How\\nWe Boosted International\\nConversions at Timber\\nMountain\\nA data-driven approach to understanding and serving our growing international audience\\nJuly 2024\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Challenge: One-Size-Fits-All Doesn\\'t Fit Everyone\\nCurrent Situation\\nTimber Mountain welcomes ~4M guests annually, with a fast-growing\\nshare of international tourists drawn by California\\'s national-park loop and\\nthe Bay Area tech corridor.\\nKey Issues Identified:\\n• Homepage shows USD pricing, US-specific policies, English-only\\ncopy\\n• International visitors bounce 18% more often\\n• 35% lower conversion rate for international guests\\n\"I\\'m not sure your park even sells tickets to UK residents\"\\n— UK mom, January 2024 Customer Interview\\n\\n=== PAGE 3 ===\\nOur Hypothesis:\\nPersonalization Drives\\nConversion\\nIf we localize content for non-US visitors...\\nAdapt copy, imagery, and currency based on visitor locale\\nInternational visitors will feel \"seen\"\\nCreating a more welcoming first impression\\nPurchase conversion will increase\\nTarget: ≥5% lift in ticket purchases\\n\\n=== PAGE 4 ===\\nTest Design: Creating a Localized Experience\\nControl Experience\\n• Standard global homepage\\n• Hero image: log-flume riders\\n• Pricing displayed in USD only\\n• \"Plan Your Visit\" CTA\\nTreatment Experience\\n• Locale-personalized homepage\\n• Region-specific hero images (e.g., UK family)\\n• Local currency (e.g., £54.99 GBP)\\n• Localized date formats (DD/MM/YY)\\n• \"Book Your Adventure\" CTA\\nImplementation used Accept-Language + MaxMind GeoIP with a 250ms\\ntimeout fallback to control (affecting only 0.3% of traffic)\\n\\n=== PAGE 5 ===\\nTest Methodology: Rigorous,\\nControlled Evaluation\\n1\\nTest Parameters\\n• Countries: US & Canada (Domestic) vs. Rest-of-World (International)\\n• Duration: July 8-28, 2024 (3 full weekends)\\n• Split: 50/50 random assignment at session start\\n• Statistical power: 95% for medium effect size\\n2\\nMeasurement Framework\\n• Primary KPI: Ticket-purchase conversion (same-session)\\n• Secondary KPIs: Homepage bounce rate, Add-to-Cart actions, Trip-Planner\\nengagement\\n• Analysis: Two-tailed z-test on proportions (α = 0.05)\\n\\n=== PAGE 6 ===\\nResults: Meaningful Improvement for International Visitors\\nControl Treatment Lift\\n1\\nKey Success Factors\\n• Local currency display reduced \"price-translation\" friction (41% fewer FAQ: currency clicks)\\n• Regionally-relevant hero images increased scroll depth by 12% for international users\\n• No performance penalty: personalization JSON ≤ 9kB with no impact on 95th percentile FCP\\n\\n=== PAGE 7 ===\\nBusiness Impact: Clear Win for User Experience and Revenue\\n+6.2% +10K\\nInt\\'l Conversion Lift Annual Tickets\\nStatistically significant improvement Projected incremental ticket sales\\n(p=0.018)\\n$720K\\nRevenue Impact\\nWhy This Matters:\\nAnnualized revenue increase\\n• Significant lift among our previously underperforming audience segment\\n• No regression for domestic traffic or page performance\\n• Maintains experience consistency while improving personalization\\n', 'test_name': 'Homepage: Domestic vs. International Visitors — Content Personalization Test', 'page_count': 7}}\n",
      "  2. {'n': {'country': 'United Kingdom, France, Germany', 'test_end': '2024-08-25', 'test_launch': '2024-08-05', 'source_pdf': '2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf', 'document_id': 'timber_mountain_002', 'text_length': 4031, 'test_hypothesis': 'Displaying 1-to-5-star guest ratings beside each hotel / flight will increase itinerary-creation rate ≥ 8 %.', 'target_segment': 'Wild Willy AI Travel Planner Users', 'test_result': 'Win — 9.1 % lift (p < 0.01). The treatment (ratings visible) beat the control (no ratings). Post-test survey showed 24 % more guests “strongly agree” they trust the AI’s picks, directly addressing the transparency friction on p. 13 of the case study .', 'page_placement': 'Wild Willy: AI Travel Planner - Itinerary Results List', 'id': 'c7c2e00c900e7897f76c8ffdf15744ab', 'text': '\\nA/B TEST: AI Planner: Add Verified Star Ratings — Trust & Adoption Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_002\\n- Test Launch Date: 2024-08-05\\n- Test End Date: 2024-08-25\\n- Country: United Kingdom, France, Germany\\n- Target Segment: Wild Willy AI Travel Planner Users\\n- Page/Placement: Wild Willy: AI Travel Planner - Itinerary Results List\\n- Test Hypothesis: Displaying 1-to-5-star guest ratings beside each hotel / flight will increase itinerary-creation rate ≥ 8 %.\\n- Test Result: Win — 9.1 % lift (p < 0.01). The treatment (ratings visible) beat the control (no ratings). Post-test survey showed 24 % more guests “strongly agree” they trust the AI’s picks, directly addressing the transparency friction on p. 13 of the case study .\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nWild Willy AI Planner: Trust &\\nAdoption A/B Test Results\\nOur recent test adding verified star ratings to Wild Willy, our AI vacation planner, delivered\\ncompelling results that address key conversion challenges. This presentation explores\\nhow transparent social proof impacts user trust and itinerary creation rates for Timber\\nMountain vacations.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Trust Gap Challenge\\nCurrent Engagement Metrics\\n30% 20%\\nVisitor Engagement Itinerary Creation\\nUsers who interact with Wild Willy Engaged users who create an\\nAI planner itinerary\\n3%\\nConversion Rate\\nSurvey data shows declining trust in Wild Willy\\'s recommendations,\\nOverall ticket purchase conversion\\ncreating a \"black-box\" transparency problem that prevents users from\\nconfidently accepting AI suggestions.\\n\\n=== PAGE 3 ===\\nOur Hypothesis: Social Proof Builds Trust\\nDisplaying verified 1-to-5-star guest ratings beside every hotel and flight card will raise the itinerary-creation rate by at least 8%, because ratings\\ntransform opaque AI recommendations into peer-endorsed suggestions.\\nWe identified trust as the key friction point on page 13 of our case study and theorized that transparent social validation would significantly impact user\\nconfidence in our AI planner\\'s recommendations.\\n\\n=== PAGE 4 ===\\nTest UX Design: Adding Verified Ratings\\nControl Version Treatment Version\\n• Hotel/flight name • Everything in control plus:\\n• Thumbnail photo • ⭐ Average rating & review count\\n• Price display • Verification tooltip\\n• \"Book\" CTA only • \"Sort by highest rated\" toggle\\nImplementation: Ratings pulled nightly from Booking.com API with fallback to control version if latency exceeded 200ms (affected ~0.4% of sessions).\\n\\n=== PAGE 5 ===\\nTest Methodology & Parameters\\nTest Scope\\nGlobal test across all Wild Willy users on both desktop and mobile platforms\\nDuration: August 5, 2024 - August 25, 2024 (20 days)\\nTraffic & Measurement\\n50/50 traffic split at session start\\nPrimary KPI: Itinerary-creation rate\\nGuardrails: 95th-percentile first contentful paint, bounce rate, add-to-cart\\nStatistical Rigor\\nTwo-tailed z-test with α = 0.05\\nAchieved 95%+ statistical power\\nConfidence intervals calculated for all primary metrics\\n\\n=== PAGE 6 ===\\nResults: Significant Win for Transparency\\nMetric Control Treatment Lift\\nItinerary-creation rate 20.1% 21.9% +9.1%\\n\"Strongly trust Willy\" 23% 28% +24%\\nThe p-value of < 0.01 indicates strong statistical significance, confirming our hypothesis. Users engaged more with cards and qualitative data showed\\nfewer \"Is this legit?\" comments in exit surveys.\\n\\n=== PAGE 7 ===\\nWhy It Worked: Trust Drives Action\\nSocial Proof Effect\\nRatings injected peer validation into otherwise opaque AI\\nrecommendations\\nIncreased Exploration\\nUsers viewed 17% more cards when ratings were present\\nMaintained Performance\\nPage weight stayed under 10 kB with no impact on load times\\nDirect Friction Reduction\\nDirectly addressed the transparency concern identified on page 13\\nof our case study\\n\\n=== PAGE 8 ===\\nConclusion: Verified Ratings\\nBuild Trust\\nThe treatment won with a statistically significant +9.1% lift in itinerary creation and a\\n24% relative jump in trust sentiment, validating our transparency hypothesis with no\\nnegative impact on performance metrics.\\nBy transforming black-box AI recommendations into peer-validated suggestions, we\\'ve\\nsuccessfully addressed a critical friction point in the user journey and created a more\\ntrusted planning experience.\\n\\n=== PAGE 9 ===\\nNext Steps: Expanding the Trust Framework\\n1\\nImmediate Launch\\nGraduate star-ratings UI to 100% of traffic with 5% holdback for\\ndrift monitoring\\n2\\nEnhanced Transparency\\nAdd written guest reviews and \"Why this pick?\" explainer panels\\nto further illuminate AI decision-making\\n3\\nExtended Coverage\\nApply ratings system to car rentals and park dining suggestions\\n4\\nTrust Messaging Tests\\nExperiment with \"Guest-approved choice\" CTAs and localized\\nrating badges\\n5\\nLong-term Evaluation\\nMonitor performance guardrails and conduct causal impact study\\nin six months\\n', 'test_name': 'AI Planner: Add Verified Star Ratings — Trust & Adoption Test', 'page_count': 9}}\n",
      "  3. {'n': {'country': 'United States', 'test_end': '2024-09-22', 'test_launch': '2024-09-02', 'source_pdf': '3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf', 'document_id': 'timber_mountain_003', 'text_length': 4595, 'test_hypothesis': 'Replacing multiple external redirects with a single Booking.com-powered bundle checkout will raise overall booking-completion ≥ 10 %.', 'target_segment': 'Visitors adding ≥ 1 non-ticket item', 'test_result': 'Win — 12.7 % lift (p = 0.006). The integrated flow (treatment) reduced average checkout steps from 7 → 3 and recovered drop-offs called-out under the “Seamless Integration from Planning to Booking” friction point on p. 15 . Big gains came from flights (+5 pp) and hotels (+4 pp) where abandonment was highest.', 'page_placement': 'Multi-step checkout funnel', 'id': '314488347796a4df5b4e5c3994cdc470', 'text': '\\nA/B TEST: Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_003\\n- Test Launch Date: 2024-09-02\\n- Test End Date: 2024-09-22\\n- Country: United States\\n- Target Segment: Visitors adding ≥ 1 non-ticket item\\n- Page/Placement: Multi-step checkout funnel\\n- Test Hypothesis: Replacing multiple external redirects with a single Booking.com-powered bundle checkout will raise overall booking-completion ≥ 10 %.\\n- Test Result: Win — 12.7 % lift (p = 0.006). The integrated flow (treatment) reduced average checkout steps from 7 → 3 and recovered drop-offs called-out under the “Seamless Integration from Planning to Booking” friction point on p. 15 . Big gains came from flights (+5 pp) and hotels (+4 pp) where abandonment was highest.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nTimber Mountain: Unified\\nBundle Flow Checkout Test\\nResults\\nA comprehensive analysis of our Seamless-Booking test initiative aimed at streamlining\\nthe checkout experience and increasing conversion rates.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Checkout Problem\\nOur Legacy Funnel Was Losing Customers\\nOur traditional checkout process was causing significant customer drop-\\noff:\\n• 7 separate checkout steps\\n• 3 domain hand-offs (Booking.com, airline GDS, rental-car sites)\\n• 42% drop-off after Step 3 (external redirect)\\n• Highest abandonment for flights & hotels bundles\\nCustomer feedback: \"Felt sketchy clicking through all those different\\nsites.\"\\n\\n=== PAGE 3 ===\\nOur Hypothesis\\nWe believed we could significantly improve the customer experience by creating a more\\nstreamlined checkout process.\\nIf we replace the multi-redirect flow with a single, Booking.com-powered in-page\\nbundle checkout, overall booking-completion will rise by ≥10% for U.S. visitors who\\nadd at least one non-ticket item.\\n\\n=== PAGE 4 ===\\nUX Comparison: Control vs. Treatment\\nControl: Legacy Checkout Treatment: Unified Flow\\n• 7-step funnel with multiple screens • 3-step in-page wizard (Review → Passenger Info → Pay)\\n• Redirects to external domains for flights, hotels, cars • Booking.com iFrame handles all bundle SKUs\\n• Separate payment screens for each component • One PCI-scoped payment call\\n• Inconsistent branding across touchpoints • Consistent Timber Mountain branding throughout\\nTechnical Note: If Booking.com API response time exceeded 250ms (occurring in 0.6% of sessions), users were gracefully redirected to the Control experience.\\n\\n=== PAGE 5 ===\\nTest Parameters & Methodology\\nGeographic Focus United States only\\nTarget Audience Visitors who added ≥1 flight, hotel, or car rental to cart\\nTest Duration September 2-22, 2024 (3 weeks)\\nTraffic Split 50/50 at session start\\nPrimary KPI Booking-completion rate (all items in cart purchased)\\nSecondary Metrics Flight-only completion, Hotel-only completion, Checkout duration\\nGuardrail Metrics Average order value, 95th-percentile First Contentful Paint\\nStatistical Method Two-tailed z-test, α = 0.05; 95%+ power\\n\\n=== PAGE 6 ===\\nResults: Primary Metrics\\nSignificant Improvements Across All Categories\\n• +12.7% increase in overall booking completion (p=0.006)\\n• +5 percentage points in flight bookings (p=0.015)\\n• +4 percentage points in hotel bookings (p=0.021)\\nThe greatest improvements were seen in the areas where we previously experienced the\\nhighest abandonment rates, confirming our hypothesis that a streamlined checkout experience\\nwould have significant impact.\\nControl Treatment\\n\\n=== PAGE 7 ===\\nResults: Secondary Metrics\\n57% 50% 18pp\\nReduction in Faster Checkout UX Satisfaction\\nCheckout Steps Time Increase\\nFrom 7.0 steps in control to From 4m 12s (control) to More users reported\\n3.0 steps in treatment 2m 05s (treatment) \"checkout felt easy\" in\\nqualitative surveys\\n0.9%\\nPerformance Impact\\nMinimal increase in 95th-\\npercentile FCP (statistically\\ninsignificant at p=0.74)\\n\\n=== PAGE 8 ===\\nHypothesis Confirmed\\nThe integrated Booking.com bundle flow\\ndelivered:\\n• Statistically significant +12.7% uplift in completed bookings\\n• No negative impact on performance metrics\\n• Maintained average order value\\n• Directly addressed the \"Seamless Integration\" friction point\\nProjected Annual Impact (U.S. Traffic Only)\\n≈ +36,000 additional completed itineraries\\n≈ +$2.4M incremental revenue (based on $67 blended ARPU)\\n\\n=== PAGE 9 ===\\nNext Steps: Implementation Plan\\nU.S. Roll-Out\\nFull deployment to 100% U.S. traffic with 5% hold-back for ongoing monitoring and comparison\\nInternational Expansion\\nRun identical tests in Canadian and UK markets to validate regional performance\\nOptimization Opportunities\\nLaunch in-wizard upsell experiments for add-ons (meal plans, express passes, priority boarding)\\nTechnical Safeguards\\nImplement performance guard-rails with alerts if 95th-percentile FCP drifts more than 50ms\\nCross-Device Experience\\nDevelop persistence of the in-page bundle during mobile app hand-off scenarios\\n\\n=== PAGE 10 ===\\nKey Takeaways\\nFrictionless Checkout Drives Conversion\\nThe 12.7% increase in booking completion demonstrates that reducing steps and\\nkeeping users within our ecosystem significantly improves conversion.\\nSpeed Matters\\nCutting checkout time in half while maintaining performance metrics shows that\\nefficiency and user experience can be improved simultaneously.\\nRevenue Impact Is Substantial\\nWith a projected $2.4M annual revenue increase from U.S. traffic alone, the\\nglobal potential of this improvement is significantly higher.\\n', 'test_name': 'Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test', 'page_count': 10}}\n",
      "  4. {'n': {'country': 'Global', 'test_end': '2024-10-21', 'test_launch': '2024-10-07', 'source_pdf': '4 - Timber-Mountain-CTA-Copy-Test-Results.pdf', 'document_id': 'timber_mountain_004', 'text_length': 4244, 'test_hypothesis': 'Swapping in the more action-oriented “Explore More” will boost detail-page CTR ≥ 3 %.', 'target_segment': 'Mobile visitors', 'test_result': 'Inconclusive — 1.1 % lift (p = 0.27). “Explore More” edged out control but not significantly. Session-replay showed most clicks cluster on imagery, not the button copy, implying micro-copy alone can’t move the needle without a larger visual hierarchy change.', 'page_placement': 'All primary CTAs', 'id': '553be49be1733dfce26081830775c4eb', 'text': '\\nA/B TEST: Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_004\\n- Test Launch Date: 2024-10-07\\n- Test End Date: 2024-10-21\\n- Country: Global\\n- Target Segment: Mobile visitors\\n- Page/Placement: All primary CTAs\\n- Test Hypothesis: Swapping in the more action-oriented “Explore More” will boost detail-page CTR ≥ 3 %.\\n- Test Result: Inconclusive — 1.1 % lift (p = 0.27). “Explore More” edged out control but not significantly. Session-replay showed most clicks cluster on imagery, not the button copy, implying micro-copy alone can’t move the needle without a larger visual hierarchy change.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nTimber Mountain: CTA Copy\\nTest Results\\nA data-driven analysis of how our CTA language impacts mobile user engagement\\nOctober 2024\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Mobile Engagement Challenge\\nOur Mobile Reality\\nWhile 63% of Timber Mountain sessions occur on mobile devices, our\\nmobile detail-page click-through rate (CTR) lags significantly behind\\ndesktop performance by 27%.\\nThis engagement gap represents a critical opportunity to improve the\\ncustomer journey on our most-used platform.\\nMobile users face navigation challenges that desktop users don\\'t\\nexperience, particularly with CTA visibility and interaction.\\n\\n=== PAGE 3 ===\\nCurrent User Experience\\nAnalysis\\nVisual Hierarchy Issues\\nHeuristic review revealed CTAs pushed low in the visual hierarchy, making them\\nless noticeable to mobile users scanning content quickly.\\nCompeting Elements\\nPrimary buttons compete with highly tappable hero images, creating confusion\\nabout the primary navigation path.\\nStakeholder Insight\\nTeam hypothesis: more action-oriented CTA copy might stimulate exploration\\nwithout requiring significant engineering resources or design overhauls.\\n\\n=== PAGE 4 ===\\nOur Test Hypothesis\\nSwapping default button copy from \"Learn More\" to the more action-oriented \"Explore\\nMore\" will raise site-wide detail-page CTR by ≥ 3% on mobile devices.\\nThe hypothesis stemmed from behavioral psychology research suggesting that verbs emphasizing discovery (\"explore\") might trigger stronger curiosity\\nthan passive learning prompts.\\n\\n=== PAGE 5 ===\\nTest Implementation Details\\nTest Parameters\\nGeo Global deployment\\nAudience Mobile visitors (UA width <\\n768px)\\nDuration Oct 7 - Oct 21, 2024 (14 days)\\nSplit 50/50 randomization at session\\nstart\\nPrimary KPI Detail-page CTR (taps ÷\\nimpressions)\\nStatistical Test Two-tailed z-test, α = 0.05 The test was designed with sufficient power (95%) to detect a minimum\\n3% lift in our primary KPI, while monitoring key guardrail metrics to ensure\\nno negative impact on user experience.\\n\\n=== PAGE 6 ===\\nComparing Control vs.\\nTreatment\\nBoth variants maintained identical visual styling, placement, and interaction behavior. The\\nonly difference was the button copy itself - ensuring we isolated this single variable for\\ntesting.\\n• Control: \"Learn More\" - our standard CTA language across the site\\n• Treatment: \"Explore More\" - more action-oriented alternative with identical styling\\n• No other design elements were modified to maintain test integrity\\n\\n=== PAGE 7 ===\\nTest Results: Metrics Overview\\nControl Treatment\\n\\n=== PAGE 8 ===\\nQualitative Insights\\nInteraction Patterns Scroll Behavior\\nSession-replay heatmaps revealed 71% of Median users reached buttons in just 1.8\\nclicks clustered on hero images or carousel seconds; however, hero imagery appears\\ncards—with button text rarely being read first and absorbs most visual attention\\nbefore interaction. during this critical decision moment.\\nUser Feedback\\nExit poll comments frequently mentioned \"I\\njust tap the picture, not the button\" -\\nindicating users follow intuitive tapping\\nbehaviors rather than explicit button\\nprompts.\\n\\n=== PAGE 9 ===\\nKey Conclusions\\nThe copy change alone did not achieve\\nstatistical significance (+1.1% lift, p =\\n0.27).\\nEvidence strongly suggests that CTA visibility and visual hierarchy, rather\\nthan verb choice, represents the more significant constraint on mobile\\nengagement.\\nMost users appear to navigate intuitively by tapping images rather than\\nsearching for and reading button text.\\nThe results challenge our initial assumption that copy changes alone\\nwould significantly influence behavior, pointing toward deeper UX patterns\\nthat merit further investigation.\\n\\n=== PAGE 10 ===\\nRecommended Next Steps\\nRethink Visual Prominence\\nTest larger, higher-contrast buttons or floating sticky CTAs that maintain\\nvisibility throughout the scrolling experience.\\nCombine Copy + Design\\nA/B test \"Explore More\" with supplementary iconography or directional\\narrows to enhance visual communication.\\nImage-Tap Instrumentation\\nTreat hero-image taps as intentional navigational clicks; evaluate routing\\nlogic to align with user expectations.\\nMultivariate Testing\\nImplement factorial design testing copy × color × placement to isolate the\\nstrongest drivers of engagement.\\n', 'test_name': 'Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test', 'page_count': 10}}\n",
      "  5. {'n': {'country': 'United States', 'test_end': '2024-11-24', 'test_launch': '2024-11-04', 'source_pdf': '5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf', 'document_id': 'timber_mountain_005', 'text_length': 5169, 'test_hypothesis': 'Surfacing limited-time bundle offers in a rotating carousel will raise add-to-cart rate ≥ 7 %.', 'target_segment': 'First-time visitors', 'test_result': 'Loss — 4.5 % drop (p = 0.04). The carousel variant under-performed: heat-maps showed attention pulled away from the Wild Willy planner module, and qualitative feedback called it “ad-like clutter.” Users scrolled faster and spent less time above the fold, hurting downstream conversions. Next step: test a slimmer static banner.', 'page_placement': 'Top-of-fold carousel after hero', 'id': '848eb690e2a49699db38e68ad172e0d6', 'text': '\\nA/B TEST: Homepage: Special Offers Carousel — Merchandising Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_005\\n- Test Launch Date: 2024-11-04\\n- Test End Date: 2024-11-24\\n- Country: United States\\n- Target Segment: First-time visitors\\n- Page/Placement: Top-of-fold carousel after hero\\n- Test Hypothesis: Surfacing limited-time bundle offers in a rotating carousel will raise add-to-cart rate ≥ 7 %.\\n- Test Result: Loss — 4.5 % drop (p = 0.04). The carousel variant under-performed: heat-maps showed attention pulled away from the Wild Willy planner module, and qualitative feedback called it “ad-like clutter.” Users scrolled faster and spent less time above the fold, hurting downstream conversions. Next step: test a slimmer static banner.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nHomepage Special Offers\\nCarousel: Merchandising Test\\nResults\\nAn analysis of our recent UX experiment testing a rotating carousel for bundle promotion\\nversus the existing Wild Willy planner layout.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nBackground & Current State\\nTimber Mountain\\'s ticket bundles combining park admission, hotel stays,\\nand express passes generate 22% of our annual revenue. However, these\\nhigh-value offerings suffer from low visibility on our homepage.\\nCurrently, bundle discovery options are positioned well below the fold,\\nrequiring significant scrolling from visitors, potentially limiting their\\nexposure and subsequent conversion rates.\\nThe Wild Willy planner module currently occupies prime real estate\\ndirectly beneath the hero section, serving as the primary entry point for trip\\nplanning.\\nThe current homepage places the Wild Willy planner immediately below\\nthe hero image, with bundle options requiring additional scrolling to\\ndiscover.\\n\\n=== PAGE 3 ===\\nThe Proposed Solution\\nRotating Carousel Strategic Placement Potential Risk\\nThe product team proposed introducing a By positioning limited-time offers at the The primary concern was displacement of\\n\"Special Offers\" carousel directly under top of the fold, we aimed to increase the popular Wild Willy planner module,\\nthe hero section, designed to showcase awareness and drive faster add-to-cart pushing it below the fold and potentially\\ntime-boxed deals and bundle offerings actions for these high-margin products. reducing engagement with this critical\\nwith greater visibility. conversion path.\\n\\n=== PAGE 4 ===\\nOur Test Hypothesis\\nSurfacing limited-time bundle offers in a top-of-fold rotating carousel will raise the add-to-cart rate by at least 7% among first-time U.S. visitors.\\nKey Assumptions\\n• Increased visibility of bundle offers would outweigh the shift of the planner below\\nthe fold\\n• Motion and rotation would attract attention to special offers\\n• Time-limited deals would create urgency and drive faster conversion\\nOur hypothesis suggested that higher visibility of time-sensitive offers would overcome\\nany potential friction from repositioning the Wild Willy planner module.\\n\\n=== PAGE 5 ===\\nTest Design: Control vs. Treatment\\nControl Layout Treatment Layout\\nHero image → Wild Willy planner module → static promo banner positioned Hero image → Special Offers carousel (4 rotating cards, 6-sec auto-\\nlower on the page advance) → Wild Willy shifted below the fold\\n\\n=== PAGE 6 ===\\nTechnical Implementation\\nCarousel Specifications\\n• Built with native CSS scroll snap and JavaScript auto-scroll\\nfunctionality\\n• Optimized with lazy-loading for the second frame to minimize\\nperformance impact\\n• Automatic advancement every 6 seconds with pause on hover\\n• Responsive design adapting to all viewport sizes\\n• Fallback to Control version for users with JavaScript disabled (< 0.2%\\nof sessions)\\nThe development team implemented the carousel using modern web\\nstandards to ensure optimal performance while maintaining accessibility\\nand graceful degradation.\\n\\n=== PAGE 7 ===\\nTest Methodology\\nGeo United States\\nAudience First-time visitors (no Timber Mountain cookie)\\nTest Period November 4, 2024 — November 24, 2024\\nTraffic Split 50/50 at session start\\nPrimary KPI Add-to-cart rate (any item)\\nSecondary Metrics Wild Willy planner starts, Scroll-depth, Time-on-page\\nGuardrails Bounce rate, 95th-percentile First Contentful Paint\\nStatistical Analysis Two-tailed z-test, α = 0.05 (≥ 95% power to detect ±7% effect)\\n\\n=== PAGE 8 ===\\nTest Results\\nControl Carousel (Treatment)\\n\\n=== PAGE 9 ===\\nQualitative Insights\\nHeat Map Analysis\\nEye-tracking shifted dramatically to the carousel motion, but this attention didn\\'t\\ntranslate to engagement. The Wild Willy planner tile lost 32% of its previous\\nclicks as users\\' visual focus was diverted.\\nSession Replay Observations\\nUsers scrolled faster past the carousel than control visitors did past the hero\\nsection, indicating a form of \"banner blindness\" or promotional content\\navoidance.\\n\"Looks like an ad slideshow; I ignored it.\"\\n— Exit poll respondent\\nUser feedback suggested the rotating carousel was perceived as advertising rather than valuable content, triggering learned avoidance behaviors similar to banner\\nblindness.\\n\\n=== PAGE 10 ===\\nConclusions & Next Steps\\nRollback Carousel Static Banner Test\\nImmediately revert to the control layout for all U.S. traffic to prevent Develop A/B test for a slim, single-frame offer banner in the same slot to\\nfurther revenue impact. evaluate messaging effectiveness without motion distraction.\\nPlanner-First Approach Design Research\\nPilot deal-badges inside the Wild Willy planner (e.g., \"Save $50\" pill on Launch a rapid UI research sprint focused on animation tolerance and\\nbundle suggestions) to integrate offers within the existing high- perceived \"ad clutter\" specifically for theme-park audience segments.\\nengagement path.\\nThe carousel test provided valuable insights about our users\\' interaction patterns. While the original hypothesis was disproven, we\\'ve identified several\\npromising alternative approaches to improve bundle visibility without sacrificing the critical Wild Willy planner engagement.\\n', 'test_name': 'Homepage: Special Offers Carousel — Merchandising Test', 'page_count': 10}}\n",
      "\n",
      "📋 Find nodes connected by TESTED_ON relationships:\n",
      "  1. {'from_type': '__Entity__', 'to_type': '__Entity__', 'connections': 6}\n",
      "\n",
      "📋 Search for nodes containing 'test' in any property:\n",
      "  1. {'node_labels': ['Document'], 'n': {'country': 'Global', 'test_end': '2024-07-28', 'test_launch': '2024-07-08', 'source_pdf': '1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf', 'document_id': 'timber_mountain_001', 'text_length': 2992, 'test_hypothesis': 'Localizing imagery, currency, and copy for international traffic will raise ticket-purchase conversion ≥ 5 % vs. one-size-fits-all content.', 'target_segment': 'Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)', 'test_result': 'Win — 6.2 % lift (p = 0.018). The personalized variant out-converted the generic page because prices showed the visitor’s local currency and shipping language, reducing cognitive load and reinforcing “this park welcomes me.” Control held flat in domestic traffic; all upside came from the int’l slice.', 'page_placement': 'Homepage hero, nav, and pricing banners', 'id': '602206da4e351b9fffd968b207fde6fd', 'text': '\\nA/B TEST: Homepage: Domestic vs. International Visitors — Content Personalization Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_001\\n- Test Launch Date: 2024-07-08\\n- Test End Date: 2024-07-28\\n- Country: Global\\n- Target Segment: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\\n- Page/Placement: Homepage hero, nav, and pricing banners\\n- Test Hypothesis: Localizing imagery, currency, and copy for international traffic will raise ticket-purchase conversion ≥ 5 % vs. one-size-fits-all content.\\n- Test Result: Win — 6.2 % lift (p = 0.018). The personalized variant out-converted the generic page because prices showed the visitor’s local currency and shipping language, reducing cognitive load and reinforcing “this park welcomes me.” Control held flat in domestic traffic; all upside came from the int’l slice.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nLocale-Aware Experience: How\\nWe Boosted International\\nConversions at Timber\\nMountain\\nA data-driven approach to understanding and serving our growing international audience\\nJuly 2024\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Challenge: One-Size-Fits-All Doesn\\'t Fit Everyone\\nCurrent Situation\\nTimber Mountain welcomes ~4M guests annually, with a fast-growing\\nshare of international tourists drawn by California\\'s national-park loop and\\nthe Bay Area tech corridor.\\nKey Issues Identified:\\n• Homepage shows USD pricing, US-specific policies, English-only\\ncopy\\n• International visitors bounce 18% more often\\n• 35% lower conversion rate for international guests\\n\"I\\'m not sure your park even sells tickets to UK residents\"\\n— UK mom, January 2024 Customer Interview\\n\\n=== PAGE 3 ===\\nOur Hypothesis:\\nPersonalization Drives\\nConversion\\nIf we localize content for non-US visitors...\\nAdapt copy, imagery, and currency based on visitor locale\\nInternational visitors will feel \"seen\"\\nCreating a more welcoming first impression\\nPurchase conversion will increase\\nTarget: ≥5% lift in ticket purchases\\n\\n=== PAGE 4 ===\\nTest Design: Creating a Localized Experience\\nControl Experience\\n• Standard global homepage\\n• Hero image: log-flume riders\\n• Pricing displayed in USD only\\n• \"Plan Your Visit\" CTA\\nTreatment Experience\\n• Locale-personalized homepage\\n• Region-specific hero images (e.g., UK family)\\n• Local currency (e.g., £54.99 GBP)\\n• Localized date formats (DD/MM/YY)\\n• \"Book Your Adventure\" CTA\\nImplementation used Accept-Language + MaxMind GeoIP with a 250ms\\ntimeout fallback to control (affecting only 0.3% of traffic)\\n\\n=== PAGE 5 ===\\nTest Methodology: Rigorous,\\nControlled Evaluation\\n1\\nTest Parameters\\n• Countries: US & Canada (Domestic) vs. Rest-of-World (International)\\n• Duration: July 8-28, 2024 (3 full weekends)\\n• Split: 50/50 random assignment at session start\\n• Statistical power: 95% for medium effect size\\n2\\nMeasurement Framework\\n• Primary KPI: Ticket-purchase conversion (same-session)\\n• Secondary KPIs: Homepage bounce rate, Add-to-Cart actions, Trip-Planner\\nengagement\\n• Analysis: Two-tailed z-test on proportions (α = 0.05)\\n\\n=== PAGE 6 ===\\nResults: Meaningful Improvement for International Visitors\\nControl Treatment Lift\\n1\\nKey Success Factors\\n• Local currency display reduced \"price-translation\" friction (41% fewer FAQ: currency clicks)\\n• Regionally-relevant hero images increased scroll depth by 12% for international users\\n• No performance penalty: personalization JSON ≤ 9kB with no impact on 95th percentile FCP\\n\\n=== PAGE 7 ===\\nBusiness Impact: Clear Win for User Experience and Revenue\\n+6.2% +10K\\nInt\\'l Conversion Lift Annual Tickets\\nStatistically significant improvement Projected incremental ticket sales\\n(p=0.018)\\n$720K\\nRevenue Impact\\nWhy This Matters:\\nAnnualized revenue increase\\n• Significant lift among our previously underperforming audience segment\\n• No regression for domestic traffic or page performance\\n• Maintains experience consistency while improving personalization\\n', 'test_name': 'Homepage: Domestic vs. International Visitors — Content Personalization Test', 'page_count': 7}}\n",
      "  2. {'node_labels': ['Document'], 'n': {'country': 'United Kingdom, France, Germany', 'test_end': '2024-08-25', 'test_launch': '2024-08-05', 'source_pdf': '2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf', 'document_id': 'timber_mountain_002', 'text_length': 4031, 'test_hypothesis': 'Displaying 1-to-5-star guest ratings beside each hotel / flight will increase itinerary-creation rate ≥ 8 %.', 'target_segment': 'Wild Willy AI Travel Planner Users', 'test_result': 'Win — 9.1 % lift (p < 0.01). The treatment (ratings visible) beat the control (no ratings). Post-test survey showed 24 % more guests “strongly agree” they trust the AI’s picks, directly addressing the transparency friction on p. 13 of the case study .', 'page_placement': 'Wild Willy: AI Travel Planner - Itinerary Results List', 'id': 'c7c2e00c900e7897f76c8ffdf15744ab', 'text': '\\nA/B TEST: AI Planner: Add Verified Star Ratings — Trust & Adoption Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_002\\n- Test Launch Date: 2024-08-05\\n- Test End Date: 2024-08-25\\n- Country: United Kingdom, France, Germany\\n- Target Segment: Wild Willy AI Travel Planner Users\\n- Page/Placement: Wild Willy: AI Travel Planner - Itinerary Results List\\n- Test Hypothesis: Displaying 1-to-5-star guest ratings beside each hotel / flight will increase itinerary-creation rate ≥ 8 %.\\n- Test Result: Win — 9.1 % lift (p < 0.01). The treatment (ratings visible) beat the control (no ratings). Post-test survey showed 24 % more guests “strongly agree” they trust the AI’s picks, directly addressing the transparency friction on p. 13 of the case study .\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nWild Willy AI Planner: Trust &\\nAdoption A/B Test Results\\nOur recent test adding verified star ratings to Wild Willy, our AI vacation planner, delivered\\ncompelling results that address key conversion challenges. This presentation explores\\nhow transparent social proof impacts user trust and itinerary creation rates for Timber\\nMountain vacations.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Trust Gap Challenge\\nCurrent Engagement Metrics\\n30% 20%\\nVisitor Engagement Itinerary Creation\\nUsers who interact with Wild Willy Engaged users who create an\\nAI planner itinerary\\n3%\\nConversion Rate\\nSurvey data shows declining trust in Wild Willy\\'s recommendations,\\nOverall ticket purchase conversion\\ncreating a \"black-box\" transparency problem that prevents users from\\nconfidently accepting AI suggestions.\\n\\n=== PAGE 3 ===\\nOur Hypothesis: Social Proof Builds Trust\\nDisplaying verified 1-to-5-star guest ratings beside every hotel and flight card will raise the itinerary-creation rate by at least 8%, because ratings\\ntransform opaque AI recommendations into peer-endorsed suggestions.\\nWe identified trust as the key friction point on page 13 of our case study and theorized that transparent social validation would significantly impact user\\nconfidence in our AI planner\\'s recommendations.\\n\\n=== PAGE 4 ===\\nTest UX Design: Adding Verified Ratings\\nControl Version Treatment Version\\n• Hotel/flight name • Everything in control plus:\\n• Thumbnail photo • ⭐ Average rating & review count\\n• Price display • Verification tooltip\\n• \"Book\" CTA only • \"Sort by highest rated\" toggle\\nImplementation: Ratings pulled nightly from Booking.com API with fallback to control version if latency exceeded 200ms (affected ~0.4% of sessions).\\n\\n=== PAGE 5 ===\\nTest Methodology & Parameters\\nTest Scope\\nGlobal test across all Wild Willy users on both desktop and mobile platforms\\nDuration: August 5, 2024 - August 25, 2024 (20 days)\\nTraffic & Measurement\\n50/50 traffic split at session start\\nPrimary KPI: Itinerary-creation rate\\nGuardrails: 95th-percentile first contentful paint, bounce rate, add-to-cart\\nStatistical Rigor\\nTwo-tailed z-test with α = 0.05\\nAchieved 95%+ statistical power\\nConfidence intervals calculated for all primary metrics\\n\\n=== PAGE 6 ===\\nResults: Significant Win for Transparency\\nMetric Control Treatment Lift\\nItinerary-creation rate 20.1% 21.9% +9.1%\\n\"Strongly trust Willy\" 23% 28% +24%\\nThe p-value of < 0.01 indicates strong statistical significance, confirming our hypothesis. Users engaged more with cards and qualitative data showed\\nfewer \"Is this legit?\" comments in exit surveys.\\n\\n=== PAGE 7 ===\\nWhy It Worked: Trust Drives Action\\nSocial Proof Effect\\nRatings injected peer validation into otherwise opaque AI\\nrecommendations\\nIncreased Exploration\\nUsers viewed 17% more cards when ratings were present\\nMaintained Performance\\nPage weight stayed under 10 kB with no impact on load times\\nDirect Friction Reduction\\nDirectly addressed the transparency concern identified on page 13\\nof our case study\\n\\n=== PAGE 8 ===\\nConclusion: Verified Ratings\\nBuild Trust\\nThe treatment won with a statistically significant +9.1% lift in itinerary creation and a\\n24% relative jump in trust sentiment, validating our transparency hypothesis with no\\nnegative impact on performance metrics.\\nBy transforming black-box AI recommendations into peer-validated suggestions, we\\'ve\\nsuccessfully addressed a critical friction point in the user journey and created a more\\ntrusted planning experience.\\n\\n=== PAGE 9 ===\\nNext Steps: Expanding the Trust Framework\\n1\\nImmediate Launch\\nGraduate star-ratings UI to 100% of traffic with 5% holdback for\\ndrift monitoring\\n2\\nEnhanced Transparency\\nAdd written guest reviews and \"Why this pick?\" explainer panels\\nto further illuminate AI decision-making\\n3\\nExtended Coverage\\nApply ratings system to car rentals and park dining suggestions\\n4\\nTrust Messaging Tests\\nExperiment with \"Guest-approved choice\" CTAs and localized\\nrating badges\\n5\\nLong-term Evaluation\\nMonitor performance guardrails and conduct causal impact study\\nin six months\\n', 'test_name': 'AI Planner: Add Verified Star Ratings — Trust & Adoption Test', 'page_count': 9}}\n",
      "  3. {'node_labels': ['Document'], 'n': {'country': 'United States', 'test_end': '2024-09-22', 'test_launch': '2024-09-02', 'source_pdf': '3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf', 'document_id': 'timber_mountain_003', 'text_length': 4595, 'test_hypothesis': 'Replacing multiple external redirects with a single Booking.com-powered bundle checkout will raise overall booking-completion ≥ 10 %.', 'target_segment': 'Visitors adding ≥ 1 non-ticket item', 'test_result': 'Win — 12.7 % lift (p = 0.006). The integrated flow (treatment) reduced average checkout steps from 7 → 3 and recovered drop-offs called-out under the “Seamless Integration from Planning to Booking” friction point on p. 15 . Big gains came from flights (+5 pp) and hotels (+4 pp) where abandonment was highest.', 'page_placement': 'Multi-step checkout funnel', 'id': '314488347796a4df5b4e5c3994cdc470', 'text': '\\nA/B TEST: Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_003\\n- Test Launch Date: 2024-09-02\\n- Test End Date: 2024-09-22\\n- Country: United States\\n- Target Segment: Visitors adding ≥ 1 non-ticket item\\n- Page/Placement: Multi-step checkout funnel\\n- Test Hypothesis: Replacing multiple external redirects with a single Booking.com-powered bundle checkout will raise overall booking-completion ≥ 10 %.\\n- Test Result: Win — 12.7 % lift (p = 0.006). The integrated flow (treatment) reduced average checkout steps from 7 → 3 and recovered drop-offs called-out under the “Seamless Integration from Planning to Booking” friction point on p. 15 . Big gains came from flights (+5 pp) and hotels (+4 pp) where abandonment was highest.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nTimber Mountain: Unified\\nBundle Flow Checkout Test\\nResults\\nA comprehensive analysis of our Seamless-Booking test initiative aimed at streamlining\\nthe checkout experience and increasing conversion rates.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Checkout Problem\\nOur Legacy Funnel Was Losing Customers\\nOur traditional checkout process was causing significant customer drop-\\noff:\\n• 7 separate checkout steps\\n• 3 domain hand-offs (Booking.com, airline GDS, rental-car sites)\\n• 42% drop-off after Step 3 (external redirect)\\n• Highest abandonment for flights & hotels bundles\\nCustomer feedback: \"Felt sketchy clicking through all those different\\nsites.\"\\n\\n=== PAGE 3 ===\\nOur Hypothesis\\nWe believed we could significantly improve the customer experience by creating a more\\nstreamlined checkout process.\\nIf we replace the multi-redirect flow with a single, Booking.com-powered in-page\\nbundle checkout, overall booking-completion will rise by ≥10% for U.S. visitors who\\nadd at least one non-ticket item.\\n\\n=== PAGE 4 ===\\nUX Comparison: Control vs. Treatment\\nControl: Legacy Checkout Treatment: Unified Flow\\n• 7-step funnel with multiple screens • 3-step in-page wizard (Review → Passenger Info → Pay)\\n• Redirects to external domains for flights, hotels, cars • Booking.com iFrame handles all bundle SKUs\\n• Separate payment screens for each component • One PCI-scoped payment call\\n• Inconsistent branding across touchpoints • Consistent Timber Mountain branding throughout\\nTechnical Note: If Booking.com API response time exceeded 250ms (occurring in 0.6% of sessions), users were gracefully redirected to the Control experience.\\n\\n=== PAGE 5 ===\\nTest Parameters & Methodology\\nGeographic Focus United States only\\nTarget Audience Visitors who added ≥1 flight, hotel, or car rental to cart\\nTest Duration September 2-22, 2024 (3 weeks)\\nTraffic Split 50/50 at session start\\nPrimary KPI Booking-completion rate (all items in cart purchased)\\nSecondary Metrics Flight-only completion, Hotel-only completion, Checkout duration\\nGuardrail Metrics Average order value, 95th-percentile First Contentful Paint\\nStatistical Method Two-tailed z-test, α = 0.05; 95%+ power\\n\\n=== PAGE 6 ===\\nResults: Primary Metrics\\nSignificant Improvements Across All Categories\\n• +12.7% increase in overall booking completion (p=0.006)\\n• +5 percentage points in flight bookings (p=0.015)\\n• +4 percentage points in hotel bookings (p=0.021)\\nThe greatest improvements were seen in the areas where we previously experienced the\\nhighest abandonment rates, confirming our hypothesis that a streamlined checkout experience\\nwould have significant impact.\\nControl Treatment\\n\\n=== PAGE 7 ===\\nResults: Secondary Metrics\\n57% 50% 18pp\\nReduction in Faster Checkout UX Satisfaction\\nCheckout Steps Time Increase\\nFrom 7.0 steps in control to From 4m 12s (control) to More users reported\\n3.0 steps in treatment 2m 05s (treatment) \"checkout felt easy\" in\\nqualitative surveys\\n0.9%\\nPerformance Impact\\nMinimal increase in 95th-\\npercentile FCP (statistically\\ninsignificant at p=0.74)\\n\\n=== PAGE 8 ===\\nHypothesis Confirmed\\nThe integrated Booking.com bundle flow\\ndelivered:\\n• Statistically significant +12.7% uplift in completed bookings\\n• No negative impact on performance metrics\\n• Maintained average order value\\n• Directly addressed the \"Seamless Integration\" friction point\\nProjected Annual Impact (U.S. Traffic Only)\\n≈ +36,000 additional completed itineraries\\n≈ +$2.4M incremental revenue (based on $67 blended ARPU)\\n\\n=== PAGE 9 ===\\nNext Steps: Implementation Plan\\nU.S. Roll-Out\\nFull deployment to 100% U.S. traffic with 5% hold-back for ongoing monitoring and comparison\\nInternational Expansion\\nRun identical tests in Canadian and UK markets to validate regional performance\\nOptimization Opportunities\\nLaunch in-wizard upsell experiments for add-ons (meal plans, express passes, priority boarding)\\nTechnical Safeguards\\nImplement performance guard-rails with alerts if 95th-percentile FCP drifts more than 50ms\\nCross-Device Experience\\nDevelop persistence of the in-page bundle during mobile app hand-off scenarios\\n\\n=== PAGE 10 ===\\nKey Takeaways\\nFrictionless Checkout Drives Conversion\\nThe 12.7% increase in booking completion demonstrates that reducing steps and\\nkeeping users within our ecosystem significantly improves conversion.\\nSpeed Matters\\nCutting checkout time in half while maintaining performance metrics shows that\\nefficiency and user experience can be improved simultaneously.\\nRevenue Impact Is Substantial\\nWith a projected $2.4M annual revenue increase from U.S. traffic alone, the\\nglobal potential of this improvement is significantly higher.\\n', 'test_name': 'Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test', 'page_count': 10}}\n",
      "  4. {'node_labels': ['Document'], 'n': {'country': 'Global', 'test_end': '2024-10-21', 'test_launch': '2024-10-07', 'source_pdf': '4 - Timber-Mountain-CTA-Copy-Test-Results.pdf', 'document_id': 'timber_mountain_004', 'text_length': 4244, 'test_hypothesis': 'Swapping in the more action-oriented “Explore More” will boost detail-page CTR ≥ 3 %.', 'target_segment': 'Mobile visitors', 'test_result': 'Inconclusive — 1.1 % lift (p = 0.27). “Explore More” edged out control but not significantly. Session-replay showed most clicks cluster on imagery, not the button copy, implying micro-copy alone can’t move the needle without a larger visual hierarchy change.', 'page_placement': 'All primary CTAs', 'id': '553be49be1733dfce26081830775c4eb', 'text': '\\nA/B TEST: Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_004\\n- Test Launch Date: 2024-10-07\\n- Test End Date: 2024-10-21\\n- Country: Global\\n- Target Segment: Mobile visitors\\n- Page/Placement: All primary CTAs\\n- Test Hypothesis: Swapping in the more action-oriented “Explore More” will boost detail-page CTR ≥ 3 %.\\n- Test Result: Inconclusive — 1.1 % lift (p = 0.27). “Explore More” edged out control but not significantly. Session-replay showed most clicks cluster on imagery, not the button copy, implying micro-copy alone can’t move the needle without a larger visual hierarchy change.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nTimber Mountain: CTA Copy\\nTest Results\\nA data-driven analysis of how our CTA language impacts mobile user engagement\\nOctober 2024\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nThe Mobile Engagement Challenge\\nOur Mobile Reality\\nWhile 63% of Timber Mountain sessions occur on mobile devices, our\\nmobile detail-page click-through rate (CTR) lags significantly behind\\ndesktop performance by 27%.\\nThis engagement gap represents a critical opportunity to improve the\\ncustomer journey on our most-used platform.\\nMobile users face navigation challenges that desktop users don\\'t\\nexperience, particularly with CTA visibility and interaction.\\n\\n=== PAGE 3 ===\\nCurrent User Experience\\nAnalysis\\nVisual Hierarchy Issues\\nHeuristic review revealed CTAs pushed low in the visual hierarchy, making them\\nless noticeable to mobile users scanning content quickly.\\nCompeting Elements\\nPrimary buttons compete with highly tappable hero images, creating confusion\\nabout the primary navigation path.\\nStakeholder Insight\\nTeam hypothesis: more action-oriented CTA copy might stimulate exploration\\nwithout requiring significant engineering resources or design overhauls.\\n\\n=== PAGE 4 ===\\nOur Test Hypothesis\\nSwapping default button copy from \"Learn More\" to the more action-oriented \"Explore\\nMore\" will raise site-wide detail-page CTR by ≥ 3% on mobile devices.\\nThe hypothesis stemmed from behavioral psychology research suggesting that verbs emphasizing discovery (\"explore\") might trigger stronger curiosity\\nthan passive learning prompts.\\n\\n=== PAGE 5 ===\\nTest Implementation Details\\nTest Parameters\\nGeo Global deployment\\nAudience Mobile visitors (UA width <\\n768px)\\nDuration Oct 7 - Oct 21, 2024 (14 days)\\nSplit 50/50 randomization at session\\nstart\\nPrimary KPI Detail-page CTR (taps ÷\\nimpressions)\\nStatistical Test Two-tailed z-test, α = 0.05 The test was designed with sufficient power (95%) to detect a minimum\\n3% lift in our primary KPI, while monitoring key guardrail metrics to ensure\\nno negative impact on user experience.\\n\\n=== PAGE 6 ===\\nComparing Control vs.\\nTreatment\\nBoth variants maintained identical visual styling, placement, and interaction behavior. The\\nonly difference was the button copy itself - ensuring we isolated this single variable for\\ntesting.\\n• Control: \"Learn More\" - our standard CTA language across the site\\n• Treatment: \"Explore More\" - more action-oriented alternative with identical styling\\n• No other design elements were modified to maintain test integrity\\n\\n=== PAGE 7 ===\\nTest Results: Metrics Overview\\nControl Treatment\\n\\n=== PAGE 8 ===\\nQualitative Insights\\nInteraction Patterns Scroll Behavior\\nSession-replay heatmaps revealed 71% of Median users reached buttons in just 1.8\\nclicks clustered on hero images or carousel seconds; however, hero imagery appears\\ncards—with button text rarely being read first and absorbs most visual attention\\nbefore interaction. during this critical decision moment.\\nUser Feedback\\nExit poll comments frequently mentioned \"I\\njust tap the picture, not the button\" -\\nindicating users follow intuitive tapping\\nbehaviors rather than explicit button\\nprompts.\\n\\n=== PAGE 9 ===\\nKey Conclusions\\nThe copy change alone did not achieve\\nstatistical significance (+1.1% lift, p =\\n0.27).\\nEvidence strongly suggests that CTA visibility and visual hierarchy, rather\\nthan verb choice, represents the more significant constraint on mobile\\nengagement.\\nMost users appear to navigate intuitively by tapping images rather than\\nsearching for and reading button text.\\nThe results challenge our initial assumption that copy changes alone\\nwould significantly influence behavior, pointing toward deeper UX patterns\\nthat merit further investigation.\\n\\n=== PAGE 10 ===\\nRecommended Next Steps\\nRethink Visual Prominence\\nTest larger, higher-contrast buttons or floating sticky CTAs that maintain\\nvisibility throughout the scrolling experience.\\nCombine Copy + Design\\nA/B test \"Explore More\" with supplementary iconography or directional\\narrows to enhance visual communication.\\nImage-Tap Instrumentation\\nTreat hero-image taps as intentional navigational clicks; evaluate routing\\nlogic to align with user expectations.\\nMultivariate Testing\\nImplement factorial design testing copy × color × placement to isolate the\\nstrongest drivers of engagement.\\n', 'test_name': 'Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test', 'page_count': 10}}\n",
      "  5. {'node_labels': ['Document'], 'n': {'country': 'United States', 'test_end': '2024-11-24', 'test_launch': '2024-11-04', 'source_pdf': '5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf', 'document_id': 'timber_mountain_005', 'text_length': 5169, 'test_hypothesis': 'Surfacing limited-time bundle offers in a rotating carousel will raise add-to-cart rate ≥ 7 %.', 'target_segment': 'First-time visitors', 'test_result': 'Loss — 4.5 % drop (p = 0.04). The carousel variant under-performed: heat-maps showed attention pulled away from the Wild Willy planner module, and qualitative feedback called it “ad-like clutter.” Users scrolled faster and spent less time above the fold, hurting downstream conversions. Next step: test a slimmer static banner.', 'page_placement': 'Top-of-fold carousel after hero', 'id': '848eb690e2a49699db38e68ad172e0d6', 'text': '\\nA/B TEST: Homepage: Special Offers Carousel — Merchandising Test\\n\\nMETADATA:\\n- Document ID: timber_mountain_005\\n- Test Launch Date: 2024-11-04\\n- Test End Date: 2024-11-24\\n- Country: United States\\n- Target Segment: First-time visitors\\n- Page/Placement: Top-of-fold carousel after hero\\n- Test Hypothesis: Surfacing limited-time bundle offers in a rotating carousel will raise add-to-cart rate ≥ 7 %.\\n- Test Result: Loss — 4.5 % drop (p = 0.04). The carousel variant under-performed: heat-maps showed attention pulled away from the Wild Willy planner module, and qualitative feedback called it “ad-like clutter.” Users scrolled faster and spent less time above the fold, hurting downstream conversions. Next step: test a slimmer static banner.\\n\\nFULL PRESENTATION CONTENT:\\n=== PAGE 1 ===\\nHomepage Special Offers\\nCarousel: Merchandising Test\\nResults\\nAn analysis of our recent UX experiment testing a rotating carousel for bundle promotion\\nversus the existing Wild Willy planner layout.\\nby Andre Rand\\n\\n=== PAGE 2 ===\\nBackground & Current State\\nTimber Mountain\\'s ticket bundles combining park admission, hotel stays,\\nand express passes generate 22% of our annual revenue. However, these\\nhigh-value offerings suffer from low visibility on our homepage.\\nCurrently, bundle discovery options are positioned well below the fold,\\nrequiring significant scrolling from visitors, potentially limiting their\\nexposure and subsequent conversion rates.\\nThe Wild Willy planner module currently occupies prime real estate\\ndirectly beneath the hero section, serving as the primary entry point for trip\\nplanning.\\nThe current homepage places the Wild Willy planner immediately below\\nthe hero image, with bundle options requiring additional scrolling to\\ndiscover.\\n\\n=== PAGE 3 ===\\nThe Proposed Solution\\nRotating Carousel Strategic Placement Potential Risk\\nThe product team proposed introducing a By positioning limited-time offers at the The primary concern was displacement of\\n\"Special Offers\" carousel directly under top of the fold, we aimed to increase the popular Wild Willy planner module,\\nthe hero section, designed to showcase awareness and drive faster add-to-cart pushing it below the fold and potentially\\ntime-boxed deals and bundle offerings actions for these high-margin products. reducing engagement with this critical\\nwith greater visibility. conversion path.\\n\\n=== PAGE 4 ===\\nOur Test Hypothesis\\nSurfacing limited-time bundle offers in a top-of-fold rotating carousel will raise the add-to-cart rate by at least 7% among first-time U.S. visitors.\\nKey Assumptions\\n• Increased visibility of bundle offers would outweigh the shift of the planner below\\nthe fold\\n• Motion and rotation would attract attention to special offers\\n• Time-limited deals would create urgency and drive faster conversion\\nOur hypothesis suggested that higher visibility of time-sensitive offers would overcome\\nany potential friction from repositioning the Wild Willy planner module.\\n\\n=== PAGE 5 ===\\nTest Design: Control vs. Treatment\\nControl Layout Treatment Layout\\nHero image → Wild Willy planner module → static promo banner positioned Hero image → Special Offers carousel (4 rotating cards, 6-sec auto-\\nlower on the page advance) → Wild Willy shifted below the fold\\n\\n=== PAGE 6 ===\\nTechnical Implementation\\nCarousel Specifications\\n• Built with native CSS scroll snap and JavaScript auto-scroll\\nfunctionality\\n• Optimized with lazy-loading for the second frame to minimize\\nperformance impact\\n• Automatic advancement every 6 seconds with pause on hover\\n• Responsive design adapting to all viewport sizes\\n• Fallback to Control version for users with JavaScript disabled (< 0.2%\\nof sessions)\\nThe development team implemented the carousel using modern web\\nstandards to ensure optimal performance while maintaining accessibility\\nand graceful degradation.\\n\\n=== PAGE 7 ===\\nTest Methodology\\nGeo United States\\nAudience First-time visitors (no Timber Mountain cookie)\\nTest Period November 4, 2024 — November 24, 2024\\nTraffic Split 50/50 at session start\\nPrimary KPI Add-to-cart rate (any item)\\nSecondary Metrics Wild Willy planner starts, Scroll-depth, Time-on-page\\nGuardrails Bounce rate, 95th-percentile First Contentful Paint\\nStatistical Analysis Two-tailed z-test, α = 0.05 (≥ 95% power to detect ±7% effect)\\n\\n=== PAGE 8 ===\\nTest Results\\nControl Carousel (Treatment)\\n\\n=== PAGE 9 ===\\nQualitative Insights\\nHeat Map Analysis\\nEye-tracking shifted dramatically to the carousel motion, but this attention didn\\'t\\ntranslate to engagement. The Wild Willy planner tile lost 32% of its previous\\nclicks as users\\' visual focus was diverted.\\nSession Replay Observations\\nUsers scrolled faster past the carousel than control visitors did past the hero\\nsection, indicating a form of \"banner blindness\" or promotional content\\navoidance.\\n\"Looks like an ad slideshow; I ignored it.\"\\n— Exit poll respondent\\nUser feedback suggested the rotating carousel was perceived as advertising rather than valuable content, triggering learned avoidance behaviors similar to banner\\nblindness.\\n\\n=== PAGE 10 ===\\nConclusions & Next Steps\\nRollback Carousel Static Banner Test\\nImmediately revert to the control layout for all U.S. traffic to prevent Develop A/B test for a slim, single-frame offer banner in the same slot to\\nfurther revenue impact. evaluate messaging effectiveness without motion distraction.\\nPlanner-First Approach Design Research\\nPilot deal-badges inside the Wild Willy planner (e.g., \"Save $50\" pill on Launch a rapid UI research sprint focused on animation tolerance and\\nbundle suggestions) to integrate offers within the existing high- perceived \"ad clutter\" specifically for theme-park audience segments.\\nengagement path.\\nThe carousel test provided valuable insights about our users\\' interaction patterns. While the original hypothesis was disproven, we\\'ve identified several\\npromising alternative approaches to improve bundle visibility without sacrificing the critical Wild Willy planner engagement.\\n', 'test_name': 'Homepage: Special Offers Carousel — Merchandising Test', 'page_count': 10}}\n",
      "\n",
      "📋 Find nodes with the most properties (likely enriched data):\n",
      "  1. {'labels': ['Document'], 'property_count': 14, 'properties': ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']}\n",
      "  2. {'labels': ['Document'], 'property_count': 14, 'properties': ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']}\n",
      "  3. {'labels': ['Document'], 'property_count': 14, 'properties': ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']}\n",
      "  4. {'labels': ['Document'], 'property_count': 14, 'properties': ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']}\n",
      "  5. {'labels': ['Document'], 'property_count': 14, 'properties': ['id', 'text', 'country', 'test_end', 'test_launch', 'source_pdf', 'document_id', 'text_length', 'test_hypothesis', 'target_segment', 'test_result', 'page_placement', 'test_name', 'page_count']}\n",
      "\n",
      "🔧 TROUBLESHOOTING INFORMATION:\n",
      "----------------------------------------\n",
      "✅ Graph structure discovered successfully!\n",
      "   • Found 11 node types\n",
      "   • Found 9 relationship types\n",
      "\n",
      "🎉 KNOWLEDGE GRAPH VALIDATION COMPLETE!\n",
      "============================================================\n",
      "✅ Your Timber Mountain A/B test knowledge graph contains data!\n",
      "🔗 Graph structure has been analyzed and validated\n",
      "🤖 Ready for GraphRAG-powered chatbot queries\n",
      "📊 Use the discovered labels and relationships for your queries\n",
      "\n",
      "📋 NEXT STEPS FOR CHATBOT DEVELOPMENT:\n",
      "----------------------------------------\n",
      "1. 🔍 Build GraphRAG query system with discovered node types\n",
      "2. 🤖 Create conversation chain for natural language queries\n",
      "3. 🌐 Develop Streamlit frontend interface\n",
      "4. 🚀 Deploy publicly accessible chatbot\n",
      "5. 📈 Test with complex A/B testing questions\n",
      "\n",
      "🌲 Timber Mountain AI Chatbot - Knowledge Graph Phase Complete! 🌲\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 10: GRAPH VALIDATION AND VERIFICATION QUERIES\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔍 VALIDATING KNOWLEDGE GRAPH STRUCTURE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# First, let's discover what actually exists in the database\n",
    "discovery_queries = {\n",
    "    \"Database Overview\": \"MATCH (n) RETURN count(n) as total_nodes, count(distinct labels(n)) as unique_label_combinations\",\n",
    "    \"All Node Labels\": \"MATCH (n) UNWIND labels(n) as label RETURN distinct label, count(*) as count ORDER BY count DESC\",\n",
    "    \"All Relationship Types\": \"MATCH ()-[r]->() RETURN distinct type(r) as relationship_type, count(*) as count ORDER BY count DESC\",\n",
    "    \"Sample Nodes\": \"MATCH (n) RETURN labels(n) as labels, keys(n) as properties, n LIMIT 10\"\n",
    "}\n",
    "\n",
    "print(\"🔎 DISCOVERING ACTUAL GRAPH STRUCTURE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "discovered_labels = set()\n",
    "discovered_relationships = set()\n",
    "\n",
    "for query_name, query in discovery_queries.items():\n",
    "    print(f\"\\n📊 {query_name}:\")\n",
    "    try:\n",
    "        results = neo4j_graph.query(query)\n",
    "        \n",
    "        if results:\n",
    "            for result in results:\n",
    "                if query_name == \"Database Overview\":\n",
    "                    print(f\"  Total nodes: {result['total_nodes']}\")\n",
    "                    print(f\"  Unique label combinations: {result['unique_label_combinations']}\")\n",
    "                    \n",
    "                elif query_name == \"All Node Labels\":\n",
    "                    label = result['label']\n",
    "                    count = result['count']\n",
    "                    discovered_labels.add(label)\n",
    "                    print(f\"  {label}: {count} nodes\")\n",
    "                    \n",
    "                elif query_name == \"All Relationship Types\":\n",
    "                    rel_type = result['relationship_type']\n",
    "                    count = result['count']\n",
    "                    discovered_relationships.add(rel_type)\n",
    "                    print(f\"  {rel_type}: {count} relationships\")\n",
    "                    \n",
    "                elif query_name == \"Sample Nodes\":\n",
    "                    labels = result['labels']\n",
    "                    props = result['properties']\n",
    "                    node = result['n']\n",
    "                    print(f\"  Labels: {labels}\")\n",
    "                    print(f\"  Properties: {props}\")\n",
    "                    # Show a few key properties if they exist\n",
    "                    if hasattr(node, 'get'):\n",
    "                        id_prop = node.get('id', node.get('name', 'N/A'))\n",
    "                        print(f\"  Sample ID/Name: {id_prop}\")\n",
    "                    print(\"  ---\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error executing query: {e}\")\n",
    "\n",
    "# Now create adaptive queries based on what we actually found\n",
    "print(f\"\\n🎯 ADAPTIVE SAMPLE QUERIES (Based on Discovered Structure):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if discovered_labels:\n",
    "    print(f\"Available node labels: {sorted(discovered_labels)}\")\n",
    "    print(f\"Available relationships: {sorted(discovered_relationships)}\")\n",
    "    \n",
    "    # Create adaptive queries based on what actually exists\n",
    "    adaptive_queries = []\n",
    "    \n",
    "    # Look for any document-related nodes\n",
    "    doc_labels = [label for label in discovered_labels if any(term in label.lower() for term in ['document', 'test', 'ab'])]\n",
    "    if doc_labels:\n",
    "        adaptive_queries.append({\n",
    "            \"name\": f\"Find all {doc_labels[0]} nodes with their properties\",\n",
    "            \"query\": f\"MATCH (n:{doc_labels[0]}) RETURN n LIMIT 5\"\n",
    "        })\n",
    "    \n",
    "    # Look for any relationship patterns\n",
    "    if discovered_relationships:\n",
    "        rel_type = list(discovered_relationships)[0]  # Use first available relationship\n",
    "        adaptive_queries.append({\n",
    "            \"name\": f\"Find nodes connected by {rel_type} relationships\",\n",
    "            \"query\": f\"MATCH (a)-[r:{rel_type}]->(b) RETURN labels(a)[0] as from_type, labels(b)[0] as to_type, count(*) as connections LIMIT 5\"\n",
    "        })\n",
    "    \n",
    "    # Generic content search\n",
    "    adaptive_queries.append({\n",
    "        \"name\": \"Search for nodes containing 'test' in any property\",\n",
    "        \"query\": \"MATCH (n) WHERE any(prop in keys(n) WHERE toString(n[prop]) CONTAINS 'test') RETURN labels(n) as node_labels, n LIMIT 5\"\n",
    "    })\n",
    "    \n",
    "    # Show nodes with most properties (likely the enriched ones)\n",
    "    adaptive_queries.append({\n",
    "        \"name\": \"Find nodes with the most properties (likely enriched data)\",\n",
    "        \"query\": \"MATCH (n) RETURN labels(n) as labels, size(keys(n)) as property_count, keys(n) as properties ORDER BY property_count DESC LIMIT 5\"\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    # If no labels found, the database might be empty\n",
    "    adaptive_queries = [{\n",
    "        \"name\": \"Check if database is empty\",\n",
    "        \"query\": \"MATCH (n) RETURN count(n) as total_nodes\"\n",
    "    }]\n",
    "\n",
    "# Execute adaptive queries\n",
    "for sample in adaptive_queries:\n",
    "    print(f\"\\n📋 {sample['name']}:\")\n",
    "    try:\n",
    "        results = neo4j_graph.query(sample['query'])\n",
    "        if results:\n",
    "            for i, result in enumerate(results):\n",
    "                print(f\"  {i+1}. {result}\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Query error: {e}\")\n",
    "\n",
    "# Troubleshooting section\n",
    "print(f\"\\n🔧 TROUBLESHOOTING INFORMATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if not discovered_labels:\n",
    "    print(\"⚠️  No node labels found in database!\")\n",
    "    print(\"   Possible issues:\")\n",
    "    print(\"   1. Step 9 (graph transformation) wasn't executed\")\n",
    "    print(\"   2. LLMGraphTransformer failed to create nodes\")\n",
    "    print(\"   3. Database connection issues\")\n",
    "    print(\"   4. Empty source documents\")\n",
    "    \n",
    "    # Check if variables from previous steps exist\n",
    "    try:\n",
    "        if 'all_graph_documents' in locals() or 'all_graph_documents' in globals():\n",
    "            print(f\"   ✅ Graph documents variable exists\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Graph documents variable not found - run Step 9 first\")\n",
    "    except:\n",
    "        print(f\"   ⚠️  Cannot check Step 9 variables\")\n",
    "else:\n",
    "    print(\"✅ Graph structure discovered successfully!\")\n",
    "    print(f\"   • Found {len(discovered_labels)} node types\")\n",
    "    print(f\"   • Found {len(discovered_relationships)} relationship types\")\n",
    "\n",
    "# Final success confirmation\n",
    "print(f\"\\n🎉 KNOWLEDGE GRAPH VALIDATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "if discovered_labels:\n",
    "    print(\"✅ Your Timber Mountain A/B test knowledge graph contains data!\")\n",
    "    print(\"🔗 Graph structure has been analyzed and validated\")\n",
    "    print(\"🤖 Ready for GraphRAG-powered chatbot queries\")\n",
    "    print(\"📊 Use the discovered labels and relationships for your queries\")\n",
    "else:\n",
    "    print(\"⚠️  Graph appears to be empty - please run Step 9 first\")\n",
    "    print(\"🔧 Check the troubleshooting information above\")\n",
    "\n",
    "print(f\"\\n📋 NEXT STEPS FOR CHATBOT DEVELOPMENT:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. 🔍 Build GraphRAG query system with discovered node types\")\n",
    "print(\"2. 🤖 Create conversation chain for natural language queries\")\n",
    "print(\"3. 🌐 Develop Streamlit frontend interface\")\n",
    "print(\"4. 🚀 Deploy publicly accessible chatbot\")\n",
    "print(\"5. 📈 Test with complex A/B testing questions\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain AI Chatbot - Knowledge Graph Phase Complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5tcdnaf1ep",
   "metadata": {},
   "outputs": [],
   "source": "# ===================================================================\n# STEP 11: CREATE RAG RETRIEVER WITH NEO4J VECTOR STORE\n# ===================================================================\n# Step 3: Build Intelligent Semantic Search System\n# This creates a RAG retriever that combines vector similarity search\n# with the rich graph structure for intelligent A/B testing queries.\n\nfrom langchain_neo4j import Neo4jVector\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\nprint(\"\\n🔍 BUILDING RAG RETRIEVER SYSTEM\")\nprint(\"=\" * 60)\nprint(\"🎯 Step 3: Transform knowledge graph into intelligent semantic search\")\n\n# Configure OpenAI embeddings for semantic search\nprint(\"\\n⚡ CONFIGURING SEMANTIC EMBEDDINGS:\")\nprint(\"-\" * 40)\n\ntry:\n    embeddings = OpenAIEmbeddings(\n        model=\"text-embedding-ada-002\",  # High-quality embedding model\n        openai_api_key=os.getenv('OPENAI_API_KEY')\n    )\n    \n    # Test embeddings configuration\n    test_embedding = embeddings.embed_query(\"A/B test conversion improvement\")\n    print(f\"✅ OpenAI embeddings configured successfully\")\n    print(f\"📊 Embedding dimension: {len(test_embedding)}\")\n    print(f\"🧠 Model: text-embedding-ada-002\")\n    \nexcept Exception as e:\n    print(f\"❌ ERROR configuring embeddings: {e}\")\n    raise\n\n# Discover existing graph structure for vector store setup\nprint(\"\\n🔎 ANALYZING GRAPH FOR VECTOR STORE SETUP:\")\nprint(\"-\" * 40)\n\n# Check what text-rich nodes exist in the graph\nnodes_query = \"MATCH (n) WHERE any(prop in keys(n) WHERE size(toString(n[prop])) > 100) RETURN labels(n) as labels, keys(n) as properties, count(*) as count ORDER BY count DESC\"\n\navailable_labels = []\ndiscovered_properties = []\n\ntry:\n    results = neo4j_graph.query(nodes_query)\n    \n    print(\"📋 Nodes with substantial text content:\")\n    for result in results:\n        labels = result['labels']\n        properties = result['properties']\n        count = result['count']\n        available_labels.extend(labels)\n        discovered_properties.extend(properties)\n        print(f\"  {labels}: {count} nodes with properties {properties}\")\n        \nexcept Exception as e:\n    print(f\"❌ Error analyzing nodes: {e}\")\n\n# Use comprehensive text-rich properties from Document nodes\nprint(f\"\\n📝 Using comprehensive text properties for rich embeddings:\")\n# Include ALL meaningful text fields for comprehensive semantic search\ntext_properties = [\n    \"text\",              # Primary document content\n    \"test_hypothesis\",   # Test hypothesis description  \n    \"test_result\",       # Test results and findings\n    \"target_segment\",    # User segment being tested\n    \"test_name\",         # Descriptive test name\n    \"page_placement\"     # Page location description\n]\n\nprint(f\"📊 Selected {len(text_properties)} text properties for embeddings:\")\nfor prop in text_properties:\n    print(f\"  • {prop}\")\n\n# Properties excluded (non-text or existing embeddings):\nexcluded_props = [\"id\", \"document_id\", \"country\", \"test_end\", \"test_launch\", \"source_pdf\", \"text_length\", \"page_count\", \"embedding\"]\nprint(f\"\\n🚫 Excluded {len(excluded_props)} non-text properties: {excluded_props}\")\n\n# Create Neo4j Vector Store from existing graph\nprint(f\"\\n🏗️  CREATING NEO4J VECTOR STORE:\")\nprint(\"-\" * 40)\n\n# Determine best configuration based on discovered structure\nnode_label = None\ntext_node_props = text_properties  # Use comprehensive text properties\n\n# Use discovered labels and properties\nif available_labels:\n    # Prefer Document or similar labels, fall back to first available\n    preferred_labels = [\"Document\", \"ABTest\", \"Test\", \"Entity\"]\n    node_label = next((label for label in preferred_labels if label in available_labels), available_labels[0])\n\nprint(f\"🎯 Vector store configuration:\")\nprint(f\"  • Target node label: {node_label}\")\nprint(f\"  • Text properties: {text_node_props}\")\nprint(f\"  • Embedding strategy: Combine all text fields for rich semantic search\")\n\ntry:\n    # Create vector store from existing graph\n    vector_store = Neo4jVector.from_existing_graph(\n        embedding=embeddings,\n        url=os.getenv('NEO4J_URI'),\n        username=os.getenv('NEO4J_USERNAME'),\n        password=os.getenv('NEO4J_PASSWORD'),\n        index_name=\"timber_mountain_embeddings\",  # Unique index name\n        node_label=node_label if node_label else \"Document\",  # Primary node type for embeddings\n        text_node_properties=text_node_props,  # Properties containing embeddable text\n        embedding_node_property=\"embedding\",  # Where to store embeddings\n    )\n    \n    print(f\"✅ Neo4j Vector Store created successfully!\")\n    print(f\"📊 Index name: timber_mountain_embeddings\")\n    print(f\"🔗 Connected to existing knowledge graph\")\n    print(f\"🧠 Embeddings created from {len(text_node_props)} text properties\")\n    \nexcept Exception as e:\n    print(f\"❌ ERROR creating vector store: {e}\")\n    print(f\"🔧 Troubleshooting tips:\")\n    print(f\"   • Ensure Neo4j database contains nodes with text content\")\n    print(f\"   • Check that Step 9 (graph population) was completed successfully\")\n    print(f\"   • Verify Neo4j connection credentials\")\n    \n    # Fallback: Create empty vector store for testing\n    print(f\"\\n⚠️  Creating fallback vector store for testing...\")\n    try:\n        vector_store = Neo4jVector(\n            embedding=embeddings,\n            url=os.getenv('NEO4J_URI'),\n            username=os.getenv('NEO4J_USERNAME'),\n            password=os.getenv('NEO4J_PASSWORD'),\n            index_name=\"timber_mountain_embeddings_fallback\"\n        )\n        print(f\"✅ Fallback vector store created\")\n    except Exception as fallback_error:\n        print(f\"❌ Fallback also failed: {fallback_error}\")\n        raise\n\nprint(f\"\\n🎯 RAG RETRIEVER SETUP COMPLETE!\")\nprint(\"Your knowledge graph now has comprehensive semantic search capabilities!\")\nprint(\"Ready for intelligent A/B testing queries and chatbot integration.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mamixmctya",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 12: CONFIGURE RETRIEVER AND TEST SEMANTIC SEARCH\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔧 CONFIGURING INTELLIGENT RETRIEVER\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create base retriever from vector store\n",
    "try:\n",
    "    base_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",  # Only return highly relevant results\n",
    "        search_kwargs={\n",
    "            \"score_threshold\": 0.7,  # Minimum similarity score (0-1)\n",
    "            \"k\": 5,                  # Maximum number of results\n",
    "            \"fetch_k\": 20,           # Number of documents to fetch before filtering\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Base retriever configured:\")\n",
    "    print(f\"  • Search type: similarity_score_threshold\")\n",
    "    print(f\"  • Score threshold: 0.7 (high relevance)\")\n",
    "    print(f\"  • Max results: 5\")\n",
    "    print(f\"  • Fetch pool: 20 documents\")\n",
    "    \n",
    "    # Create enhanced retriever with compression for better context\n",
    "    try:\n",
    "        llm_for_compression = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            api_key=os.getenv('OPENAI_API_KEY')\n",
    "        )\n",
    "        \n",
    "        compressor = LLMChainExtractor.from_llm(llm_for_compression)\n",
    "        \n",
    "        enhanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor,\n",
    "            base_retriever=base_retriever\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Enhanced retriever with contextual compression enabled\")\n",
    "        print(f\"  • Uses GPT-4o-mini for intelligent result compression\")\n",
    "        print(f\"  • Removes irrelevant content, keeps essential context\")\n",
    "        \n",
    "        # Use enhanced retriever as primary\n",
    "        primary_retriever = enhanced_retriever\n",
    "        retriever_type = \"Enhanced (with compression)\"\n",
    "        \n",
    "    except Exception as compression_error:\n",
    "        print(f\"⚠️  Compression setup failed: {compression_error}\")\n",
    "        print(f\"✅ Using base retriever without compression\")\n",
    "        primary_retriever = base_retriever\n",
    "        retriever_type = \"Base (without compression)\"\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR configuring retriever: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test retriever with A/B testing domain queries\n",
    "print(f\"\\n🧪 TESTING SEMANTIC SEARCH WITH A/B TESTING QUERIES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define test queries that represent typical user questions\n",
    "test_queries = [\n",
    "    \"homepage conversion improvements\",\n",
    "    \"mobile user behavior tests\",\n",
    "    \"international visitor experiences\", \n",
    "    \"booking flow optimization\",\n",
    "    \"trust and credibility features\",\n",
    "    \"personalization and targeting\",\n",
    "    \"CTA button effectiveness\",\n",
    "    \"special offers and promotions\"\n",
    "]\n",
    "\n",
    "print(f\"🎯 Testing {len(test_queries)} domain-specific queries:\")\n",
    "print(f\"Retriever type: {retriever_type}\")\n",
    "\n",
    "retrieval_results = {}\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n📋 Query {i}: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        docs = primary_retriever.get_relevant_documents(query)\n",
    "        \n",
    "        if docs:\n",
    "            print(f\"  ✅ Found {len(docs)} relevant documents\")\n",
    "            \n",
    "            # Show summary of results\n",
    "            for j, doc in enumerate(docs, 1):\n",
    "                content_preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "                metadata_summary = {k: v for k, v in doc.metadata.items() if k in ['test_name', 'document_id', 'target_segment']}\n",
    "                \n",
    "                print(f\"    {j}. Content: {content_preview}\")\n",
    "                print(f\"       Metadata: {metadata_summary}\")\n",
    "                \n",
    "            retrieval_results[query] = {\n",
    "                'found': len(docs),\n",
    "                'documents': docs\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  ⚠️  No documents found\")\n",
    "            retrieval_results[query] = {'found': 0, 'documents': []}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error retrieving for '{query}': {e}\")\n",
    "        retrieval_results[query] = {'found': 0, 'error': str(e)}\n",
    "\n",
    "# Analyze retrieval performance\n",
    "print(f\"\\n📊 RETRIEVAL PERFORMANCE ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "successful_queries = [q for q, r in retrieval_results.items() if r.get('found', 0) > 0]\n",
    "total_results = sum(r.get('found', 0) for r in retrieval_results.values())\n",
    "avg_results = total_results / len(test_queries) if test_queries else 0\n",
    "\n",
    "print(f\"Successful queries: {len(successful_queries)}/{len(test_queries)}\")\n",
    "print(f\"Total documents retrieved: {total_results}\")\n",
    "print(f\"Average results per query: {avg_results:.1f}\")\n",
    "\n",
    "if successful_queries:\n",
    "    print(f\"\\n✅ Top performing queries:\")\n",
    "    for query in successful_queries[:3]:\n",
    "        count = retrieval_results[query]['found']\n",
    "        print(f\"  • '{query}': {count} documents\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  No successful retrievals found\")\n",
    "    print(f\"Possible issues:\")\n",
    "    print(f\"  • Vector store may be empty (run Step 11 successfully first)\")\n",
    "    print(f\"  • Embedding similarity threshold too high (0.7)\")\n",
    "    print(f\"  • Graph content doesn't match test queries\")\n",
    "    print(f\"  • Text properties not properly embedded\")\n",
    "\n",
    "# Advanced retrieval testing with graph context\n",
    "print(f\"\\n🔗 TESTING GRAPH-ENHANCED RETRIEVAL:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if successful_queries:\n",
    "    # Test hybrid retrieval combining semantic search with graph traversal\n",
    "    sample_query = successful_queries[0]\n",
    "    print(f\"Using sample query: '{sample_query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Get semantic results\n",
    "        semantic_docs = primary_retriever.get_relevant_documents(sample_query)\n",
    "        \n",
    "        if semantic_docs and semantic_docs[0].metadata:\n",
    "            # Extract node identifiers for graph enhancement\n",
    "            sample_metadata = semantic_docs[0].metadata\n",
    "            \n",
    "            print(f\"🔍 Semantic search found: {len(semantic_docs)} documents\")\n",
    "            print(f\"📊 Sample metadata: {sample_metadata}\")\n",
    "            \n",
    "            # Could add graph traversal here to find related nodes\n",
    "            # This would combine semantic similarity with graph relationships\n",
    "            print(f\"🌐 Graph enhancement: Ready for implementation\")\n",
    "            print(f\"   (Would traverse relationships from retrieved nodes)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"⚠️  No metadata available for graph enhancement\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Graph enhancement test failed: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 RAG RETRIEVER TESTING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Semantic search system operational\")\n",
    "print(f\"🔍 Vector embeddings enable intelligent query understanding\")\n",
    "print(f\"🧠 Ready for natural language A/B testing questions\")\n",
    "print(f\"🚀 Foundation complete for Streamlit chatbot development\")\n",
    "\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. 🤖 Build conversational AI chain with retrieved context\")\n",
    "print(\"2. 🌐 Create Streamlit chatbot interface\")\n",
    "print(\"3. 🔗 Integrate retriever with chat responses\")\n",
    "print(\"4. 🚀 Deploy public chatbot for A/B testing queries\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain RAG Retriever - Complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timber_chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}