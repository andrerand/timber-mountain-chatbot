{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21a1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\n",
      "============================================================\n",
      "✅ All libraries imported successfully!\n",
      "📅 Processing started at: 2025-07-01 06:06:47\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# TIMBER MOUNTAIN AI CHATBOT - DATA PROCESSING PIPELINE\n",
    "# ===================================================================\n",
    "# Step 1: Process and Combine Your Data\n",
    "# This notebook processes A/B test metadata and PDF presentations\n",
    "# to create a unified dataset for the Neo4j GraphRAG system.\n",
    "\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🌲 Timber Mountain AI Chatbot - Data Processing Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📅 Processing started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b42d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 LOADING A/B TEST METADATA\n",
      "----------------------------------------\n",
      "✅ Successfully loaded metadata for 5 A/B tests\n",
      "📁 Metadata file: Timber Mountain - AB Test Metadata.xlsx\n",
      "📋 Columns: ['Test Name', 'PDF File Name', 'Test Launch', 'Test End', 'Country', 'Target Segment', 'Page / Placement', 'Test Hypothesis', 'Test Result & Interpretation']\n",
      "\n",
      "🧪 A/B TEST OVERVIEW:\n",
      "----------------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   📄 PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   📅 Duration: 2024-07-08 → 2024-07-28\n",
      "   🎯 Target: Browser-locale ≠ “en-US” (Int’l) vs “en-US” (Domestic)\n",
      "\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   📄 PDF: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   📅 Duration: 2024-08-05 → 2024-08-25\n",
      "   🎯 Target: Wild Willy AI Travel Planner Users\n",
      "\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   📄 PDF: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   📅 Duration: 2024-09-02 → 2024-09-22\n",
      "   🎯 Target: Visitors adding ≥ 1 non-ticket item\n",
      "\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   📄 PDF: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   📅 Duration: 2024-10-07 → 2024-10-21\n",
      "   🎯 Target: Mobile visitors\n",
      "\n",
      "5. Homepage: Special Offers Carousel — Merchandising Test\n",
      "   📄 PDF: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   📅 Duration: 2024-11-04 → 2024-11-24\n",
      "   🎯 Target: First-time visitors\n",
      "\n",
      "🔍 Created metadata lookup for 5 tests\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: LOAD AND ANALYZE A/B TEST METADATA\n",
    "# ===================================================================\n",
    "\n",
    "# Define file paths\n",
    "project_root = Path(\"..\")\n",
    "metadata_path = project_root / \"2 - Synthetic Metadata\" / \"Timber Mountain - AB Test Metadata.xlsx\"\n",
    "pdf_directory = project_root / \"3 - Synthetic A:B Test Results Decks\" / \"2 - Results Presentations\"\n",
    "\n",
    "print(\"📊 LOADING A/B TEST METADATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Load metadata from Excel file\n",
    "    metadata_df = pd.read_excel(metadata_path, sheet_name='Metadata')\n",
    "    \n",
    "    print(f\"✅ Successfully loaded metadata for {len(metadata_df)} A/B tests\")\n",
    "    print(f\"📁 Metadata file: {metadata_path.name}\")\n",
    "    print(f\"📋 Columns: {list(metadata_df.columns)}\")\n",
    "    \n",
    "    # Display summary of tests\n",
    "    print(f\"\\n🧪 A/B TEST OVERVIEW:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        print(f\"{i+1}. {row['Test Name']}\")\n",
    "        print(f\"   📄 PDF: {row['PDF File Name']}\")\n",
    "        print(f\"   📅 Duration: {row['Test Launch'].strftime('%Y-%m-%d')} → {row['Test End'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   🎯 Target: {row['Target Segment']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create metadata lookup dictionary for efficient access\n",
    "    metadata_lookup = {}\n",
    "    for i, row in metadata_df.iterrows():\n",
    "        pdf_filename = row['PDF File Name']\n",
    "        metadata_lookup[pdf_filename] = {\n",
    "            'test_name': row['Test Name'],\n",
    "            'test_launch': row['Test Launch'].strftime('%Y-%m-%d'),\n",
    "            'test_end': row['Test End'].strftime('%Y-%m-%d'),\n",
    "            'country': row['Country'],\n",
    "            'target_segment': row['Target Segment'],\n",
    "            'page_placement': row['Page / Placement'],\n",
    "            'test_hypothesis': row['Test Hypothesis'],\n",
    "            'test_result': row['Test Result & Interpretation']\n",
    "        }\n",
    "    \n",
    "    print(f\"🔍 Created metadata lookup for {len(metadata_lookup)} tests\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Metadata file not found at {metadata_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "h26wg3rx0si",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 PROCESSING PDF FILES\n",
      "----------------------------------------\n",
      "📂 Found 5 PDF files to process:\n",
      "   - 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   - 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   - 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   - 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   - 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "\n",
      "🔍 EXTRACTING TEXT FROM PDF FILES:\n",
      "----------------------------------------\n",
      "Processing: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Extracted 2,992 characters from 7 pages\n",
      "Processing: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Extracted 4,031 characters from 9 pages\n",
      "Processing: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Extracted 4,595 characters from 10 pages\n",
      "Processing: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Extracted 4,244 characters from 10 pages\n",
      "Processing: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Extracted 5,169 characters from 10 pages\n",
      "\n",
      "📊 PDF PROCESSING SUMMARY:\n",
      "   • Successfully processed: 5\n",
      "   • Failed to process: 0\n",
      "   • Total text extracted: 21,031 characters\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 2: PROCESS PDF FILES AND EXTRACT TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n📄 PROCESSING PDF FILES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find all PDF files in the results presentations directory\n",
    "pdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"❌ ERROR: No PDF files found in {pdf_directory}\")\n",
    "    raise FileNotFoundError(\"PDF files not found\")\n",
    "\n",
    "print(f\"📂 Found {len(pdf_files)} PDF files to process:\")\n",
    "for pdf_file in sorted(pdf_files):\n",
    "    print(f\"   - {pdf_file.name}\")\n",
    "\n",
    "# Dictionary to store extracted text content\n",
    "pdf_text_content = {}\n",
    "\n",
    "print(f\"\\n🔍 EXTRACTING TEXT FROM PDF FILES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for pdf_path in sorted(pdf_files):\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract all text from the PDF\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Extract text from all pages\n",
    "            all_pages_text = []\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n{page_text}\")\n",
    "                else:\n",
    "                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n[No text extracted]\")\n",
    "            \n",
    "            # Combine all pages\n",
    "            full_text = \"\\n\\n\".join(all_pages_text)\n",
    "            \n",
    "            # Store the text content\n",
    "            pdf_text_content[pdf_path.name] = {\n",
    "                'file_path': str(pdf_path),\n",
    "                'page_count': len(pdf.pages),\n",
    "                'text_length': len(full_text),\n",
    "                'full_text': full_text\n",
    "            }\n",
    "            \n",
    "            print(f\"   ✅ Extracted {len(full_text):,} characters from {len(pdf.pages)} pages\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERROR processing {pdf_path.name}: {e}\")\n",
    "        pdf_text_content[pdf_path.name] = {\n",
    "            'file_path': str(pdf_path),\n",
    "            'page_count': 0,\n",
    "            'text_length': 0,\n",
    "            'full_text': f\"[ERROR: Could not extract text - {e}]\",\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(f\"\\n📊 PDF PROCESSING SUMMARY:\")\n",
    "print(f\"   • Successfully processed: {len([k for k, v in pdf_text_content.items() if 'error' not in v])}\")\n",
    "print(f\"   • Failed to process: {len([k for k, v in pdf_text_content.items() if 'error' in v])}\")\n",
    "print(f\"   • Total text extracted: {sum(v['text_length'] for v in pdf_text_content.values()):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2apak0qhg45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 COMBINING METADATA WITH PDF CONTENT\n",
      "----------------------------------------\n",
      "Combining data for: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "   ✅ Metadata found: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "   ✅ PDF content: 2,992 characters\n",
      "\n",
      "Combining data for: 2 - Wild-Willy-AI-Planner-Trust-and-Adoption-AB-Test-Results.pdf\n",
      "   ✅ Metadata found: AI Planner: Add Verified Star Ratings — Trust & Adoption Test\n",
      "   ✅ PDF content: 4,031 characters\n",
      "\n",
      "Combining data for: 3 - Timber-Mountain-Unified-Bundle-Flow-Checkout-Test-Results.pdf\n",
      "   ✅ Metadata found: Checkout: Unified Booking.com Bundle Flow — Seamless-Booking Test\n",
      "   ✅ PDF content: 4,595 characters\n",
      "\n",
      "Combining data for: 4 - Timber-Mountain-CTA-Copy-Test-Results.pdf\n",
      "   ✅ Metadata found: Site-wide CTA Copy: “Learn More” vs. “Explore More” — Engagement Nudge Test\n",
      "   ✅ PDF content: 4,244 characters\n",
      "\n",
      "Combining data for: 5 - Homepage-Special-Offers-Carousel-Merchandising-Test-Results.pdf\n",
      "   ✅ Metadata found: Homepage: Special Offers Carousel — Merchandising Test\n",
      "   ✅ PDF content: 5,169 characters\n",
      "\n",
      "📊 COMBINATION SUMMARY:\n",
      "   • Total documents processed: 5\n",
      "   • Successful metadata matches: 5\n",
      "   • Missing metadata: 0\n",
      "   • PDF extraction failures: 0\n",
      "   • Unified documents created: 5\n",
      "\n",
      "📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\n",
      "----------------------------------------\n",
      "Document ID: timber_mountain_001\n",
      "Source PDF: 1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\n",
      "Test Name: Homepage: Domestic vs. International Visitors — Content Personalization Test\n",
      "Content Length: === PAGE 1 ===\n",
      "Locale-Aware Experience: How\n",
      "We Boosted International\n",
      "Conversions at Timber\n",
      "Mountain\n",
      "...\n",
      "Metadata Keys: ['test_name', 'test_launch', 'test_end', 'country', 'target_segment', 'page_placement', 'test_hypothesis', 'test_result']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 3: COMBINE METADATA WITH PDF TEXT CONTENT\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n🔗 COMBINING METADATA WITH PDF CONTENT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "unified_documents = []\n",
    "processing_summary = {\n",
    "    'successful_matches': 0,\n",
    "    'missing_metadata': 0,\n",
    "    'missing_pdf_content': 0,\n",
    "    'total_processed': 0\n",
    "}\n",
    "\n",
    "# Iterate through each PDF file and combine with metadata\n",
    "for pdf_filename in pdf_text_content.keys():\n",
    "    print(f\"Combining data for: {pdf_filename}\")\n",
    "    \n",
    "    # Get metadata for this PDF\n",
    "    metadata = metadata_lookup.get(pdf_filename, {})\n",
    "    \n",
    "    # Get PDF text content\n",
    "    pdf_data = pdf_text_content.get(pdf_filename, {})\n",
    "    \n",
    "    # Create unified document\n",
    "    unified_doc = {\n",
    "        'document_id': f\"timber_mountain_{len(unified_documents) + 1:03d}\",\n",
    "        'source_pdf_filename': pdf_filename,\n",
    "        'source_pdf_path': pdf_data.get('file_path', ''),\n",
    "        'pdf_processing': {\n",
    "            'page_count': pdf_data.get('page_count', 0),\n",
    "            'text_length': pdf_data.get('text_length', 0),\n",
    "            'extraction_error': pdf_data.get('error', None)\n",
    "        },\n",
    "        'metadata': metadata,\n",
    "        'content': {\n",
    "            'full_text': pdf_data.get('full_text', ''),\n",
    "            'processed_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Update processing summary\n",
    "    processing_summary['total_processed'] += 1\n",
    "    \n",
    "    if metadata:\n",
    "        processing_summary['successful_matches'] += 1\n",
    "        print(f\"   ✅ Metadata found: {metadata.get('test_name', 'Unknown')}\")\n",
    "    else:\n",
    "        processing_summary['missing_metadata'] += 1\n",
    "        print(f\"   ⚠️  No metadata found for {pdf_filename}\")\n",
    "    \n",
    "    if pdf_data.get('full_text') and 'error' not in pdf_data:\n",
    "        print(f\"   ✅ PDF content: {pdf_data['text_length']:,} characters\")\n",
    "    else:\n",
    "        processing_summary['missing_pdf_content'] += 1\n",
    "        print(f\"   ⚠️  PDF content extraction failed\")\n",
    "    \n",
    "    unified_documents.append(unified_doc)\n",
    "    print()\n",
    "\n",
    "print(f\"📊 COMBINATION SUMMARY:\")\n",
    "print(f\"   • Total documents processed: {processing_summary['total_processed']}\")\n",
    "print(f\"   • Successful metadata matches: {processing_summary['successful_matches']}\")\n",
    "print(f\"   • Missing metadata: {processing_summary['missing_metadata']}\")\n",
    "print(f\"   • PDF extraction failures: {processing_summary['missing_pdf_content']}\")\n",
    "print(f\"   • Unified documents created: {len(unified_documents)}\")\n",
    "\n",
    "# Display sample unified document structure\n",
    "if unified_documents:\n",
    "    print(f\"\\n📋 SAMPLE UNIFIED DOCUMENT STRUCTURE:\")\n",
    "    print(\"-\" * 40)\n",
    "    sample_doc = unified_documents[0]\n",
    "    print(f\"Document ID: {sample_doc['document_id']}\")\n",
    "    print(f\"Source PDF: {sample_doc['source_pdf_filename']}\")\n",
    "    print(f\"Test Name: {sample_doc['metadata'].get('test_name', 'N/A')}\")\n",
    "    print(f\"Content Length: {sample_doc['content']['full_text'][:100]}...\")\n",
    "    print(f\"Metadata Keys: {list(sample_doc['metadata'].keys()) if sample_doc['metadata'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12r7oth4vzba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 GENERATING UNIFIED JSON OUTPUT\n",
      "----------------------------------------\n",
      "✅ Successfully saved unified dataset!\n",
      "📁 Output file: processed_documents.json\n",
      "📏 File size: 0.03 MB\n",
      "📊 Contains 5 documents\n",
      "\n",
      "📋 JSON STRUCTURE SUMMARY:\n",
      "----------------------------------------\n",
      "└── processing_metadata\n",
      "    ├── created_timestamp\n",
      "    ├── source_excel_file\n",
      "    ├── source_pdf_directory\n",
      "    ├── total_documents\n",
      "    └── processing_summary\n",
      "└── documents (array)\n",
      "    └── [document]\n",
      "        ├── document_id\n",
      "        ├── source_pdf_filename\n",
      "        ├── source_pdf_path\n",
      "        ├── pdf_processing\n",
      "        ├── metadata\n",
      "        └── content\n",
      "\n",
      "🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\n",
      "📝 Use 'processed_documents.json' as input for graph population\n",
      "\n",
      "📄 SAMPLE JSON OUTPUT:\n",
      "----------------------------------------\n",
      "{\n",
      "  \"processing_metadata\": {\n",
      "    \"created_timestamp\": \"2025-07-01T06:16:03.658141\",\n",
      "    \"source_excel_file\": \"../2 - Synthetic Metadata/Timber Mountain - AB Test Metadata.xlsx\",\n",
      "    \"source_pdf_directory\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations\",\n",
      "    \"total_documents\": 5,\n",
      "    \"processing_summary\": {\n",
      "      \"successful_matches\": 5,\n",
      "      \"missing_metadata\": 0,\n",
      "      \"missing_pdf_content\": 0,\n",
      "      \"total_processed\": 5\n",
      "    }\n",
      "  },\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"document_id\": \"timber_mountain_001\",\n",
      "      \"source_pdf_filename\": \"1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"source_pdf_path\": \"../3 - Synthetic A:B Test Results Decks/2 - Results Presentations/1 - Locale-Aware-Experience-How-We-Boosted-International-Conversions-at-Timber-Mountain.pdf\",\n",
      "      \"pdf_processing\": {\n",
      "        \"page_count\": 7,\n",
      "        \"text_length\": 2992,\n",
      "        \"extraction_error\": null\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"test_name\": \"...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 4: GENERATE UNIFIED JSON OUTPUT FOR NEO4J GRAPHRAG\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n💾 GENERATING UNIFIED JSON OUTPUT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define output path\n",
    "output_json_path = project_root / \"processed_documents.json\"\n",
    "\n",
    "# Create the final output structure optimized for GraphRAG\n",
    "final_output = {\n",
    "    'processing_metadata': {\n",
    "        'created_timestamp': datetime.now().isoformat(),\n",
    "        'source_excel_file': str(metadata_path),\n",
    "        'source_pdf_directory': str(pdf_directory),\n",
    "        'total_documents': len(unified_documents),\n",
    "        'processing_summary': processing_summary\n",
    "    },\n",
    "    'documents': unified_documents\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Save to JSON file with proper formatting\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Calculate file size\n",
    "    file_size_mb = output_json_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✅ Successfully saved unified dataset!\")\n",
    "    print(f\"📁 Output file: {output_json_path.name}\")\n",
    "    print(f\"📏 File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"📊 Contains {len(unified_documents)} documents\")\n",
    "    \n",
    "    # Display JSON structure summary\n",
    "    print(f\"\\n📋 JSON STRUCTURE SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"└── processing_metadata\")\n",
    "    print(\"    ├── created_timestamp\")\n",
    "    print(\"    ├── source_excel_file\") \n",
    "    print(\"    ├── source_pdf_directory\")\n",
    "    print(\"    ├── total_documents\")\n",
    "    print(\"    └── processing_summary\")\n",
    "    print(\"└── documents (array)\")\n",
    "    print(\"    └── [document]\")\n",
    "    print(\"        ├── document_id\")\n",
    "    print(\"        ├── source_pdf_filename\")\n",
    "    print(\"        ├── source_pdf_path\")\n",
    "    print(\"        ├── pdf_processing\")\n",
    "    print(\"        ├── metadata\")\n",
    "    print(\"        └── content\")\n",
    "    \n",
    "    print(f\"\\n🎯 READY FOR NEXT STEP: Populating Neo4j Graph Database\")\n",
    "    print(f\"📝 Use '{output_json_path.name}' as input for graph population\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR saving JSON file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display sample of final JSON structure\n",
    "print(f\"\\n📄 SAMPLE JSON OUTPUT:\")\n",
    "print(\"-\" * 40)\n",
    "sample_output = {\n",
    "    'processing_metadata': final_output['processing_metadata'],\n",
    "    'documents': [final_output['documents'][0]] if final_output['documents'] else []\n",
    "}\n",
    "\n",
    "print(json.dumps(sample_output, indent=2)[:1000] + \"...\" if len(str(sample_output)) > 1000 else json.dumps(sample_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d72gx3i5aw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DATA VALIDATION AND FINAL SUMMARY\n",
      "============================================================\n",
      "🔍 VALIDATION RESULTS:\n",
      "------------------------------\n",
      "✅ All A/B tests processed: True (5/5)\n",
      "✅ All metadata matched: True (5/5)\n",
      "✅ All PDFs extracted: True (failures: 0)\n",
      "✅ Output file created: True (processed_documents.json)\n",
      "\n",
      "📊 CONTENT STATISTICS:\n",
      "------------------------------\n",
      "Total text content: 21,031 characters\n",
      "Average per document: 4,206 characters\n",
      "\n",
      "📋 DOCUMENT-LEVEL BREAKDOWN:\n",
      "------------------------------\n",
      "1. Homepage: Domestic vs. International Visitors — Co...\n",
      "   📄 7 pages, 2,992 characters\n",
      "2. AI Planner: Add Verified Star Ratings — Trust & Ad...\n",
      "   📄 9 pages, 4,031 characters\n",
      "3. Checkout: Unified Booking.com Bundle Flow — Seamle...\n",
      "   📄 10 pages, 4,595 characters\n",
      "4. Site-wide CTA Copy: “Learn More” vs. “Explore More...\n",
      "   📄 10 pages, 4,244 characters\n",
      "5. Homepage: Special Offers Carousel — Merchandising ...\n",
      "   📄 10 pages, 5,169 characters\n",
      "\n",
      "⚠️  POTENTIAL ISSUES:\n",
      "------------------------------\n",
      "✅ No issues detected - data quality looks good!\n",
      "\n",
      "🎯 PROCESSING COMPLETE!\n",
      "============================================================\n",
      "📈 Successfully processed 5 A/B test documents\n",
      "💾 Output saved to: ../processed_documents.json\n",
      "🔗 Ready for Neo4j graph database population\n",
      "📅 Processing completed at: 2025-07-01 06:17:28\n",
      "\n",
      "📋 NEXT STEPS:\n",
      "------------------------------\n",
      "1. 🗄️  Set up Neo4j database connection\n",
      "2. 🏗️  Create graph schema for A/B test data\n",
      "3. 📥 Import processed_documents.json into Neo4j\n",
      "4. 🔍 Build GraphRAG query system\n",
      "5. 🤖 Integrate with LangChain for chatbot responses\n",
      "6. 🌐 Deploy Streamlit frontend\n",
      "\n",
      "🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 5: DATA VALIDATION AND SUMMARY STATISTICS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n✅ DATA VALIDATION AND FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validation checks\n",
    "validation_results = {\n",
    "    'all_tests_processed': len(unified_documents) == len(metadata_df),\n",
    "    'all_metadata_matched': processing_summary['missing_metadata'] == 0,\n",
    "    'all_pdfs_extracted': processing_summary['missing_pdf_content'] == 0,\n",
    "    'output_file_created': output_json_path.exists(),\n",
    "    'total_characters': sum(len(doc['content']['full_text']) for doc in unified_documents),\n",
    "    'avg_characters_per_doc': 0\n",
    "}\n",
    "\n",
    "if len(unified_documents) > 0:\n",
    "    validation_results['avg_characters_per_doc'] = validation_results['total_characters'] / len(unified_documents)\n",
    "\n",
    "# Display validation results\n",
    "print(\"🔍 VALIDATION RESULTS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"✅ All A/B tests processed: {validation_results['all_tests_processed']} ({len(unified_documents)}/{len(metadata_df)})\")\n",
    "print(f\"✅ All metadata matched: {validation_results['all_metadata_matched']} ({processing_summary['successful_matches']}/{len(unified_documents)})\")\n",
    "print(f\"✅ All PDFs extracted: {validation_results['all_pdfs_extracted']} (failures: {processing_summary['missing_pdf_content']})\")\n",
    "print(f\"✅ Output file created: {validation_results['output_file_created']} ({output_json_path.name})\")\n",
    "\n",
    "print(f\"\\n📊 CONTENT STATISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total text content: {validation_results['total_characters']:,} characters\")\n",
    "print(f\"Average per document: {validation_results['avg_characters_per_doc']:,.0f} characters\")\n",
    "\n",
    "# Document-level statistics\n",
    "print(f\"\\n📋 DOCUMENT-LEVEL BREAKDOWN:\")\n",
    "print(\"-\" * 30)\n",
    "for i, doc in enumerate(unified_documents, 1):\n",
    "    test_name = doc['metadata'].get('test_name', 'Unknown Test')\n",
    "    char_count = len(doc['content']['full_text'])\n",
    "    page_count = doc['pdf_processing']['page_count']\n",
    "    \n",
    "    print(f\"{i}. {test_name[:50]}{'...' if len(test_name) > 50 else ''}\")\n",
    "    print(f\"   📄 {page_count} pages, {char_count:,} characters\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(f\"\\n⚠️  POTENTIAL ISSUES:\")\n",
    "print(\"-\" * 30)\n",
    "issues_found = 0\n",
    "\n",
    "for doc in unified_documents:\n",
    "    if doc['pdf_processing'].get('extraction_error'):\n",
    "        print(f\"❌ PDF extraction error in: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if not doc['metadata']:\n",
    "        print(f\"⚠️  Missing metadata for: {doc['source_pdf_filename']}\")\n",
    "        issues_found += 1\n",
    "    \n",
    "    if len(doc['content']['full_text']) < 100:\n",
    "        print(f\"⚠️  Very short content in: {doc['source_pdf_filename']} ({len(doc['content']['full_text'])} chars)\")\n",
    "        issues_found += 1\n",
    "\n",
    "if issues_found == 0:\n",
    "    print(\"✅ No issues detected - data quality looks good!\")\n",
    "\n",
    "# Final processing summary\n",
    "print(f\"\\n🎯 PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📈 Successfully processed {len(unified_documents)} A/B test documents\")\n",
    "print(f\"💾 Output saved to: {output_json_path}\")\n",
    "print(f\"🔗 Ready for Neo4j graph database population\")\n",
    "print(f\"📅 Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Next steps guidance\n",
    "print(f\"\\n📋 NEXT STEPS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. 🗄️  Set up Neo4j database connection\")\n",
    "print(\"2. 🏗️  Create graph schema for A/B test data\")\n",
    "print(\"3. 📥 Import processed_documents.json into Neo4j\")\n",
    "print(\"4. 🔍 Build GraphRAG query system\")\n",
    "print(\"5. 🤖 Integrate with LangChain for chatbot responses\")\n",
    "print(f\"6. 🌐 Deploy Streamlit frontend\")\n",
    "\n",
    "print(f\"\\n🌲 Timber Mountain AI Chatbot data processing pipeline complete! 🌲\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timber_chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
