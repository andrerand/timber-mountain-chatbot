{
 "cells": [
  {
   "cell_type": "code",
   "id": "c21a1170",
   "metadata": {},
   "outputs": [],
   "source": "# ===================================================================\n# TIMBER MOUNTAIN AI CHATBOT - DATA PROCESSING PIPELINE\n# ===================================================================\n# Step 1: Process and Combine Your Data\n# This notebook processes A/B test metadata and PDF presentations\n# to create a unified dataset for the Neo4j GraphRAG system.\n\nimport pandas as pd\nimport pdfplumber\nimport json\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\nprint(\"üå≤ Timber Mountain AI Chatbot - Data Processing Pipeline\")\nprint(\"=\" * 60)\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(f\"üìÖ Processing started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "code",
   "id": "7b42d926",
   "metadata": {},
   "outputs": [],
   "source": "# ===================================================================\n# STEP 1: LOAD AND ANALYZE A/B TEST METADATA\n# ===================================================================\n\n# Define file paths\nproject_root = Path(\"..\")\nmetadata_path = project_root / \"2 - Synthetic Metadata\" / \"Timber Mountain - AB Test Metadata.xlsx\"\npdf_directory = project_root / \"3 - Synthetic A:B Test Results Decks\" / \"2 - Results Presentations\"\n\nprint(\"üìä LOADING A/B TEST METADATA\")\nprint(\"-\" * 40)\n\ntry:\n    # Load metadata from Excel file\n    metadata_df = pd.read_excel(metadata_path, sheet_name='Metadata')\n    \n    print(f\"‚úÖ Successfully loaded metadata for {len(metadata_df)} A/B tests\")\n    print(f\"üìÅ Metadata file: {metadata_path.name}\")\n    print(f\"üìã Columns: {list(metadata_df.columns)}\")\n    \n    # Display summary of tests\n    print(f\"\\nüß™ A/B TEST OVERVIEW:\")\n    print(\"-\" * 40)\n    for i, row in metadata_df.iterrows():\n        print(f\"{i+1}. {row['Test Name']}\")\n        print(f\"   üìÑ PDF: {row['PDF File Name']}\")\n        print(f\"   üìÖ Duration: {row['Test Launch'].strftime('%Y-%m-%d')} ‚Üí {row['Test End'].strftime('%Y-%m-%d')}\")\n        print(f\"   üéØ Target: {row['Target Segment']}\")\n        print()\n    \n    # Create metadata lookup dictionary for efficient access\n    metadata_lookup = {}\n    for i, row in metadata_df.iterrows():\n        pdf_filename = row['PDF File Name']\n        metadata_lookup[pdf_filename] = {\n            'test_name': row['Test Name'],\n            'test_launch': row['Test Launch'].strftime('%Y-%m-%d'),\n            'test_end': row['Test End'].strftime('%Y-%m-%d'),\n            'country': row['Country'],\n            'target_segment': row['Target Segment'],\n            'page_placement': row['Page / Placement'],\n            'test_hypothesis': row['Test Hypothesis'],\n            'test_result': row['Test Result & Interpretation']\n        }\n    \n    print(f\"üîç Created metadata lookup for {len(metadata_lookup)} tests\")\n    \nexcept FileNotFoundError:\n    print(f\"‚ùå ERROR: Metadata file not found at {metadata_path}\")\n    raise\nexcept Exception as e:\n    print(f\"‚ùå ERROR loading metadata: {e}\")\n    raise"
  },
  {
   "cell_type": "code",
   "id": "h26wg3rx0si",
   "source": "# ===================================================================\n# STEP 2: PROCESS PDF FILES AND EXTRACT TEXT CONTENT\n# ===================================================================\n\nprint(\"\\nüìÑ PROCESSING PDF FILES\")\nprint(\"-\" * 40)\n\n# Find all PDF files in the results presentations directory\npdf_files = [f for f in pdf_directory.iterdir() if f.suffix.lower() == '.pdf']\n\nif not pdf_files:\n    print(f\"‚ùå ERROR: No PDF files found in {pdf_directory}\")\n    raise FileNotFoundError(\"PDF files not found\")\n\nprint(f\"üìÇ Found {len(pdf_files)} PDF files to process:\")\nfor pdf_file in sorted(pdf_files):\n    print(f\"   - {pdf_file.name}\")\n\n# Dictionary to store extracted text content\npdf_text_content = {}\n\nprint(f\"\\nüîç EXTRACTING TEXT FROM PDF FILES:\")\nprint(\"-\" * 40)\n\nfor pdf_path in sorted(pdf_files):\n    print(f\"Processing: {pdf_path.name}\")\n    \n    try:\n        # Extract all text from the PDF\n        with pdfplumber.open(pdf_path) as pdf:\n            # Extract text from all pages\n            all_pages_text = []\n            \n            for page_num, page in enumerate(pdf.pages, 1):\n                page_text = page.extract_text()\n                if page_text:\n                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n{page_text}\")\n                else:\n                    all_pages_text.append(f\"=== PAGE {page_num} ===\\n[No text extracted]\")\n            \n            # Combine all pages\n            full_text = \"\\n\\n\".join(all_pages_text)\n            \n            # Store the text content\n            pdf_text_content[pdf_path.name] = {\n                'file_path': str(pdf_path),\n                'page_count': len(pdf.pages),\n                'text_length': len(full_text),\n                'full_text': full_text\n            }\n            \n            print(f\"   ‚úÖ Extracted {len(full_text):,} characters from {len(pdf.pages)} pages\")\n    \n    except Exception as e:\n        print(f\"   ‚ùå ERROR processing {pdf_path.name}: {e}\")\n        pdf_text_content[pdf_path.name] = {\n            'file_path': str(pdf_path),\n            'page_count': 0,\n            'text_length': 0,\n            'full_text': f\"[ERROR: Could not extract text - {e}]\",\n            'error': str(e)\n        }\n\nprint(f\"\\nüìä PDF PROCESSING SUMMARY:\")\nprint(f\"   ‚Ä¢ Successfully processed: {len([k for k, v in pdf_text_content.items() if 'error' not in v])}\")\nprint(f\"   ‚Ä¢ Failed to process: {len([k for k, v in pdf_text_content.items() if 'error' in v])}\")\nprint(f\"   ‚Ä¢ Total text extracted: {sum(v['text_length'] for v in pdf_text_content.values()):,} characters\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2apak0qhg45",
   "source": "# ===================================================================\n# STEP 3: COMBINE METADATA WITH PDF TEXT CONTENT\n# ===================================================================\n\nprint(\"\\nüîó COMBINING METADATA WITH PDF CONTENT\")\nprint(\"-\" * 40)\n\nunified_documents = []\nprocessing_summary = {\n    'successful_matches': 0,\n    'missing_metadata': 0,\n    'missing_pdf_content': 0,\n    'total_processed': 0\n}\n\n# Iterate through each PDF file and combine with metadata\nfor pdf_filename in pdf_text_content.keys():\n    print(f\"Combining data for: {pdf_filename}\")\n    \n    # Get metadata for this PDF\n    metadata = metadata_lookup.get(pdf_filename, {})\n    \n    # Get PDF text content\n    pdf_data = pdf_text_content.get(pdf_filename, {})\n    \n    # Create unified document\n    unified_doc = {\n        'document_id': f\"timber_mountain_{len(unified_documents) + 1:03d}\",\n        'source_pdf_filename': pdf_filename,\n        'source_pdf_path': pdf_data.get('file_path', ''),\n        'pdf_processing': {\n            'page_count': pdf_data.get('page_count', 0),\n            'text_length': pdf_data.get('text_length', 0),\n            'extraction_error': pdf_data.get('error', None)\n        },\n        'metadata': metadata,\n        'content': {\n            'full_text': pdf_data.get('full_text', ''),\n            'processed_timestamp': datetime.now().isoformat()\n        }\n    }\n    \n    # Update processing summary\n    processing_summary['total_processed'] += 1\n    \n    if metadata:\n        processing_summary['successful_matches'] += 1\n        print(f\"   ‚úÖ Metadata found: {metadata.get('test_name', 'Unknown')}\")\n    else:\n        processing_summary['missing_metadata'] += 1\n        print(f\"   ‚ö†Ô∏è  No metadata found for {pdf_filename}\")\n    \n    if pdf_data.get('full_text') and 'error' not in pdf_data:\n        print(f\"   ‚úÖ PDF content: {pdf_data['text_length']:,} characters\")\n    else:\n        processing_summary['missing_pdf_content'] += 1\n        print(f\"   ‚ö†Ô∏è  PDF content extraction failed\")\n    \n    unified_documents.append(unified_doc)\n    print()\n\nprint(f\"üìä COMBINATION SUMMARY:\")\nprint(f\"   ‚Ä¢ Total documents processed: {processing_summary['total_processed']}\")\nprint(f\"   ‚Ä¢ Successful metadata matches: {processing_summary['successful_matches']}\")\nprint(f\"   ‚Ä¢ Missing metadata: {processing_summary['missing_metadata']}\")\nprint(f\"   ‚Ä¢ PDF extraction failures: {processing_summary['missing_pdf_content']}\")\nprint(f\"   ‚Ä¢ Unified documents created: {len(unified_documents)}\")\n\n# Display sample unified document structure\nif unified_documents:\n    print(f\"\\nüìã SAMPLE UNIFIED DOCUMENT STRUCTURE:\")\n    print(\"-\" * 40)\n    sample_doc = unified_documents[0]\n    print(f\"Document ID: {sample_doc['document_id']}\")\n    print(f\"Source PDF: {sample_doc['source_pdf_filename']}\")\n    print(f\"Test Name: {sample_doc['metadata'].get('test_name', 'N/A')}\")\n    print(f\"Content Length: {sample_doc['content']['full_text'][:100]}...\")\n    print(f\"Metadata Keys: {list(sample_doc['metadata'].keys()) if sample_doc['metadata'] else 'None'}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "12r7oth4vzba",
   "source": "# ===================================================================\n# STEP 4: GENERATE UNIFIED JSON OUTPUT FOR NEO4J GRAPHRAG\n# ===================================================================\n\nprint(\"\\nüíæ GENERATING UNIFIED JSON OUTPUT\")\nprint(\"-\" * 40)\n\n# Define output path\noutput_json_path = project_root / \"processed_documents.json\"\n\n# Create the final output structure optimized for GraphRAG\nfinal_output = {\n    'processing_metadata': {\n        'created_timestamp': datetime.now().isoformat(),\n        'source_excel_file': str(metadata_path),\n        'source_pdf_directory': str(pdf_directory),\n        'total_documents': len(unified_documents),\n        'processing_summary': processing_summary\n    },\n    'documents': unified_documents\n}\n\ntry:\n    # Save to JSON file with proper formatting\n    with open(output_json_path, 'w', encoding='utf-8') as f:\n        json.dump(final_output, f, indent=2, ensure_ascii=False)\n    \n    # Calculate file size\n    file_size_mb = output_json_path.stat().st_size / (1024 * 1024)\n    \n    print(f\"‚úÖ Successfully saved unified dataset!\")\n    print(f\"üìÅ Output file: {output_json_path.name}\")\n    print(f\"üìè File size: {file_size_mb:.2f} MB\")\n    print(f\"üìä Contains {len(unified_documents)} documents\")\n    \n    # Display JSON structure summary\n    print(f\"\\nüìã JSON STRUCTURE SUMMARY:\")\n    print(\"-\" * 40)\n    print(\"‚îî‚îÄ‚îÄ processing_metadata\")\n    print(\"    ‚îú‚îÄ‚îÄ created_timestamp\")\n    print(\"    ‚îú‚îÄ‚îÄ source_excel_file\") \n    print(\"    ‚îú‚îÄ‚îÄ source_pdf_directory\")\n    print(\"    ‚îú‚îÄ‚îÄ total_documents\")\n    print(\"    ‚îî‚îÄ‚îÄ processing_summary\")\n    print(\"‚îî‚îÄ‚îÄ documents (array)\")\n    print(\"    ‚îî‚îÄ‚îÄ [document]\")\n    print(\"        ‚îú‚îÄ‚îÄ document_id\")\n    print(\"        ‚îú‚îÄ‚îÄ source_pdf_filename\")\n    print(\"        ‚îú‚îÄ‚îÄ source_pdf_path\")\n    print(\"        ‚îú‚îÄ‚îÄ pdf_processing\")\n    print(\"        ‚îú‚îÄ‚îÄ metadata\")\n    print(\"        ‚îî‚îÄ‚îÄ content\")\n    \n    print(f\"\\nüéØ READY FOR NEXT STEP: Populating Neo4j Graph Database\")\n    print(f\"üìù Use '{output_json_path.name}' as input for graph population\")\n    \nexcept Exception as e:\n    print(f\"‚ùå ERROR saving JSON file: {e}\")\n    raise\n\n# Display sample of final JSON structure\nprint(f\"\\nüìÑ SAMPLE JSON OUTPUT:\")\nprint(\"-\" * 40)\nsample_output = {\n    'processing_metadata': final_output['processing_metadata'],\n    'documents': [final_output['documents'][0]] if final_output['documents'] else []\n}\n\nprint(json.dumps(sample_output, indent=2)[:1000] + \"...\" if len(str(sample_output)) > 1000 else json.dumps(sample_output, indent=2))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d72gx3i5aw",
   "source": "# ===================================================================\n# STEP 5: DATA VALIDATION AND SUMMARY STATISTICS\n# ===================================================================\n\nprint(\"\\n‚úÖ DATA VALIDATION AND FINAL SUMMARY\")\nprint(\"=\" * 60)\n\n# Validation checks\nvalidation_results = {\n    'all_tests_processed': len(unified_documents) == len(metadata_df),\n    'all_metadata_matched': processing_summary['missing_metadata'] == 0,\n    'all_pdfs_extracted': processing_summary['missing_pdf_content'] == 0,\n    'output_file_created': output_json_path.exists(),\n    'total_characters': sum(len(doc['content']['full_text']) for doc in unified_documents),\n    'avg_characters_per_doc': 0\n}\n\nif len(unified_documents) > 0:\n    validation_results['avg_characters_per_doc'] = validation_results['total_characters'] / len(unified_documents)\n\n# Display validation results\nprint(\"üîç VALIDATION RESULTS:\")\nprint(\"-\" * 30)\nprint(f\"‚úÖ All A/B tests processed: {validation_results['all_tests_processed']} ({len(unified_documents)}/{len(metadata_df)})\")\nprint(f\"‚úÖ All metadata matched: {validation_results['all_metadata_matched']} ({processing_summary['successful_matches']}/{len(unified_documents)})\")\nprint(f\"‚úÖ All PDFs extracted: {validation_results['all_pdfs_extracted']} (failures: {processing_summary['missing_pdf_content']})\")\nprint(f\"‚úÖ Output file created: {validation_results['output_file_created']} ({output_json_path.name})\")\n\nprint(f\"\\nüìä CONTENT STATISTICS:\")\nprint(\"-\" * 30)\nprint(f\"Total text content: {validation_results['total_characters']:,} characters\")\nprint(f\"Average per document: {validation_results['avg_characters_per_doc']:,.0f} characters\")\n\n# Document-level statistics\nprint(f\"\\nüìã DOCUMENT-LEVEL BREAKDOWN:\")\nprint(\"-\" * 30)\nfor i, doc in enumerate(unified_documents, 1):\n    test_name = doc['metadata'].get('test_name', 'Unknown Test')\n    char_count = len(doc['content']['full_text'])\n    page_count = doc['pdf_processing']['page_count']\n    \n    print(f\"{i}. {test_name[:50]}{'...' if len(test_name) > 50 else ''}\")\n    print(f\"   üìÑ {page_count} pages, {char_count:,} characters\")\n\n# Check for potential issues\nprint(f\"\\n‚ö†Ô∏è  POTENTIAL ISSUES:\")\nprint(\"-\" * 30)\nissues_found = 0\n\nfor doc in unified_documents:\n    if doc['pdf_processing'].get('extraction_error'):\n        print(f\"‚ùå PDF extraction error in: {doc['source_pdf_filename']}\")\n        issues_found += 1\n    \n    if not doc['metadata']:\n        print(f\"‚ö†Ô∏è  Missing metadata for: {doc['source_pdf_filename']}\")\n        issues_found += 1\n    \n    if len(doc['content']['full_text']) < 100:\n        print(f\"‚ö†Ô∏è  Very short content in: {doc['source_pdf_filename']} ({len(doc['content']['full_text'])} chars)\")\n        issues_found += 1\n\nif issues_found == 0:\n    print(\"‚úÖ No issues detected - data quality looks good!\")\n\n# Final processing summary\nprint(f\"\\nüéØ PROCESSING COMPLETE!\")\nprint(\"=\" * 60)\nprint(f\"üìà Successfully processed {len(unified_documents)} A/B test documents\")\nprint(f\"üíæ Output saved to: {output_json_path}\")\nprint(f\"üîó Ready for Neo4j graph database population\")\nprint(f\"üìÖ Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Next steps guidance\nprint(f\"\\nüìã NEXT STEPS:\")\nprint(\"-\" * 30)\nprint(\"1. üóÑÔ∏è  Set up Neo4j database connection\")\nprint(\"2. üèóÔ∏è  Create graph schema for A/B test data\")\nprint(\"3. üì• Import processed_documents.json into Neo4j\")\nprint(\"4. üîç Build GraphRAG query system\")\nprint(\"5. ü§ñ Integrate with LangChain for chatbot responses\")\nprint(f\"6. üåê Deploy Streamlit frontend\")\n\nprint(f\"\\nüå≤ Timber Mountain AI Chatbot data processing pipeline complete! üå≤\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timber_chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}